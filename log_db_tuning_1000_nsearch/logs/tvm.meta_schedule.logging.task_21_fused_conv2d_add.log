2023-05-18 11:57:31 [INFO] [task_scheduler.cc:160] Initializing Task #21: "fused_conv2d_add"
2023-05-18 11:57:31 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(3), T.int64(642), T.int64(450)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(inp_0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(641) and T.int64(1) <= v_i3 and v_i3 < T.int64(449), inp_0[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(64), T.int64(640), T.int64(448), T.int64(3), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(640), T.int64(448)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv1[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv1[v_ax0, v_ax1, T.int64(0), T.int64(0)]
2023-05-18 11:57:31 [INFO] [task_scheduler.cc:164] Total 1 design space(s) generated
2023-05-18 11:57:31 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 512})
            var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
            self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(32), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                        for rc_0, ry_0, rx_0 in T.grid(T.int64(3), T.int64(1), T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(9184)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(3), rc_0)
                                    v2 = T.axis.spatial(T.int64(642), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(80) + ax0_ax1_ax2_ax3_fused // T.int64(112))
                                    v3 = T.axis.spatial(T.int64(450), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + ax0_ax1_ax2_ax3_fused % T.int64(112))
                                    T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 2})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(192)):
                                with T.block("self_rrdb_conv_first.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused // T.int64(3))
                                    v1 = T.axis.spatial(T.int64(3), rc_0)
                                    v2 = T.axis.spatial(T.int64(3), ax0_ax1_ax2_ax3_fused % T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), rx_0)
                                    T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                    T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 3})
                                    self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(5), T.int64(2), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(2)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(14) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(20) + yy_3 * T.int64(4) + yy_4)
                                    v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(14) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                    v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1 + rc_2)
                                    v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                    v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(20), T.int64(4)):
                            with T.block("var_conv2d_nchw_intermediate_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(14) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                                v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) // T.int64(2) * T.int64(20) + ax2)
                                v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(14) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                                T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                                T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                                var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 16, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 1, 4, 5, 4])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 14, 2, 2, 2])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
2023-05-18 12:04:42 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 12:04:42 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2023-05-18 12:04:44 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 487 failure(s)
2023-05-18 12:04:44 [INFO] [evolutionary_search.cc:723] Sampled 25 candidate(s)
2023-05-18 12:04:48 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 104 failure(s)
2023-05-18 12:04:52 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 93 failure(s)
2023-05-18 12:04:56 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 101 failure(s)
2023-05-18 12:04:59 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 76 failure(s)
2023-05-18 12:05:00 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9998  0.9988  0.9983  0.9981  0.9976  0.9969  0.9961  0.9956  0.9946  0.9931  0.9930  0.9927  0.9896  0.9896  0.9892  0.9883
[17 : 32]:	0.9872  0.9869  0.9861  0.9850  0.9848  0.9846  0.9844  0.9843  0.9843  0.9838  0.9838  0.9834  0.9813  0.9809  0.9801  0.9799
[33 : 48]:	0.9796  0.9795  0.9792  0.9788  0.9778  0.9778  0.9771  0.9768  0.9764  0.9745  0.9734  0.9728  0.9728  0.9726  0.9721  0.9717
[49 : 64]:	0.9710  0.9703  0.9696  0.9695  0.9690  0.9674  0.9666  0.9663  0.9661  0.9651  0.9650  0.9638  0.9628  0.9618  0.9608  0.9607
2023-05-18 12:05:00 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 12:05:00 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #1: GFLOPs: 222.8443. Time: 4528.9667 us. Best GFLOPs: 222.8443
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #2: GFLOPs: 89.6400. Time: 11258.9677 us. Best GFLOPs: 222.8443
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #3: GFLOPs: 12.2085. Time: 82668.4860 us. Best GFLOPs: 222.8443
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #4: GFLOPs: 86.7159. Time: 11638.6342 us. Best GFLOPs: 222.8443
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #5: GFLOPs: 575.7703. Time: 1752.8768 us. Best GFLOPs: 575.7703
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #6: GFLOPs: 33.8679. Time: 29799.7500 us. Best GFLOPs: 575.7703
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #7: GFLOPs: 109.1774. Time: 9244.1666 us. Best GFLOPs: 575.7703
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #8: GFLOPs: 30.0859. Time: 33545.7917 us. Best GFLOPs: 575.7703
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #9: GFLOPs: 136.7246. Time: 7381.6607 us. Best GFLOPs: 575.7703
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #10: GFLOPs: 116.7598. Time: 8643.8542 us. Best GFLOPs: 575.7703
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #11: GFLOPs: 26.9766. Time: 37412.1530 us. Best GFLOPs: 575.7703
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #12: GFLOPs: 88.7770. Time: 11368.4208 us. Best GFLOPs: 575.7703
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #13: GFLOPs: 191.8883. Time: 5259.5934 us. Best GFLOPs: 575.7703
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #14: GFLOPs: 59.9642. Time: 16830.9515 us. Best GFLOPs: 575.7703
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #15: GFLOPs: 7.1314. Time: 141522.4723 us. Best GFLOPs: 575.7703
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #16: GFLOPs: 722.5245. Time: 1396.8446 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #17: GFLOPs: 8.9960. Time: 112189.0000 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #18: GFLOPs: 6.0615. Time: 166502.9027 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #19: GFLOPs: 318.2998. Time: 3170.7665 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #20: GFLOPs: 15.3701. Time: 65663.5697 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #21: GFLOPs: 54.0697. Time: 18665.8195 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #22: GFLOPs: 24.8302. Time: 40646.1943 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #23: GFLOPs: 68.9984. Time: 14627.2135 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #24: GFLOPs: 110.7793. Time: 9110.4924 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #25: GFLOPs: 44.2501. Time: 22807.9500 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #26: GFLOPs: 123.0163. Time: 8204.2308 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #27: GFLOPs: 7.8971. Time: 127801.3193 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #28: GFLOPs: 7.4003. Time: 136380.1667 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #29: GFLOPs: 55.4419. Time: 18203.8195 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #30: GFLOPs: 60.8375. Time: 16589.3393 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #31: GFLOPs: 17.5996. Time: 57345.1667 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #32: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x0000000116b08273
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[13:20:08] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(28) * T.int64(16) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(28) // T.int64(14) * T.int64(16) + yy_3_init * T.int64(8) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(14) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(3), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(24)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(642), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(168) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(114))
                                        v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(168) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(114))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(3876))
                                        T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("self_rrdb_conv_first.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(168) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(168) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(168) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(576))
                                        T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(3), T.int64(1), T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(28) * T.int64(16) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(28) // T.int64(14) * T.int64(16) + yy_3 * T.int64(8) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(14) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(28) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(28) // T.int64(14) * T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(14) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 2, 8, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[20, 1, 2, 2, 8])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 8, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 3], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 3], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #33: GFLOPs: 41.6433. Time: 24235.7168 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #34: GFLOPs: 217.9821. Time: 4629.9881 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #35: GFLOPs: 16.1637. Time: 62439.5277 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #36: GFLOPs: 131.7928. Time: 7657.8869 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #37: GFLOPs: 15.3531. Time: 65736.1390 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #38: GFLOPs: 48.6727. Time: 20735.5500 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #39: GFLOPs: 11.1444. Time: 90561.8610 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #40: GFLOPs: 142.0998. Time: 7102.4361 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #41: GFLOPs: 24.6178. Time: 40996.9723 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #42: GFLOPs: 18.1061. Time: 55741.0970 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #43: GFLOPs: 46.0846. Time: 21900.0418 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #44: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x0000000105e627e3
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[13:20:29] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(128), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(35), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(16), T.int64(2), T.int64(8), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(10) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(56) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(8) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(196)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(35), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(35) + ax0_ax1_ax2_ax3_fused_1) // T.int64(2280))
                                    v2 = T.axis.spatial(T.int64(642), ry_0 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(35) + ax0_ax1_ax2_ax3_fused_1) % T.int64(2280) // T.int64(114))
                                    v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(35) + ax0_ax1_ax2_ax3_fused_1) % T.int64(114))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(35) + ax0_ax1_ax2_ax3_fused_1 < T.int64(6840))
                                    T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(35), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("self_rrdb_conv_first.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(70) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(70) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), ry_0)
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(70) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(35) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(576))
                                        T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(16), T.int64(2), T.int64(8), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(10) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(56) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(8) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 * T.int64(3) + rc_1 * T.int64(3) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(64), T.int64(2), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0, v1 = T.axis.remap("SS", [ax0, ax1])
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(20) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(10) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(7) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(56) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 16, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 2, 5, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 2, 7, 8, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 35], preserve_unit_iters=True)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 35, 2], preserve_unit_iters=True)
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #45: GFLOPs: 54.1579. Time: 18635.3888 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #46: GFLOPs: 48.4644. Time: 20824.6582 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #47: GFLOPs: 190.6333. Time: 5294.2198 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #48: GFLOPs: 53.4240. Time: 18891.4097 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #49: GFLOPs: 67.0561. Time: 15050.9047 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #50: GFLOPs: 165.0060. Time: 6116.4717 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #51: GFLOPs: 27.3464. Time: 36906.2500 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #52: GFLOPs: 42.6663. Time: 23654.5918 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #53: GFLOPs: 22.2869. Time: 45284.7360 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #54: GFLOPs: 26.5900. Time: 37956.0970 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #55: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x00000001149195f3
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:109
    int2 v__1 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) / int2(3, 3)) : ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) : ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 109
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[13:20:40] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: Fail to compile metal source:program_source:334:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
program_source:334:525: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__1 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) / int2(3, 3)) : ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) : ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:336:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__2 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:336:581: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__2 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:338:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__3 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:338:589: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__3 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:340:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__4 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:340:589: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__4 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:342:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__5 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:342:589: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__5 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:344:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__6 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:344:589: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__6 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:346:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__7 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:346:589: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__7 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:348:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__8 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:348:589: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__8 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:350:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__9 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:350:589: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__9 = ((((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) / int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(9, 9)) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) : ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(10), T.int64(8)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(40) + yy_3_init * T.int64(10) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + xx_3_init * T.int64(8) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(1), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(50)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(96) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1600))
                                        v2 = T.axis.spatial(T.int64(642), ry_0 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(96) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1600) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(96) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("self_rrdb_conv_first.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), ry_0)
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(4), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(10), T.int64(8)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(40) + yy_3 * T.int64(10) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + xx_3 * T.int64(8) + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 * T.int64(3) + rc_1 * T.int64(3) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(40), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(80) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(40) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 16, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 2, 2, 4, 10])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[56, 1, 1, 1, 8])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 3], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #56: GFLOPs: 34.4400. Time: 29304.7085 us. Best GFLOPs: 722.5245
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #57: GFLOPs: 724.9074. Time: 1392.2528 us. Best GFLOPs: 724.9074
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #58: GFLOPs: 134.5383. Time: 7501.6161 us. Best GFLOPs: 724.9074
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #59: GFLOPs: 47.7029. Time: 21157.0916 us. Best GFLOPs: 724.9074
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #60: GFLOPs: 46.4878. Time: 21710.0918 us. Best GFLOPs: 724.9074
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #61: GFLOPs: 28.7487. Time: 35106.1250 us. Best GFLOPs: 724.9074
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #62: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x0000000112660273
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[13:20:46] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(80), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(8), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(28) * T.int64(16) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(28) // T.int64(14) * T.int64(16) + yy_3_init * T.int64(8) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(14) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(3), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(24)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(642), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(168) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(114))
                                        v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(168) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(114))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(3876))
                                        T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(4)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(56), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("self_rrdb_conv_first.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(168) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(168) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(168) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(56) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(576))
                                        T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(8), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(8), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(28) * T.int64(16) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(28) // T.int64(14) * T.int64(16) + yy_3 * T.int64(8) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(14) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(16), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(28) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(28) // T.int64(14) * T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(14) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(14) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 2, 8, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[20, 1, 2, 2, 8])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 8, 14, 1, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 56, 3], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 56, 3], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #63: GFLOPs: 16.1301. Time: 62569.5557 us. Best GFLOPs: 724.9074
2023-05-18 13:20:48 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #64: GFLOPs: 29.9336. Time: 33716.4027 us. Best GFLOPs: 724.9074
2023-05-18 18:52:45 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 18:52:45 [INFO] [evolutionary_search.cc:715] Picked top 60 candidate(s) from database
2023-05-18 18:52:47 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 431 failure(s)
2023-05-18 18:52:47 [INFO] [evolutionary_search.cc:723] Sampled 21 candidate(s)
2023-05-18 18:52:51 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 87 failure(s)
2023-05-18 18:52:57 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 48 failure(s)
2023-05-18 18:53:03 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 43 failure(s)
2023-05-18 18:53:09 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 49 failure(s)
2023-05-18 18:53:11 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0206  1.0131  1.0036  1.0001  0.9962  0.9906  0.9782  0.9768  0.9722  0.9667  0.9637  0.9633  0.9623  0.9581  0.9543  0.9542
[17 : 32]:	0.9531  0.9522  0.9522  0.9493  0.9476  0.9455  0.9425  0.9423  0.9411  0.9386  0.9371  0.9367  0.9335  0.9329  0.9299  0.9299
[33 : 48]:	0.9295  0.9285  0.9259  0.9256  0.9244  0.9235  0.9205  0.9188  0.9179  0.9174  0.9168  0.9167  0.9159  0.9148  0.9141  0.9136
[49 : 64]:	0.9135  0.9103  0.9080  0.9069  0.9069  0.9062  0.9051  0.9048  0.9038  0.9038  0.9032  0.9028  0.9022  0.9020  0.9019  0.9002
2023-05-18 18:53:11 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 18:53:11 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #65: GFLOPs: 726.7571. Time: 1388.7095 us. Best GFLOPs: 726.7571
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #66: GFLOPs: 425.9275. Time: 2369.5450 us. Best GFLOPs: 726.7571
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #67: GFLOPs: 705.5539. Time: 1430.4426 us. Best GFLOPs: 726.7571
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #68: GFLOPs: 754.5943. Time: 1337.4794 us. Best GFLOPs: 754.5943
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #69: GFLOPs: 731.9176. Time: 1378.9181 us. Best GFLOPs: 754.5943
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #70: GFLOPs: 888.9981. Time: 1135.2717 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #71: GFLOPs: 221.6997. Time: 4552.3490 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #72: GFLOPs: 754.5704. Time: 1337.5219 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #73: GFLOPs: 656.5064. Time: 1537.3109 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #74: GFLOPs: 323.8609. Time: 3116.3204 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #75: GFLOPs: 631.6997. Time: 1597.6808 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #76: GFLOPs: 224.8098. Time: 4489.3698 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #77: GFLOPs: 711.3841. Time: 1418.7194 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #78: GFLOPs: 433.5515. Time: 2327.8765 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #79: GFLOPs: 701.0036. Time: 1439.7279 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #80: GFLOPs: 653.6838. Time: 1543.9489 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #81: GFLOPs: 747.7849. Time: 1349.6587 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #82: GFLOPs: 773.3404. Time: 1305.0584 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #83: GFLOPs: 746.3348. Time: 1352.2810 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #84: GFLOPs: 726.2882. Time: 1389.6060 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #85: GFLOPs: 584.9412. Time: 1725.3945 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #86: GFLOPs: 648.6098. Time: 1556.0271 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #87: GFLOPs: 724.6961. Time: 1392.6588 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #88: GFLOPs: 740.2072. Time: 1363.4756 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #89: GFLOPs: 821.5044. Time: 1228.5441 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #90: GFLOPs: 599.6363. Time: 1683.1109 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #91: GFLOPs: 634.1832. Time: 1591.4241 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #92: GFLOPs: 651.4228. Time: 1549.3078 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #93: GFLOPs: 377.2318. Time: 2675.4221 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #94: GFLOPs: 838.7340. Time: 1203.3068 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #95: GFLOPs: 705.7476. Time: 1430.0500 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #96: GFLOPs: 692.8022. Time: 1456.7714 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #97: GFLOPs: 858.3765. Time: 1175.7713 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #98: GFLOPs: 718.4439. Time: 1404.7783 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #99: GFLOPs: 669.0635. Time: 1508.4583 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #100: GFLOPs: 673.1407. Time: 1499.3216 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #101: GFLOPs: 802.5554. Time: 1257.5510 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #102: GFLOPs: 689.9995. Time: 1462.6886 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #103: GFLOPs: 631.4176. Time: 1598.3946 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #104: GFLOPs: 683.5769. Time: 1476.4313 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #105: GFLOPs: 261.3077. Time: 3862.3214 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #106: GFLOPs: 225.4492. Time: 4476.6376 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #107: GFLOPs: 634.8821. Time: 1589.6721 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #108: GFLOPs: 701.1296. Time: 1439.4692 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #109: GFLOPs: 633.7551. Time: 1592.4989 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #110: GFLOPs: 732.6438. Time: 1377.5513 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #111: GFLOPs: 642.7868. Time: 1570.1231 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #112: GFLOPs: 615.7704. Time: 1639.0110 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #113: GFLOPs: 353.0391. Time: 2858.7611 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #114: GFLOPs: 511.7221. Time: 1972.2705 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #115: GFLOPs: 614.8334. Time: 1641.5088 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #116: GFLOPs: 718.5946. Time: 1404.4837 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #117: GFLOPs: 829.3947. Time: 1216.8566 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #118: GFLOPs: 752.6028. Time: 1341.0187 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #119: GFLOPs: 771.7538. Time: 1307.7414 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #120: GFLOPs: 751.4613. Time: 1343.0557 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #121: GFLOPs: 404.1221. Time: 2497.3995 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #122: GFLOPs: 470.5769. Time: 2144.7173 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #123: GFLOPs: 826.2826. Time: 1221.4398 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #124: GFLOPs: 719.0265. Time: 1403.6401 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #125: GFLOPs: 713.6928. Time: 1414.1301 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #126: GFLOPs: 15.7894. Time: 63919.9167 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #127: GFLOPs: 26.1005. Time: 38667.9583 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #128: GFLOPs: 120.4625. Time: 8378.1632 us. Best GFLOPs: 888.9981
2023-05-18 18:54:57 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 18:54:57 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 18:54:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 389 failure(s)
2023-05-18 18:54:59 [INFO] [evolutionary_search.cc:723] Sampled 21 candidate(s)
2023-05-18 18:55:03 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 56 failure(s)
2023-05-18 18:55:09 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 46 failure(s)
2023-05-18 18:55:14 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 49 failure(s)
2023-05-18 18:55:20 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 45 failure(s)
2023-05-18 18:55:22 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0470  1.0469  1.0192  1.0192  1.0163  1.0103  0.9994  0.9942  0.9931  0.9921  0.9898  0.9883  0.9864  0.9858  0.9833  0.9790
[17 : 32]:	0.9782  0.9782  0.9768  0.9743  0.9742  0.9727  0.9719  0.9689  0.9646  0.9610  0.9575  0.9575  0.9542  0.9536  0.9536  0.9531
[33 : 48]:	0.9528  0.9522  0.9519  0.9519  0.9504  0.9499  0.9465  0.9450  0.9432  0.9423  0.9423  0.9422  0.9414  0.9413  0.9411  0.9411
[49 : 64]:	0.9410  0.9384  0.9358  0.9348  0.9338  0.9305  0.9293  0.9285  0.9285  0.9283  0.9279  0.9278  0.9253  0.9253  0.9242  0.9242
2023-05-18 18:55:22 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 18:55:22 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #129: GFLOPs: 528.9580. Time: 1908.0048 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #130: GFLOPs: 427.9354. Time: 2358.4271 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #131: GFLOPs: 667.0372. Time: 1513.0405 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #132: GFLOPs: 657.8589. Time: 1534.1502 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #133: GFLOPs: 383.6420. Time: 2630.7192 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #134: GFLOPs: 386.9465. Time: 2608.2530 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #135: GFLOPs: 377.1385. Time: 2676.0843 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #136: GFLOPs: 359.6051. Time: 2806.5633 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #137: GFLOPs: 233.3762. Time: 4324.5808 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #138: GFLOPs: 350.0095. Time: 2883.5061 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #139: GFLOPs: 428.8893. Time: 2353.1814 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #140: GFLOPs: 237.2082. Time: 4254.7196 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #141: GFLOPs: 428.9128. Time: 2353.0524 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #142: GFLOPs: 332.8904. Time: 3031.7917 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #143: GFLOPs: 717.1125. Time: 1407.3864 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #144: GFLOPs: 324.3283. Time: 3111.8296 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #145: GFLOPs: 219.4146. Time: 4599.7591 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #146: GFLOPs: 227.7732. Time: 4430.9621 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #147: GFLOPs: 684.8286. Time: 1473.7328 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #148: GFLOPs: 669.4392. Time: 1507.6118 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #149: GFLOPs: 220.7437. Time: 4572.0644 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #150: GFLOPs: 384.9077. Time: 2622.0684 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #151: GFLOPs: 832.3308. Time: 1212.5641 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #152: GFLOPs: 284.3778. Time: 3548.9921 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #153: GFLOPs: 334.7243. Time: 3015.1812 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #154: GFLOPs: 835.0902. Time: 1208.5573 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #155: GFLOPs: 232.6985. Time: 4337.1755 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #156: GFLOPs: 629.5667. Time: 1603.0936 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #157: GFLOPs: 677.2337. Time: 1490.2603 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #158: GFLOPs: 232.2675. Time: 4345.2235 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #159: GFLOPs: 223.6095. Time: 4513.4684 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #160: GFLOPs: 751.3659. Time: 1343.2263 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #161: GFLOPs: 298.3628. Time: 3382.6419 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #162: GFLOPs: 459.0218. Time: 2198.7069 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #163: GFLOPs: 298.2661. Time: 3383.7381 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #164: GFLOPs: 288.6944. Time: 3495.9266 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #165: GFLOPs: 561.4521. Time: 1797.5789 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #166: GFLOPs: 429.6941. Time: 2348.7742 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #167: GFLOPs: 660.7401. Time: 1527.4604 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #168: GFLOPs: 558.0165. Time: 1808.6462 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #169: GFLOPs: 827.3334. Time: 1219.8884 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #170: GFLOPs: 789.7464. Time: 1277.9474 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #171: GFLOPs: 439.1095. Time: 2298.4116 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #172: GFLOPs: 762.6463. Time: 1323.3584 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #173: GFLOPs: 688.8531. Time: 1465.1229 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #174: GFLOPs: 229.2449. Time: 4402.5167 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #175: GFLOPs: 828.9055. Time: 1217.5747 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #176: GFLOPs: 839.0690. Time: 1202.8265 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #177: GFLOPs: 514.7694. Time: 1960.5952 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #178: GFLOPs: 624.8928. Time: 1615.0840 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #179: GFLOPs: 423.4086. Time: 2383.6415 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #180: GFLOPs: 426.4228. Time: 2366.7925 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #181: GFLOPs: 427.1691. Time: 2362.6581 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #182: GFLOPs: 572.7674. Time: 1762.0669 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #183: GFLOPs: 755.5974. Time: 1335.7039 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #184: GFLOPs: 690.0489. Time: 1462.5839 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #185: GFLOPs: 631.9063. Time: 1597.1582 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #186: GFLOPs: 698.0183. Time: 1445.8853 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #187: GFLOPs: 266.3281. Time: 3789.5156 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #188: GFLOPs: 764.9146. Time: 1319.4340 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #189: GFLOPs: 684.0728. Time: 1475.3611 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #190: GFLOPs: 16.2788. Time: 61998.0140 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #191: GFLOPs: 17.8546. Time: 56526.3057 us. Best GFLOPs: 888.9981
2023-05-18 18:57:09 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #192: GFLOPs: 72.0462. Time: 14008.4243 us. Best GFLOPs: 888.9981
2023-05-18 18:57:35 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 18:57:35 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 18:57:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 392 failure(s)
2023-05-18 18:57:37 [INFO] [evolutionary_search.cc:723] Sampled 18 candidate(s)
2023-05-18 18:57:41 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 72 failure(s)
2023-05-18 18:57:47 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 40 failure(s)
2023-05-18 18:57:52 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 55 failure(s)
2023-05-18 18:57:57 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 51 failure(s)
2023-05-18 18:58:00 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0627  1.0547  1.0378  1.0332  1.0259  1.0211  1.0193  1.0104  1.0081  1.0070  1.0054  1.0001  0.9970  0.9870  0.9844  0.9841
[17 : 32]:	0.9840  0.9839  0.9833  0.9792  0.9755  0.9746  0.9741  0.9739  0.9732  0.9730  0.9722  0.9698  0.9688  0.9687  0.9678  0.9641
[33 : 48]:	0.9632  0.9632  0.9590  0.9577  0.9561  0.9554  0.9543  0.9531  0.9520  0.9503  0.9495  0.9486  0.9480  0.9473  0.9450  0.9433
[49 : 64]:	0.9423  0.9415  0.9415  0.9412  0.9410  0.9403  0.9399  0.9386  0.9372  0.9369  0.9361  0.9357  0.9353  0.9338  0.9306  0.9306
2023-05-18 18:58:00 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 18:58:00 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #193: GFLOPs: 392.1912. Time: 2573.3733 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #194: GFLOPs: 370.7676. Time: 2722.0675 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #195: GFLOPs: 401.5104. Time: 2513.6446 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #196: GFLOPs: 383.6915. Time: 2630.3797 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #197: GFLOPs: 456.8151. Time: 2209.3279 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #198: GFLOPs: 227.2332. Time: 4441.4924 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #199: GFLOPs: 316.9645. Time: 3184.1240 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #200: GFLOPs: 785.4120. Time: 1285.0000 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #201: GFLOPs: 568.4361. Time: 1775.4931 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #202: GFLOPs: 237.7353. Time: 4245.2854 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #203: GFLOPs: 525.1524. Time: 1921.8314 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #204: GFLOPs: 694.5257. Time: 1453.1564 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #205: GFLOPs: 228.2483. Time: 4421.7393 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #206: GFLOPs: 447.2760. Time: 2256.4467 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #207: GFLOPs: 348.6779. Time: 2894.5175 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #208: GFLOPs: 390.2467. Time: 2586.1957 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #209: GFLOPs: 685.3486. Time: 1472.6147 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #210: GFLOPs: 747.2958. Time: 1350.5420 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #211: GFLOPs: 690.5298. Time: 1461.5652 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #212: GFLOPs: 683.6356. Time: 1476.3046 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #213: GFLOPs: 384.1494. Time: 2627.2445 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #214: GFLOPs: 382.5185. Time: 2638.4458 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #215: GFLOPs: 554.6649. Time: 1819.5751 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #216: GFLOPs: 428.9437. Time: 2352.8832 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #217: GFLOPs: 355.9508. Time: 2835.3759 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #218: GFLOPs: 741.7334. Time: 1360.6701 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #219: GFLOPs: 654.3984. Time: 1542.2629 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #220: GFLOPs: 393.4025. Time: 2565.4497 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #221: GFLOPs: 447.7046. Time: 2254.2865 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #222: GFLOPs: 330.8503. Time: 3050.4864 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #223: GFLOPs: 354.0491. Time: 2850.6056 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #224: GFLOPs: 395.0380. Time: 2554.8288 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #225: GFLOPs: 345.3129. Time: 2922.7246 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #226: GFLOPs: 422.7362. Time: 2387.4332 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #227: GFLOPs: 399.5926. Time: 2525.7083 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #228: GFLOPs: 379.3811. Time: 2660.2655 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #229: GFLOPs: 392.6275. Time: 2570.5136 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #230: GFLOPs: 701.1967. Time: 1439.3315 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #231: GFLOPs: 713.1198. Time: 1415.2663 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #232: GFLOPs: 453.8399. Time: 2223.8113 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #233: GFLOPs: 677.9907. Time: 1488.5962 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #234: GFLOPs: 445.4802. Time: 2265.5425 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #235: GFLOPs: 455.3908. Time: 2216.2380 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #236: GFLOPs: 487.1455. Time: 2071.7721 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #237: GFLOPs: 232.5745. Time: 4339.4893 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #238: GFLOPs: 429.0213. Time: 2352.4575 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #239: GFLOPs: 552.3054. Time: 1827.3484 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #240: GFLOPs: 349.4665. Time: 2887.9858 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #241: GFLOPs: 744.5452. Time: 1355.5315 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #242: GFLOPs: 743.0852. Time: 1358.1947 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #243: GFLOPs: 794.6169. Time: 1270.1145 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #244: GFLOPs: 861.9406. Time: 1170.9095 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #245: GFLOPs: 536.3649. Time: 1881.6562 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #246: GFLOPs: 556.0836. Time: 1814.9329 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #247: GFLOPs: 792.7796. Time: 1273.0581 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #248: GFLOPs: 575.0991. Time: 1754.9225 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #249: GFLOPs: 438.6308. Time: 2300.9200 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #250: GFLOPs: 617.4821. Time: 1634.4675 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #251: GFLOPs: 387.1490. Time: 2606.8889 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #252: GFLOPs: 399.9583. Time: 2523.3993 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #253: GFLOPs: 355.2787. Time: 2840.7402 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #254: GFLOPs: 19.1996. Time: 52566.5137 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #255: GFLOPs: 82.5761. Time: 12222.1111 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #256: GFLOPs: 7.4162. Time: 136088.3473 us. Best GFLOPs: 888.9981
2023-05-18 18:59:05 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 18:59:06 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 18:59:08 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 391 failure(s)
2023-05-18 18:59:08 [INFO] [evolutionary_search.cc:723] Sampled 19 candidate(s)
2023-05-18 18:59:12 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 59 failure(s)
2023-05-18 18:59:17 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 48 failure(s)
2023-05-18 18:59:23 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 65 failure(s)
2023-05-18 18:59:28 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 65 failure(s)
2023-05-18 18:59:31 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0531  1.0227  0.9941  0.9940  0.9887  0.9797  0.9778  0.9777  0.9765  0.9758  0.9746  0.9739  0.9732  0.9720  0.9716  0.9693
[17 : 32]:	0.9644  0.9642  0.9610  0.9569  0.9560  0.9543  0.9543  0.9541  0.9531  0.9524  0.9514  0.9464  0.9451  0.9450  0.9449  0.9444
[33 : 48]:	0.9426  0.9423  0.9414  0.9414  0.9412  0.9412  0.9400  0.9388  0.9369  0.9330  0.9309  0.9307  0.9302  0.9300  0.9299  0.9298
[49 : 64]:	0.9297  0.9295  0.9278  0.9256  0.9255  0.9255  0.9253  0.9242  0.9235  0.9230  0.9228  0.9215  0.9206  0.9201  0.9189  0.9188
2023-05-18 18:59:31 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 18:59:31 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #257: GFLOPs: 311.5606. Time: 3239.3517 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #258: GFLOPs: 799.8475. Time: 1261.8085 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #259: GFLOPs: 345.5777. Time: 2920.4846 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #260: GFLOPs: 373.4656. Time: 2702.4028 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #261: GFLOPs: 701.7728. Time: 1438.1498 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #262: GFLOPs: 817.3888. Time: 1234.7300 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #263: GFLOPs: 574.4567. Time: 1756.8850 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #264: GFLOPs: 477.5316. Time: 2113.4819 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #265: GFLOPs: 426.7491. Time: 2364.9829 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #266: GFLOPs: 607.7134. Time: 1660.7407 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #267: GFLOPs: 569.0625. Time: 1773.5387 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #268: GFLOPs: 595.5520. Time: 1694.6537 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #269: GFLOPs: 566.0000. Time: 1783.1351 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #270: GFLOPs: 863.8107. Time: 1168.3745 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #271: GFLOPs: 350.9994. Time: 2875.3733 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #272: GFLOPs: 561.0520. Time: 1798.8607 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #273: GFLOPs: 455.0428. Time: 2217.9330 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #274: GFLOPs: 415.4732. Time: 2429.1682 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #275: GFLOPs: 527.8180. Time: 1912.1257 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #276: GFLOPs: 366.2865. Time: 2755.3692 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #277: GFLOPs: 581.2885. Time: 1736.2367 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #278: GFLOPs: 598.0389. Time: 1687.6065 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #279: GFLOPs: 716.3062. Time: 1408.9706 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #280: GFLOPs: 550.2913. Time: 1834.0367 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #281: GFLOPs: 747.9938. Time: 1349.2818 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #282: GFLOPs: 423.1816. Time: 2384.9201 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #283: GFLOPs: 165.5650. Time: 6095.8183 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #284: GFLOPs: 887.4390. Time: 1137.2662 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #285: GFLOPs: 547.6430. Time: 1842.9057 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #286: GFLOPs: 550.2031. Time: 1834.3306 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #287: GFLOPs: 751.5119. Time: 1342.9654 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #288: GFLOPs: 549.1404. Time: 1837.8805 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #289: GFLOPs: 867.1114. Time: 1163.9271 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #290: GFLOPs: 741.7870. Time: 1360.5716 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #291: GFLOPs: 791.3500. Time: 1275.3578 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #292: GFLOPs: 791.9052. Time: 1274.4637 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #293: GFLOPs: 857.5607. Time: 1176.8898 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #294: GFLOPs: 857.4305. Time: 1177.0685 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #295: GFLOPs: 714.5661. Time: 1412.4018 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #296: GFLOPs: 575.4270. Time: 1753.9225 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #297: GFLOPs: 609.3546. Time: 1656.2679 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #298: GFLOPs: 817.1460. Time: 1235.0968 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #299: GFLOPs: 750.8931. Time: 1344.0720 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #300: GFLOPs: 634.0109. Time: 1591.8566 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #301: GFLOPs: 559.2769. Time: 1804.5703 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #302: GFLOPs: 434.4478. Time: 2323.0742 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #303: GFLOPs: 640.1304. Time: 1576.6389 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #304: GFLOPs: 755.0460. Time: 1336.6793 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #305: GFLOPs: 609.7448. Time: 1655.2078 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #306: GFLOPs: 667.8669. Time: 1511.1611 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #307: GFLOPs: 784.8754. Time: 1285.8785 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #308: GFLOPs: 606.4925. Time: 1664.0839 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #309: GFLOPs: 619.4435. Time: 1629.2922 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #310: GFLOPs: 618.4074. Time: 1632.0219 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #311: GFLOPs: 667.2789. Time: 1512.4927 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #312: GFLOPs: 478.8905. Time: 2107.4849 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #313: GFLOPs: 659.1454. Time: 1531.1560 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #314: GFLOPs: 615.9455. Time: 1638.5450 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #315: GFLOPs: 576.7563. Time: 1749.8802 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #316: GFLOPs: 648.1166. Time: 1557.2110 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #317: GFLOPs: 467.0326. Time: 2160.9933 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #318: GFLOPs: 230.8201. Time: 4372.4710 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #319: GFLOPs: 25.2256. Time: 40009.1667 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #320: GFLOPs: 42.8640. Time: 23545.5250 us. Best GFLOPs: 888.9981
2023-05-18 19:00:32 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 19:00:33 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 19:00:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 395 failure(s)
2023-05-18 19:00:34 [INFO] [evolutionary_search.cc:723] Sampled 15 candidate(s)
2023-05-18 19:00:38 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 67 failure(s)
2023-05-18 19:00:44 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 55 failure(s)
2023-05-18 19:00:49 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 46 failure(s)
2023-05-18 19:00:55 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 50 failure(s)
2023-05-18 19:00:57 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0207  1.0118  0.9922  0.9922  0.9910  0.9906  0.9761  0.9665  0.9652  0.9562  0.9560  0.9559  0.9532  0.9507  0.9494  0.9493
[17 : 32]:	0.9489  0.9489  0.9487  0.9487  0.9486  0.9486  0.9465  0.9440  0.9424  0.9415  0.9413  0.9409  0.9409  0.9404  0.9387  0.9372
[33 : 48]:	0.9339  0.9310  0.9309  0.9306  0.9304  0.9295  0.9294  0.9294  0.9280  0.9280  0.9279  0.9266  0.9259  0.9250  0.9246  0.9245
[49 : 64]:	0.9245  0.9242  0.9229  0.9218  0.9212  0.9211  0.9206  0.9206  0.9201  0.9189  0.9188  0.9183  0.9174  0.9172  0.9171  0.9171
2023-05-18 19:00:57 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 19:00:57 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #321: GFLOPs: 347.5994. Time: 2903.4991 us. Best GFLOPs: 888.9981
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #322: GFLOPs: 813.2696. Time: 1240.9838 us. Best GFLOPs: 888.9981
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #323: GFLOPs: 880.7410. Time: 1145.9151 us. Best GFLOPs: 888.9981
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #324: GFLOPs: 898.4253. Time: 1123.3593 us. Best GFLOPs: 898.4253
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #325: GFLOPs: 804.2615. Time: 1254.8834 us. Best GFLOPs: 898.4253
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #326: GFLOPs: 870.7827. Time: 1159.0198 us. Best GFLOPs: 898.4253
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #327: GFLOPs: 355.3999. Time: 2839.7708 us. Best GFLOPs: 898.4253
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #328: GFLOPs: 684.6003. Time: 1474.2242 us. Best GFLOPs: 898.4253
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #329: GFLOPs: 394.3724. Time: 2559.1407 us. Best GFLOPs: 898.4253
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #330: GFLOPs: 950.0087. Time: 1062.3633 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #331: GFLOPs: 384.3820. Time: 2625.6544 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #332: GFLOPs: 660.9418. Time: 1526.9943 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #333: GFLOPs: 791.3937. Time: 1275.2874 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #334: GFLOPs: 749.3286. Time: 1346.8782 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #335: GFLOPs: 769.3491. Time: 1311.8288 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #336: GFLOPs: 693.0143. Time: 1456.3255 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #337: GFLOPs: 671.9951. Time: 1501.8777 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #338: GFLOPs: 660.1138. Time: 1528.9098 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #339: GFLOPs: 537.9160. Time: 1876.2304 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #340: GFLOPs: 534.1782. Time: 1889.3591 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #341: GFLOPs: 446.5703. Time: 2260.0121 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #342: GFLOPs: 455.2723. Time: 2216.8150 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #343: GFLOPs: 635.0465. Time: 1589.2605 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #344: GFLOPs: 570.2648. Time: 1769.7997 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #345: GFLOPs: 870.9291. Time: 1158.8250 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #346: GFLOPs: 745.6070. Time: 1353.6011 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #347: GFLOPs: 361.0775. Time: 2795.1185 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #348: GFLOPs: 463.0612. Time: 2179.5270 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #349: GFLOPs: 806.0412. Time: 1252.1127 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #350: GFLOPs: 661.2858. Time: 1526.2000 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #351: GFLOPs: 422.4603. Time: 2388.9921 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #352: GFLOPs: 829.3743. Time: 1216.8865 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #353: GFLOPs: 339.8818. Time: 2969.4280 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #354: GFLOPs: 866.7526. Time: 1164.4089 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #355: GFLOPs: 708.5099. Time: 1424.4746 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #356: GFLOPs: 666.0383. Time: 1515.3100 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #357: GFLOPs: 425.6067. Time: 2371.3312 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #358: GFLOPs: 840.4119. Time: 1200.9044 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #359: GFLOPs: 819.6880. Time: 1231.2665 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #360: GFLOPs: 817.9560. Time: 1233.8737 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #361: GFLOPs: 745.4449. Time: 1353.8954 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #362: GFLOPs: 845.8652. Time: 1193.1622 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #363: GFLOPs: 821.7804. Time: 1228.1315 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #364: GFLOPs: 921.7305. Time: 1094.9561 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #365: GFLOPs: 675.2469. Time: 1494.6450 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #366: GFLOPs: 672.7996. Time: 1500.0818 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #367: GFLOPs: 669.9136. Time: 1506.5442 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #368: GFLOPs: 840.0532. Time: 1201.4173 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #369: GFLOPs: 903.3377. Time: 1117.2504 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #370: GFLOPs: 721.8469. Time: 1398.1558 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #371: GFLOPs: 801.1463. Time: 1259.7629 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #372: GFLOPs: 572.5570. Time: 1762.7142 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #373: GFLOPs: 723.8875. Time: 1394.2145 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #374: GFLOPs: 356.1343. Time: 2833.9150 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #375: GFLOPs: 827.0445. Time: 1220.3145 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #376: GFLOPs: 737.2535. Time: 1368.9380 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #377: GFLOPs: 604.9659. Time: 1668.2832 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #378: GFLOPs: 745.5675. Time: 1353.6727 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #379: GFLOPs: 821.5043. Time: 1228.5442 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #380: GFLOPs: 550.1586. Time: 1834.4792 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #381: GFLOPs: 571.8220. Time: 1764.9801 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #382: GFLOPs: 586.3882. Time: 1721.1368 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #383: GFLOPs: 134.1164. Time: 7525.2114 us. Best GFLOPs: 950.0087
2023-05-18 19:01:57 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #384: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x0000000101cb3543
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[19:01:56] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(10), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(10) + yy_3_init * T.int64(10) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(4) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(3), T.int64(3), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(80)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(3), rc_0)
                                    v2 = T.axis.spatial(T.int64(642), ry_0 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) // T.int64(64))
                                    v3 = T.axis.spatial(T.int64(450), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1) % T.int64(64))
                                    T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                                with T.block("self_rrdb_conv_first.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1)
                                    v1, v2, v3 = T.axis.remap("SSS", [rc_0, ry_0, rx_0])
                                    T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                    T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                    self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(10), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(10) + yy_3 * T.int64(10) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(4) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(16), T.int64(10), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(7) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(40) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(10) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(7) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 4, 1, 8, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 2, 4, 1, 10])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[7, 1, 16, 1, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 64], preserve_unit_iters=True)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
2023-05-18 19:01:57 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 19:01:58 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 19:01:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 395 failure(s)
2023-05-18 19:01:59 [INFO] [evolutionary_search.cc:723] Sampled 15 candidate(s)
2023-05-18 19:02:03 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 54 failure(s)
2023-05-18 19:02:09 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 48 failure(s)
2023-05-18 19:02:14 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 45 failure(s)
2023-05-18 19:02:20 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 62 failure(s)
2023-05-18 19:02:22 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0239  1.0130  0.9968  0.9943  0.9910  0.9819  0.9751  0.9734  0.9723  0.9717  0.9690  0.9676  0.9667  0.9625  0.9615  0.9612
[17 : 32]:	0.9604  0.9580  0.9561  0.9552  0.9540  0.9532  0.9489  0.9483  0.9452  0.9450  0.9423  0.9400  0.9367  0.9354  0.9350  0.9337
[33 : 48]:	0.9329  0.9308  0.9304  0.9301  0.9300  0.9299  0.9298  0.9295  0.9294  0.9283  0.9277  0.9276  0.9276  0.9275  0.9273  0.9273
[49 : 64]:	0.9273  0.9244  0.9244  0.9244  0.9242  0.9227  0.9222  0.9222  0.9219  0.9206  0.9187  0.9186  0.9184  0.9179  0.9178  0.9171
2023-05-18 19:02:22 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 19:02:22 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #385: GFLOPs: 883.8572. Time: 1141.8750 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #386: GFLOPs: 891.6195. Time: 1131.9339 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #387: GFLOPs: 709.2119. Time: 1423.0647 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #388: GFLOPs: 858.3629. Time: 1175.7899 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #389: GFLOPs: 818.0698. Time: 1233.7020 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #390: GFLOPs: 868.3097. Time: 1162.3208 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #391: GFLOPs: 861.0667. Time: 1172.0978 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #392: GFLOPs: 884.1466. Time: 1141.5012 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #393: GFLOPs: 705.2954. Time: 1430.9670 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #394: GFLOPs: 487.2841. Time: 2071.1825 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #395: GFLOPs: 229.6235. Time: 4395.2571 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #396: GFLOPs: 831.2764. Time: 1214.1021 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #397: GFLOPs: 756.5686. Time: 1333.9892 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #398: GFLOPs: 901.6848. Time: 1119.2984 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #399: GFLOPs: 478.7680. Time: 2108.0238 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #400: GFLOPs: 438.2575. Time: 2302.8799 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #401: GFLOPs: 733.6890. Time: 1375.5889 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #402: GFLOPs: 415.0351. Time: 2431.7327 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #403: GFLOPs: 725.8176. Time: 1390.5068 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #404: GFLOPs: 856.3462. Time: 1178.5588 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #405: GFLOPs: 807.9296. Time: 1249.1860 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #406: GFLOPs: 790.9314. Time: 1276.0327 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #407: GFLOPs: 643.2633. Time: 1568.9600 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #408: GFLOPs: 664.2107. Time: 1519.4792 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #409: GFLOPs: 799.7336. Time: 1261.9882 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #410: GFLOPs: 793.3369. Time: 1272.1637 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #411: GFLOPs: 515.4789. Time: 1957.8965 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #412: GFLOPs: 744.0426. Time: 1356.4470 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #413: GFLOPs: 473.8643. Time: 2129.8384 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #414: GFLOPs: 842.7196. Time: 1197.6158 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #415: GFLOPs: 445.5753. Time: 2265.0593 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #416: GFLOPs: 477.4506. Time: 2113.8405 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #417: GFLOPs: 870.7884. Time: 1159.0122 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #418: GFLOPs: 535.2025. Time: 1885.7432 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #419: GFLOPs: 425.2889. Time: 2373.1033 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #420: GFLOPs: 740.0294. Time: 1363.8032 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #421: GFLOPs: 802.1752. Time: 1258.1471 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #422: GFLOPs: 722.4867. Time: 1396.9176 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #423: GFLOPs: 664.3026. Time: 1519.2692 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #424: GFLOPs: 842.9350. Time: 1197.3099 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #425: GFLOPs: 824.1806. Time: 1224.5549 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #426: GFLOPs: 662.6566. Time: 1523.0429 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #427: GFLOPs: 933.6794. Time: 1080.9432 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #428: GFLOPs: 556.1645. Time: 1814.6690 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #429: GFLOPs: 506.7449. Time: 1991.6421 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #430: GFLOPs: 514.0061. Time: 1963.5067 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #431: GFLOPs: 455.3476. Time: 2216.4485 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #432: GFLOPs: 456.7781. Time: 2209.5069 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #433: GFLOPs: 441.1743. Time: 2287.6545 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #434: GFLOPs: 947.3464. Time: 1065.3488 us. Best GFLOPs: 950.0087
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #435: GFLOPs: 965.3404. Time: 1045.4907 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #436: GFLOPs: 898.9916. Time: 1122.6517 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #437: GFLOPs: 699.9909. Time: 1441.8108 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #438: GFLOPs: 474.2120. Time: 2128.2767 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #439: GFLOPs: 583.1578. Time: 1730.6711 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #440: GFLOPs: 591.1754. Time: 1707.1997 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #441: GFLOPs: 593.4725. Time: 1700.5915 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #442: GFLOPs: 873.0873. Time: 1155.9605 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #443: GFLOPs: 695.1762. Time: 1451.7966 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #444: GFLOPs: 822.5595. Time: 1226.9682 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #445: GFLOPs: 425.6219. Time: 2371.2463 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #446: GFLOPs: 25.1001. Time: 40209.1803 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #447: GFLOPs: 80.3326. Time: 12563.4444 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #448: GFLOPs: 45.3608. Time: 22249.4668 us. Best GFLOPs: 965.3404
2023-05-18 19:03:24 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 19:03:24 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 19:03:26 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 388 failure(s)
2023-05-18 19:03:26 [INFO] [evolutionary_search.cc:723] Sampled 22 candidate(s)
2023-05-18 19:03:30 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 42 failure(s)
2023-05-18 19:03:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 42 failure(s)
2023-05-18 19:03:41 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 39 failure(s)
2023-05-18 19:03:46 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 66 failure(s)
2023-05-18 19:03:49 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0258  1.0224  1.0207  1.0138  1.0075  1.0037  0.9936  0.9936  0.9932  0.9869  0.9837  0.9837  0.9819  0.9714  0.9702  0.9649
[17 : 32]:	0.9638  0.9626  0.9624  0.9616  0.9611  0.9587  0.9568  0.9552  0.9544  0.9540  0.9524  0.9495  0.9493  0.9490  0.9477  0.9469
[33 : 48]:	0.9464  0.9457  0.9453  0.9452  0.9448  0.9445  0.9438  0.9421  0.9416  0.9416  0.9415  0.9415  0.9401  0.9395  0.9372  0.9356
[49 : 64]:	0.9339  0.9334  0.9322  0.9301  0.9256  0.9255  0.9249  0.9237  0.9236  0.9236  0.9227  0.9209  0.9209  0.9196  0.9191  0.9188
2023-05-18 19:03:49 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 19:03:49 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #449: GFLOPs: 378.9450. Time: 2663.3268 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #450: GFLOPs: 732.2423. Time: 1378.3065 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #451: GFLOPs: 816.8446. Time: 1235.5526 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #452: GFLOPs: 325.8782. Time: 3097.0298 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #453: GFLOPs: 861.0689. Time: 1172.0949 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #454: GFLOPs: 794.7543. Time: 1269.8948 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #455: GFLOPs: 712.2795. Time: 1416.9359 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #456: GFLOPs: 717.3954. Time: 1406.8314 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #457: GFLOPs: 751.8196. Time: 1342.4157 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #458: GFLOPs: 380.9489. Time: 2649.3170 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #459: GFLOPs: 371.6006. Time: 2715.9654 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #460: GFLOPs: 424.8541. Time: 2375.5316 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #461: GFLOPs: 847.9212. Time: 1190.2691 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #462: GFLOPs: 857.6018. Time: 1176.8333 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #463: GFLOPs: 749.1460. Time: 1347.2065 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #464: GFLOPs: 842.8044. Time: 1197.4955 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #465: GFLOPs: 795.0561. Time: 1269.4128 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #466: GFLOPs: 800.5019. Time: 1260.7770 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #467: GFLOPs: 865.4564. Time: 1166.1528 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #468: GFLOPs: 805.2319. Time: 1253.3711 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #469: GFLOPs: 817.9263. Time: 1233.9186 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #470: GFLOPs: 422.8964. Time: 2386.5288 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #471: GFLOPs: 377.6629. Time: 2672.3681 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #472: GFLOPs: 885.7461. Time: 1139.4398 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #473: GFLOPs: 798.2030. Time: 1264.4082 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #474: GFLOPs: 403.7909. Time: 2499.4481 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #475: GFLOPs: 882.5535. Time: 1143.5617 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #476: GFLOPs: 312.2555. Time: 3232.1424 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #477: GFLOPs: 759.3082. Time: 1329.1762 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #478: GFLOPs: 801.7558. Time: 1258.8053 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #479: GFLOPs: 637.0332. Time: 1584.3043 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #480: GFLOPs: 890.4180. Time: 1133.4614 us. Best GFLOPs: 965.3404
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #481: GFLOPs: 967.0957. Time: 1043.5931 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #482: GFLOPs: 806.7280. Time: 1251.0468 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #483: GFLOPs: 960.2901. Time: 1050.9891 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #484: GFLOPs: 807.4648. Time: 1249.9051 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #485: GFLOPs: 849.7345. Time: 1187.7292 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #486: GFLOPs: 876.4422. Time: 1151.5356 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #487: GFLOPs: 859.2594. Time: 1174.5631 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #488: GFLOPs: 723.2463. Time: 1395.4504 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #489: GFLOPs: 888.2280. Time: 1136.2560 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #490: GFLOPs: 893.2348. Time: 1129.8870 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #491: GFLOPs: 818.2297. Time: 1233.4609 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #492: GFLOPs: 612.3577. Time: 1648.1452 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #493: GFLOPs: 488.5955. Time: 2065.6237 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #494: GFLOPs: 560.1087. Time: 1801.8903 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #495: GFLOPs: 875.6881. Time: 1152.5273 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #496: GFLOPs: 550.4315. Time: 1833.5694 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #497: GFLOPs: 688.3106. Time: 1466.2776 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #498: GFLOPs: 686.9384. Time: 1469.2066 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #499: GFLOPs: 621.8078. Time: 1623.0970 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #500: GFLOPs: 816.9606. Time: 1235.3770 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #501: GFLOPs: 825.7531. Time: 1222.2229 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #502: GFLOPs: 845.4858. Time: 1193.6977 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #503: GFLOPs: 901.0249. Time: 1120.1183 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #504: GFLOPs: 761.9485. Time: 1324.5703 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #505: GFLOPs: 460.5170. Time: 2191.5682 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #506: GFLOPs: 480.7241. Time: 2099.4461 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #507: GFLOPs: 910.4057. Time: 1108.5765 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #508: GFLOPs: 814.1453. Time: 1239.6489 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #509: GFLOPs: 823.7588. Time: 1225.1819 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #510: GFLOPs: 74.9005. Time: 13474.6094 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #511: GFLOPs: 5.8436. Time: 172712.2640 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #512: GFLOPs: 6.7882. Time: 148676.9720 us. Best GFLOPs: 967.0957
2023-05-18 19:04:56 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 19:04:57 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 19:04:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 394 failure(s)
2023-05-18 19:04:59 [INFO] [evolutionary_search.cc:723] Sampled 16 candidate(s)
2023-05-18 19:05:03 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 49 failure(s)
2023-05-18 19:05:08 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 44 failure(s)
2023-05-18 19:05:13 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 58 failure(s)
2023-05-18 19:05:19 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 39 failure(s)
2023-05-18 19:05:21 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0194  1.0180  1.0063  1.0063  0.9922  0.9836  0.9819  0.9807  0.9798  0.9738  0.9729  0.9695  0.9678  0.9669  0.9659  0.9647
[17 : 32]:	0.9638  0.9573  0.9554  0.9554  0.9545  0.9532  0.9516  0.9508  0.9499  0.9475  0.9469  0.9469  0.9464  0.9456  0.9452  0.9444
[33 : 48]:	0.9436  0.9433  0.9423  0.9417  0.9416  0.9410  0.9397  0.9387  0.9373  0.9352  0.9318  0.9314  0.9298  0.9296  0.9294  0.9288
[49 : 64]:	0.9284  0.9281  0.9269  0.9259  0.9258  0.9255  0.9234  0.9227  0.9227  0.9225  0.9211  0.9205  0.9203  0.9200  0.9189  0.9188
2023-05-18 19:05:21 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 19:05:21 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #513: GFLOPs: 712.6940. Time: 1416.1118 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #514: GFLOPs: 453.0770. Time: 2227.5558 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #515: GFLOPs: 767.5776. Time: 1314.8565 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #516: GFLOPs: 788.0302. Time: 1280.7307 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #517: GFLOPs: 897.3431. Time: 1124.7141 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #518: GFLOPs: 794.0340. Time: 1271.0469 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #519: GFLOPs: 841.9474. Time: 1198.7144 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #520: GFLOPs: 801.9965. Time: 1258.4274 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #521: GFLOPs: 800.3035. Time: 1261.0896 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #522: GFLOPs: 789.7243. Time: 1277.9832 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #523: GFLOPs: 844.6936. Time: 1194.8172 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #524: GFLOPs: 540.3329. Time: 1867.8383 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #525: GFLOPs: 352.5342. Time: 2862.8551 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #526: GFLOPs: 600.3549. Time: 1681.0963 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #527: GFLOPs: 898.0040. Time: 1123.8863 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #528: GFLOPs: 817.3231. Time: 1234.8292 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #529: GFLOPs: 801.0217. Time: 1259.9588 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #530: GFLOPs: 951.3763. Time: 1060.8361 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #531: GFLOPs: 491.4487. Time: 2053.6311 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #532: GFLOPs: 462.3115. Time: 2183.0613 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #533: GFLOPs: 904.6035. Time: 1115.6871 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #534: GFLOPs: 793.5899. Time: 1271.7582 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #535: GFLOPs: 751.3518. Time: 1343.2514 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #536: GFLOPs: 804.9889. Time: 1253.7495 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #537: GFLOPs: 663.9525. Time: 1520.0701 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #538: GFLOPs: 351.1867. Time: 2873.8397 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #539: GFLOPs: 854.4342. Time: 1181.1962 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #540: GFLOPs: 876.1640. Time: 1151.9012 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #541: GFLOPs: 897.9984. Time: 1123.8933 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #542: GFLOPs: 754.0495. Time: 1338.4459 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #543: GFLOPs: 811.4734. Time: 1243.7307 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #544: GFLOPs: 816.1323. Time: 1236.6308 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #545: GFLOPs: 933.6604. Time: 1080.9651 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #546: GFLOPs: 890.6975. Time: 1133.1057 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #547: GFLOPs: 840.3349. Time: 1201.0144 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #548: GFLOPs: 824.9274. Time: 1223.4464 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #549: GFLOPs: 903.5306. Time: 1117.0119 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #550: GFLOPs: 538.0761. Time: 1875.6721 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #551: GFLOPs: 876.2099. Time: 1151.8408 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #552: GFLOPs: 635.3912. Time: 1588.3984 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #553: GFLOPs: 682.2492. Time: 1479.3047 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #554: GFLOPs: 712.3146. Time: 1416.8660 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #555: GFLOPs: 826.0472. Time: 1221.7878 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #556: GFLOPs: 763.1157. Time: 1322.5444 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #557: GFLOPs: 841.3997. Time: 1199.4946 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #558: GFLOPs: 557.1212. Time: 1811.5525 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #559: GFLOPs: 862.0299. Time: 1170.7881 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #560: GFLOPs: 730.3094. Time: 1381.9545 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #561: GFLOPs: 484.7172. Time: 2082.1511 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #562: GFLOPs: 443.6070. Time: 2275.1092 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #563: GFLOPs: 464.3341. Time: 2173.5523 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #564: GFLOPs: 783.2197. Time: 1288.5968 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #565: GFLOPs: 470.8031. Time: 2143.6868 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #566: GFLOPs: 950.7275. Time: 1061.5601 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #567: GFLOPs: 880.1290. Time: 1146.7119 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #568: GFLOPs: 935.1578. Time: 1079.2343 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #569: GFLOPs: 939.1280. Time: 1074.6718 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #570: GFLOPs: 474.8940. Time: 2125.2202 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #571: GFLOPs: 813.8850. Time: 1240.0455 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #572: GFLOPs: 557.7737. Time: 1809.4336 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #573: GFLOPs: 482.3595. Time: 2092.3281 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #574: GFLOPs: 125.6746. Time: 8030.6923 us. Best GFLOPs: 967.0957
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #575: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x000000011746e2a3
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:109
    int2 v__1 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) / int2(9, 9)) : ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) : ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 109
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[19:06:19] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: Fail to compile metal source:program_source:50:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
program_source:50:527: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__1 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) / int2(9, 9)) : ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) : ((int2(((((int)threadIdx) * 2))+(1*0), ((((int)threadIdx) * 2))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:52:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__2 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:52:583: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__2 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 64))+(1*0), (((((int)threadIdx) * 2) + 64))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:54:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__3 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:54:591: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__3 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 128))+(1*0), (((((int)threadIdx) * 2) + 128))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:56:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__4 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:56:591: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__4 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 192))+(1*0), (((((int)threadIdx) * 2) + 192))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:58:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__5 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:58:591: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__5 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 256))+(1*0), (((((int)threadIdx) * 2) + 256))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:60:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__6 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:60:591: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__6 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 320))+(1*0), (((((int)threadIdx) * 2) + 320))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:62:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__7 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:62:591: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__7 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 384))+(1*0), (((((int)threadIdx) * 2) + 384))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:64:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__8 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:64:591: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__8 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 448))+(1*0), (((((int)threadIdx) * 2) + 448))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:66:20: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__9 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                   ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
program_source:66:591: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
    int2 v__9 = ((((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) / int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) / int2(9, 9)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + ((((int2(9, 9) >= int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) >= int2(0, 0))) || ((int2(9, 9) < int2(0, 0)) && ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) <= int2(0, 0)))) ? (int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) : ((int2((((((int)threadIdx) * 2) + 512))+(1*0), (((((int)threadIdx) * 2) + 512))+(1*1)) % int2(9, 9)) + int2(9, 9)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(56), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(5), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(16), T.int64(16), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(16) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(3), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(44)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(642), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(34))
                                        v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(34))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(5508))
                                        T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("self_rrdb_conv_first.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(16), T.int64(16), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(16) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(32), T.int64(16)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 16, 4, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[4, 5, 1, 16, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 1, 2, 16, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2023-05-18 19:06:21 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #576: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x00000001131a35e3
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[19:06:20] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(32), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(56) * T.int64(32) + ff_3_init * T.int64(32) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(56) // T.int64(7) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(56) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(8) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(3), T.int64(3), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(64)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(3), rc_0)
                                    v2 = T.axis.spatial(T.int64(642), ry_0 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) // T.int64(112))
                                    v3 = T.axis.spatial(T.int64(450), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) % T.int64(112))
                                    T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                with T.block("self_rrdb_conv_first.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(64), ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1)
                                    v1, v2, v3 = T.axis.remap("SSS", [rc_0, ry_0, rx_0])
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1 < T.int64(64))
                                    T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                    T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                    self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(32), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(56) * T.int64(32) + ff_3 * T.int64(32) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(56) // T.int64(7) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(56) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(8) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(1), T.int64(8)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(56) * T.int64(32) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(56) // T.int64(7) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(56) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(8) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 2, 1, 32])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[10, 8, 8, 1, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 2, 7, 2, 4])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117 = sch.split(loop=l115, factors=[None, 112], preserve_unit_iters=True)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b118 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b118, ann_key="meta_schedule.unroll_explicit")
b119, b120, b121, b122 = sch.get_child_blocks(b118)
l123, l124, l125, l126, l127, l128, l129, l130 = sch.get_loops(block=b119)
l131, l132, l133, l134, l135, l136, l137, l138 = sch.get_loops(block=b120)
l139, l140, l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158 = sch.get_loops(block=b121)
sch.annotate(block_or_loop=l139, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l139, ann_key="pragma_unroll_explicit", ann_val=1)
l159, l160, l161, l162, l163, l164, l165 = sch.get_loops(block=b122)
b166 = sch.get_block(name="conv2d_nchw", func_name="main")
l167, l168, l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186 = sch.get_loops(block=b166)
b187 = sch.decompose_reduction(block=b166, loop=l170)
2023-05-18 19:06:21 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 19:06:21 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 19:06:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 397 failure(s)
2023-05-18 19:06:23 [INFO] [evolutionary_search.cc:723] Sampled 13 candidate(s)
2023-05-18 19:06:27 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 48 failure(s)
2023-05-18 19:06:32 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 48 failure(s)
2023-05-18 19:06:38 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 53 failure(s)
2023-05-18 19:06:43 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 47 failure(s)
2023-05-18 19:06:46 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0207  1.0116  1.0013  0.9973  0.9899  0.9870  0.9870  0.9848  0.9845  0.9842  0.9804  0.9778  0.9731  0.9729  0.9717  0.9695
[17 : 32]:	0.9677  0.9676  0.9676  0.9670  0.9645  0.9637  0.9623  0.9620  0.9611  0.9593  0.9588  0.9588  0.9523  0.9508  0.9492  0.9472
[33 : 48]:	0.9464  0.9463  0.9457  0.9456  0.9443  0.9433  0.9420  0.9405  0.9391  0.9389  0.9383  0.9382  0.9381  0.9370  0.9357  0.9357
[49 : 64]:	0.9354  0.9343  0.9329  0.9326  0.9325  0.9324  0.9314  0.9313  0.9310  0.9293  0.9287  0.9284  0.9264  0.9255  0.9255  0.9245
2023-05-18 19:06:46 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 19:06:46 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #577: GFLOPs: 868.2689. Time: 1162.3754 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #578: GFLOPs: 338.7495. Time: 2979.3528 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #579: GFLOPs: 490.4596. Time: 2057.7727 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #580: GFLOPs: 379.7668. Time: 2657.5633 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #581: GFLOPs: 473.9432. Time: 2129.4838 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #582: GFLOPs: 820.6565. Time: 1229.8134 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #583: GFLOPs: 799.8878. Time: 1261.7449 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #584: GFLOPs: 833.1282. Time: 1211.4035 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #585: GFLOPs: 439.9100. Time: 2294.2295 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #586: GFLOPs: 537.4069. Time: 1878.0079 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #587: GFLOPs: 856.7853. Time: 1177.9548 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #588: GFLOPs: 566.6873. Time: 1780.9722 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #589: GFLOPs: 783.6367. Time: 1287.9111 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #590: GFLOPs: 858.3361. Time: 1175.8266 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #591: GFLOPs: 487.1910. Time: 2071.5787 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #592: GFLOPs: 531.6217. Time: 1898.4446 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #593: GFLOPs: 444.6291. Time: 2269.8795 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #594: GFLOPs: 861.3384. Time: 1171.7281 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #595: GFLOPs: 839.4114. Time: 1202.3358 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #596: GFLOPs: 340.3893. Time: 2965.0000 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #597: GFLOPs: 552.1700. Time: 1827.7966 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #598: GFLOPs: 740.5615. Time: 1362.8232 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #599: GFLOPs: 853.7754. Time: 1182.1076 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #600: GFLOPs: 793.8536. Time: 1271.3356 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #601: GFLOPs: 824.5647. Time: 1223.9845 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #602: GFLOPs: 504.4725. Time: 2000.6133 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #603: GFLOPs: 558.6879. Time: 1806.4726 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #604: GFLOPs: 551.6746. Time: 1829.4377 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #605: GFLOPs: 852.5013. Time: 1183.8744 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #606: GFLOPs: 791.2082. Time: 1275.5863 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #607: GFLOPs: 456.5784. Time: 2210.4733 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #608: GFLOPs: 896.5465. Time: 1125.7133 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #609: GFLOPs: 901.2685. Time: 1119.8155 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #610: GFLOPs: 610.9738. Time: 1651.8782 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #611: GFLOPs: 814.9748. Time: 1238.3873 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #612: GFLOPs: 899.2936. Time: 1122.2747 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #613: GFLOPs: 876.5151. Time: 1151.4399 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #614: GFLOPs: 916.4810. Time: 1101.2279 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #615: GFLOPs: 551.3463. Time: 1830.5272 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #616: GFLOPs: 800.8361. Time: 1260.2509 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #617: GFLOPs: 891.9668. Time: 1131.4933 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #618: GFLOPs: 702.9493. Time: 1435.7428 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #619: GFLOPs: 748.6902. Time: 1348.0267 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #620: GFLOPs: 735.9465. Time: 1371.3693 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #621: GFLOPs: 479.2090. Time: 2106.0840 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #622: GFLOPs: 895.9524. Time: 1126.4598 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #623: GFLOPs: 536.2518. Time: 1882.0530 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #624: GFLOPs: 480.8722. Time: 2098.7995 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #625: GFLOPs: 489.7449. Time: 2060.7758 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #626: GFLOPs: 869.5665. Time: 1160.6408 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #627: GFLOPs: 605.8684. Time: 1665.7982 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #628: GFLOPs: 538.5259. Time: 1874.1056 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #629: GFLOPs: 268.2811. Time: 3761.9281 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #630: GFLOPs: 632.1309. Time: 1596.5909 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #631: GFLOPs: 848.8775. Time: 1188.9283 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #632: GFLOPs: 468.6266. Time: 2153.6432 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #633: GFLOPs: 871.4158. Time: 1158.1778 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #634: GFLOPs: 726.2091. Time: 1389.7574 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #635: GFLOPs: 442.6402. Time: 2280.0784 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #636: GFLOPs: 811.4916. Time: 1243.7028 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #637: GFLOPs: 881.7914. Time: 1144.5500 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #638: GFLOPs: 24.8667. Time: 40586.6110 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #639: GFLOPs: 123.7662. Time: 8154.5224 us. Best GFLOPs: 967.0957
2023-05-18 19:08:02 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #640: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x00000001154dc113
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[19:08:02] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(14)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(14) + xx_3_init * T.int64(14) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(3), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(30)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(642), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(114))
                                        v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(256) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(114))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(7524))
                                        T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(128), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("self_rrdb_conv_first.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(384) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(576))
                                        T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(14)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(14) + xx_3 * T.int64(14) + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(2), T.int64(14)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(14) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 8, 1, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[10, 2, 16, 2, 1])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 8, 1, 14])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 128, 2], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 128, 3], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l143, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l143, ann_key="pragma_unroll_explicit", ann_val=1)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2023-05-18 19:08:02 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 19:08:03 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 19:08:04 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 388 failure(s)
2023-05-18 19:08:04 [INFO] [evolutionary_search.cc:723] Sampled 22 candidate(s)
2023-05-18 19:08:08 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 53 failure(s)
2023-05-18 19:08:13 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 59 failure(s)
2023-05-18 19:08:19 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 67 failure(s)
2023-05-18 19:08:24 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 42 failure(s)
2023-05-18 19:08:27 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0075  1.0037  0.9974  0.9910  0.9875  0.9807  0.9807  0.9741  0.9740  0.9729  0.9723  0.9714  0.9691  0.9681  0.9677  0.9664
[17 : 32]:	0.9649  0.9624  0.9623  0.9623  0.9617  0.9604  0.9528  0.9525  0.9523  0.9522  0.9495  0.9488  0.9484  0.9466  0.9460  0.9457
[33 : 48]:	0.9456  0.9437  0.9432  0.9423  0.9408  0.9408  0.9397  0.9384  0.9370  0.9370  0.9367  0.9359  0.9357  0.9331  0.9319  0.9317
[49 : 64]:	0.9314  0.9307  0.9307  0.9304  0.9302  0.9301  0.9300  0.9297  0.9294  0.9285  0.9284  0.9282  0.9282  0.9264  0.9261  0.9261
2023-05-18 19:08:27 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 19:08:27 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #641: GFLOPs: 830.8028. Time: 1214.7941 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #642: GFLOPs: 827.1850. Time: 1220.1072 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #643: GFLOPs: 317.1980. Time: 3181.7806 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #644: GFLOPs: 821.1485. Time: 1229.0766 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #645: GFLOPs: 859.7544. Time: 1173.8869 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #646: GFLOPs: 738.7973. Time: 1366.0776 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #647: GFLOPs: 743.5736. Time: 1357.3026 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #648: GFLOPs: 374.5061. Time: 2694.8946 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #649: GFLOPs: 760.8238. Time: 1326.5284 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #650: GFLOPs: 858.8190. Time: 1175.1654 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #651: GFLOPs: 840.4552. Time: 1200.8426 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #652: GFLOPs: 861.1371. Time: 1172.0020 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #653: GFLOPs: 706.2421. Time: 1429.0489 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #654: GFLOPs: 534.8270. Time: 1887.0670 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #655: GFLOPs: 328.6740. Time: 3070.6849 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #656: GFLOPs: 825.9553. Time: 1221.9238 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #657: GFLOPs: 877.0684. Time: 1150.7135 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #658: GFLOPs: 765.0622. Time: 1319.1796 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #659: GFLOPs: 891.7061. Time: 1131.8240 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #660: GFLOPs: 897.8459. Time: 1124.0842 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #661: GFLOPs: 392.2033. Time: 2573.2942 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #662: GFLOPs: 150.6489. Time: 6699.3833 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #663: GFLOPs: 860.7574. Time: 1172.5190 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #664: GFLOPs: 755.7053. Time: 1335.5132 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #665: GFLOPs: 906.0177. Time: 1113.9455 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #666: GFLOPs: 694.6077. Time: 1452.9847 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #667: GFLOPs: 349.7262. Time: 2885.8411 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #668: GFLOPs: 488.5477. Time: 2065.8256 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #669: GFLOPs: 806.1067. Time: 1252.0109 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #670: GFLOPs: 769.6709. Time: 1311.2805 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #671: GFLOPs: 807.8966. Time: 1249.2371 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #672: GFLOPs: 732.9514. Time: 1376.9732 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #673: GFLOPs: 732.1964. Time: 1378.3931 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #674: GFLOPs: 495.9553. Time: 2034.9706 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #675: GFLOPs: 889.5096. Time: 1134.6188 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #676: GFLOPs: 899.0412. Time: 1122.5897 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #677: GFLOPs: 961.6540. Time: 1049.4985 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #678: GFLOPs: 896.3296. Time: 1125.9858 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #679: GFLOPs: 906.9496. Time: 1112.8010 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #680: GFLOPs: 888.5613. Time: 1135.8298 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #681: GFLOPs: 878.7664. Time: 1148.4900 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #682: GFLOPs: 348.3029. Time: 2897.6342 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #683: GFLOPs: 156.5889. Time: 6445.2500 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #684: GFLOPs: 441.6464. Time: 2285.2091 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #685: GFLOPs: 701.9184. Time: 1437.8515 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #686: GFLOPs: 327.2867. Time: 3083.7009 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #687: GFLOPs: 767.2401. Time: 1315.4349 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #688: GFLOPs: 332.6887. Time: 3033.6296 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #689: GFLOPs: 851.4610. Time: 1185.3208 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #690: GFLOPs: 624.9637. Time: 1614.9009 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #691: GFLOPs: 610.5760. Time: 1652.9545 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #692: GFLOPs: 813.7469. Time: 1240.2559 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #693: GFLOPs: 377.7808. Time: 2671.5342 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #694: GFLOPs: 797.6569. Time: 1265.2738 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #695: GFLOPs: 829.3287. Time: 1216.9534 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #696: GFLOPs: 339.9963. Time: 2968.4278 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #697: GFLOPs: 907.2689. Time: 1112.4093 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #698: GFLOPs: 296.7576. Time: 3400.9386 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #699: GFLOPs: 831.9693. Time: 1213.0909 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #700: GFLOPs: 805.8076. Time: 1252.4756 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #701: GFLOPs: 905.6299. Time: 1114.4226 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #702: GFLOPs: 115.3717. Time: 8747.8507 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #703: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x0000000105e03c33
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[19:09:25] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(8), T.int64(8)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(28) * T.int64(8) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(28) // T.int64(7) * T.int64(8) + yy_3_init * T.int64(8) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(16) + xx_3_init * T.int64(8) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(3), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(22)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(642), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(336) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(112))
                                        v3 = T.axis.spatial(T.int64(450), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(336) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(112))
                                        T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(112), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("self_rrdb_conv_first.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(336) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(336) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(112) + ax0_ax1_ax2_ax3_fused_1) * T.int64(3) + ax0_ax1_ax2_ax3_fused_2 < T.int64(192))
                                        T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(8), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(8), T.int64(8)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(28) * T.int64(8) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(28) // T.int64(7) * T.int64(8) + yy_3 * T.int64(8) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(16) + xx_3 * T.int64(8) + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(8), T.int64(16)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(28) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(4) * T.int64(64) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(28) // T.int64(7) * T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(4) * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(7) * T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 4, 8, 1])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[10, 2, 4, 1, 8])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 7, 2, 8])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 112, 3], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 112, 3], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141, l142 = sch.get_loops(block=b122)
l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162 = sch.get_loops(block=b123)
l163, l164, l165, l166, l167, l168, l169 = sch.get_loops(block=b124)
b170 = sch.get_block(name="conv2d_nchw", func_name="main")
l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190 = sch.get_loops(block=b170)
b191 = sch.decompose_reduction(block=b170, loop=l174)
2023-05-18 19:09:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #704: GFLOPs: 38.1768. Time: 26436.3332 us. Best GFLOPs: 967.0957
2023-05-18 19:09:27 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 19:09:27 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 19:09:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 387 failure(s)
2023-05-18 19:09:29 [INFO] [evolutionary_search.cc:723] Sampled 23 candidate(s)
2023-05-18 19:09:33 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 61 failure(s)
2023-05-18 19:09:38 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 50 failure(s)
2023-05-18 19:09:44 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 55 failure(s)
2023-05-18 19:09:49 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 50 failure(s)
2023-05-18 19:09:52 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0172  0.9910  0.9875  0.9875  0.9805  0.9804  0.9804  0.9797  0.9792  0.9779  0.9702  0.9678  0.9638  0.9635  0.9612  0.9578
[17 : 32]:	0.9565  0.9559  0.9528  0.9514  0.9512  0.9497  0.9493  0.9493  0.9478  0.9477  0.9472  0.9469  0.9469  0.9446  0.9409  0.9397
[33 : 48]:	0.9397  0.9384  0.9384  0.9383  0.9380  0.9380  0.9378  0.9370  0.9356  0.9347  0.9339  0.9317  0.9309  0.9288  0.9274  0.9268
[49 : 64]:	0.9264  0.9260  0.9246  0.9245  0.9243  0.9241  0.9236  0.9236  0.9234  0.9214  0.9211  0.9197  0.9196  0.9188  0.9183  0.9183
2023-05-18 19:09:52 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 19:09:52 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #705: GFLOPs: 753.0734. Time: 1340.1807 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #706: GFLOPs: 705.1023. Time: 1431.3589 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #707: GFLOPs: 839.6995. Time: 1201.9233 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #708: GFLOPs: 868.7175. Time: 1161.7752 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #709: GFLOPs: 691.2549. Time: 1460.0321 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #710: GFLOPs: 862.3773. Time: 1170.3166 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #711: GFLOPs: 863.3710. Time: 1168.9695 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #712: GFLOPs: 812.3315. Time: 1242.4170 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #713: GFLOPs: 738.7160. Time: 1366.2278 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #714: GFLOPs: 728.0677. Time: 1386.2095 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #715: GFLOPs: 754.6260. Time: 1337.4234 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #716: GFLOPs: 877.0709. Time: 1150.7102 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #717: GFLOPs: 621.5420. Time: 1623.7911 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #718: GFLOPs: 753.8902. Time: 1338.7287 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #719: GFLOPs: 815.3718. Time: 1237.7842 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #720: GFLOPs: 398.4925. Time: 2532.6812 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #721: GFLOPs: 536.8481. Time: 1879.9628 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #722: GFLOPs: 495.4496. Time: 2037.0476 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #723: GFLOPs: 821.0739. Time: 1229.1883 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #724: GFLOPs: 575.7859. Time: 1752.8292 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #725: GFLOPs: 330.0373. Time: 3058.0010 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #726: GFLOPs: 330.2347. Time: 3056.1732 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #727: GFLOPs: 809.2558. Time: 1247.1389 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #728: GFLOPs: 553.2141. Time: 1824.3467 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #729: GFLOPs: 782.0630. Time: 1290.5027 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #730: GFLOPs: 657.9729. Time: 1533.8844 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #731: GFLOPs: 895.8622. Time: 1126.5733 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #732: GFLOPs: 361.3381. Time: 2793.1025 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #733: GFLOPs: 496.6846. Time: 2031.9824 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #734: GFLOPs: 322.9258. Time: 3125.3442 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #735: GFLOPs: 405.3376. Time: 2489.9107 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #736: GFLOPs: 753.0670. Time: 1340.1920 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #737: GFLOPs: 755.1855. Time: 1336.4324 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #738: GFLOPs: 805.1372. Time: 1253.5186 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #739: GFLOPs: 742.5197. Time: 1359.2292 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #740: GFLOPs: 678.1494. Time: 1488.2479 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #741: GFLOPs: 901.7913. Time: 1119.1663 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #742: GFLOPs: 900.0366. Time: 1121.3482 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #743: GFLOPs: 534.4225. Time: 1888.4952 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #744: GFLOPs: 891.0725. Time: 1132.6289 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #745: GFLOPs: 336.8192. Time: 2996.4279 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #746: GFLOPs: 570.3329. Time: 1769.5882 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #747: GFLOPs: 536.0103. Time: 1882.9011 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #748: GFLOPs: 897.2534. Time: 1124.8264 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #749: GFLOPs: 706.3697. Time: 1428.7905 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #750: GFLOPs: 278.9404. Time: 3618.1719 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #751: GFLOPs: 514.6298. Time: 1961.1270 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #752: GFLOPs: 318.0708. Time: 3173.0494 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #753: GFLOPs: 840.9687. Time: 1200.1094 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #754: GFLOPs: 446.9573. Time: 2258.0553 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #755: GFLOPs: 541.9806. Time: 1862.1595 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #756: GFLOPs: 959.7989. Time: 1051.5270 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #757: GFLOPs: 576.1844. Time: 1751.6172 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #758: GFLOPs: 883.6275. Time: 1142.1718 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #759: GFLOPs: 303.4916. Time: 3325.4771 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #760: GFLOPs: 520.5787. Time: 1938.7162 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #761: GFLOPs: 825.4839. Time: 1222.6216 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #762: GFLOPs: 327.5303. Time: 3081.4076 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #763: GFLOPs: 493.4120. Time: 2045.4597 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #764: GFLOPs: 749.4860. Time: 1346.5954 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #765: GFLOPs: 869.9313. Time: 1160.1542 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #766: GFLOPs: 11.0892. Time: 91012.7500 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #767: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x00000001176365b3
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[19:10:56] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 512, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(112), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(16)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(14) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(10) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(10) * T.int64(2) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(224) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(14) * T.int64(16) + xx_3_init * T.int64(16) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(3), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(125)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(3), rc_0)
                                    v2 = T.axis.spatial(T.int64(642), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(20) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1) // T.int64(226))
                                    v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(224) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1) % T.int64(226))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1 < T.int64(4972))
                                    T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(8)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("self_rrdb_conv_first.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(9))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(80) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(576))
                                        T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(16)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(14) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(10) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(10) * T.int64(2) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(224) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(14) * T.int64(16) + xx_3 * T.int64(16) + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(16)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused // T.int64(14) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(10) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(2) * T.int64(20) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(10) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(2) * T.int64(224) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(14) * T.int64(16) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 8, 4, 1, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[32, 1, 10, 1, 2])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[2, 14, 1, 1, 16])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=3)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 40, 2], preserve_unit_iters=True)
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=512)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
2023-05-18 19:10:57 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #768: GFLOPs: 11.0974. Time: 90945.0970 us. Best GFLOPs: 967.0957
2023-05-18 19:10:57 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 19:10:58 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 19:11:00 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 391 failure(s)
2023-05-18 19:11:00 [INFO] [evolutionary_search.cc:723] Sampled 19 candidate(s)
2023-05-18 19:11:04 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 54 failure(s)
2023-05-18 19:11:09 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 61 failure(s)
2023-05-18 19:11:15 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 44 failure(s)
2023-05-18 19:11:20 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 59 failure(s)
2023-05-18 19:11:22 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0318  1.0318  1.0075  1.0063  1.0059  1.0017  1.0017  0.9977  0.9963  0.9792  0.9792  0.9790  0.9785  0.9672  0.9623  0.9612
[17 : 32]:	0.9591  0.9569  0.9555  0.9535  0.9531  0.9524  0.9490  0.9490  0.9488  0.9488  0.9472  0.9460  0.9451  0.9422  0.9420  0.9410
[33 : 48]:	0.9408  0.9397  0.9389  0.9387  0.9382  0.9380  0.9378  0.9378  0.9363  0.9351  0.9347  0.9337  0.9337  0.9309  0.9306  0.9264
[49 : 64]:	0.9260  0.9255  0.9246  0.9246  0.9244  0.9233  0.9215  0.9211  0.9209  0.9202  0.9196  0.9189  0.9183  0.9182  0.9177  0.9177
2023-05-18 19:11:22 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 19:11:22 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #769: GFLOPs: 755.4394. Time: 1335.9832 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #770: GFLOPs: 761.4595. Time: 1325.4209 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #771: GFLOPs: 853.8055. Time: 1182.0659 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #772: GFLOPs: 822.0776. Time: 1227.6875 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #773: GFLOPs: 435.9250. Time: 2315.2019 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #774: GFLOPs: 814.3906. Time: 1239.2756 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #775: GFLOPs: 797.8509. Time: 1264.9662 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #776: GFLOPs: 789.2125. Time: 1278.8120 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #777: GFLOPs: 872.9127. Time: 1156.1917 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #778: GFLOPs: 743.1618. Time: 1358.0548 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #779: GFLOPs: 754.8343. Time: 1337.0543 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #780: GFLOPs: 815.1142. Time: 1238.1754 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #781: GFLOPs: 762.2716. Time: 1324.0090 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #782: GFLOPs: 747.1824. Time: 1350.7470 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #783: GFLOPs: 896.0152. Time: 1126.3809 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #784: GFLOPs: 814.4061. Time: 1239.2521 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #785: GFLOPs: 380.2317. Time: 2654.3142 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #786: GFLOPs: 847.4440. Time: 1190.9394 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #787: GFLOPs: 605.5711. Time: 1666.6158 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #788: GFLOPs: 700.5736. Time: 1440.6116 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #789: GFLOPs: 758.5904. Time: 1330.4339 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #790: GFLOPs: 935.7481. Time: 1078.5535 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #791: GFLOPs: 813.3818. Time: 1240.8127 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #792: GFLOPs: 815.3049. Time: 1237.8858 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #793: GFLOPs: 354.9242. Time: 2843.5770 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #794: GFLOPs: 401.6223. Time: 2512.9438 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #795: GFLOPs: 857.1301. Time: 1177.4810 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #796: GFLOPs: 730.9189. Time: 1380.8021 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #797: GFLOPs: 379.0269. Time: 2662.7516 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #798: GFLOPs: 831.7928. Time: 1213.3483 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #799: GFLOPs: 504.8142. Time: 1999.2592 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #800: GFLOPs: 815.0298. Time: 1238.3037 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #801: GFLOPs: 932.0425. Time: 1082.8417 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #802: GFLOPs: 925.8961. Time: 1090.0298 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #803: GFLOPs: 687.5971. Time: 1467.7990 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #804: GFLOPs: 839.2411. Time: 1202.5798 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #805: GFLOPs: 848.7162. Time: 1189.1541 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #806: GFLOPs: 888.4446. Time: 1135.9790 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #807: GFLOPs: 494.0684. Time: 2042.7422 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #808: GFLOPs: 369.7219. Time: 2729.7665 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #809: GFLOPs: 855.5752. Time: 1179.6210 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #810: GFLOPs: 745.3496. Time: 1354.0685 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #811: GFLOPs: 483.0225. Time: 2089.4561 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #812: GFLOPs: 812.2005. Time: 1242.6173 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #813: GFLOPs: 402.1287. Time: 2509.7799 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #814: GFLOPs: 818.4211. Time: 1233.1725 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #815: GFLOPs: 818.4762. Time: 1233.0895 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #816: GFLOPs: 877.6150. Time: 1149.9968 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #817: GFLOPs: 451.3746. Time: 2235.9575 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #818: GFLOPs: 959.7517. Time: 1051.5786 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #819: GFLOPs: 484.0020. Time: 2085.2277 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #820: GFLOPs: 871.9848. Time: 1157.4221 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #821: GFLOPs: 731.7188. Time: 1379.2926 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #822: GFLOPs: 504.7254. Time: 1999.6109 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #823: GFLOPs: 545.3569. Time: 1850.6311 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #824: GFLOPs: 382.7651. Time: 2636.7461 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #825: GFLOPs: 867.2222. Time: 1163.7783 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #826: GFLOPs: 373.1429. Time: 2704.7398 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #827: GFLOPs: 533.4998. Time: 1891.7615 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #828: GFLOPs: 895.5101. Time: 1127.0162 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #829: GFLOPs: 584.9039. Time: 1725.5046 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #830: GFLOPs: 78.3482. Time: 12881.6511 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #831: GFLOPs: 41.3296. Time: 24419.6500 us. Best GFLOPs: 967.0957
2023-05-18 19:12:23 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #832: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x0000000106590073
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:109
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 109
        int2 v__1 = (((((((int2(3, 3) >= int2(0, 0)) && ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) / int2(3, 3)) : ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) : ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                        ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[19:12:22] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: Fail to compile metal source:program_source:36:25: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
program_source:36:860: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
        int2 v__1 = (((((((int2(3, 3) >= int2(0, 0)) && ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) / int2(3, 3)) : ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) / int2(3, 3)) - int2(1, 1))) * int2(27, 27)) + int2((rc_0 * 9), (rc_0 * 9))) + int2((ry_0 * 3), (ry_0 * 3))) + ((((int2(3, 3) >= int2(0, 0)) && ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) >= int2(0, 0))) || ((int2(3, 3) < int2(0, 0)) && ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) <= int2(0, 0)))) ? (int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) : ((int2((((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*0), (((ax0_ax1_ax2_ax3_fused_0_1 * 64) + (((int)threadIdx) * 2)))+(1*1)) % int2(3, 3)) + int2(3, 3)));
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~



# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(40), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(4), T.int64(4), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + yy_3_init * T.int64(4) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(3), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(85)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(3), rc_0)
                                    v2 = T.axis.spatial(T.int64(642), ry_0 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(80) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(34))
                                    v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) * T.int64(32) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(34))
                                    T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("self_rrdb_conv_first.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3))
                                        v1, v2 = T.axis.remap("SS", [rc_0, ry_0])
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(4), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + yy_3 * T.int64(4) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(8), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(64), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(14) * T.int64(80) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(8) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(8) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(14) * T.int64(32) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(8) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 16, 1, 4])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[8, 5, 2, 2, 4])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[14, 8, 1, 4, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108 = sch.split(loop=l106, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l109, l110, l111, l112, l113, l114, l115 = sch.get_loops(block=b86)
l116, l117, l118 = sch.split(loop=l115, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l118)
sch.bind(loop=l117, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131 = sch.get_loops(block=b120)
l132, l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
2023-05-18 19:12:23 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 19:12:23 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 19:12:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 392 failure(s)
2023-05-18 19:12:25 [INFO] [evolutionary_search.cc:723] Sampled 18 candidate(s)
2023-05-18 19:12:29 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 60 failure(s)
2023-05-18 19:12:35 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 62 failure(s)
2023-05-18 19:12:40 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 51 failure(s)
2023-05-18 19:12:46 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 53 failure(s)
2023-05-18 19:12:48 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0318  1.0172  1.0003  0.9854  0.9835  0.9805  0.9793  0.9667  0.9645  0.9638  0.9638  0.9623  0.9612  0.9612  0.9587  0.9569
[17 : 32]:	0.9569  0.9545  0.9512  0.9497  0.9493  0.9482  0.9465  0.9465  0.9460  0.9408  0.9400  0.9378  0.9356  0.9342  0.9334  0.9310
[33 : 48]:	0.9255  0.9248  0.9232  0.9211  0.9206  0.9193  0.9188  0.9185  0.9183  0.9171  0.9170  0.9160  0.9158  0.9154  0.9153  0.9152
[49 : 64]:	0.9150  0.9147  0.9145  0.9141  0.9139  0.9138  0.9137  0.9135  0.9120  0.9118  0.9110  0.9103  0.9097  0.9095  0.9093  0.9093
2023-05-18 19:12:48 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 19:12:48 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #833: GFLOPs: 758.3392. Time: 1330.8745 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #834: GFLOPs: 765.4146. Time: 1318.5721 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #835: GFLOPs: 811.3473. Time: 1243.9240 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #836: GFLOPs: 572.7440. Time: 1762.1389 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #837: GFLOPs: 804.5400. Time: 1254.4490 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #838: GFLOPs: 702.7604. Time: 1436.1288 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #839: GFLOPs: 828.7170. Time: 1217.8516 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #840: GFLOPs: 560.5593. Time: 1800.4419 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #841: GFLOPs: 573.2807. Time: 1760.4891 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #842: GFLOPs: 389.9851. Time: 2587.9308 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #843: GFLOPs: 767.6986. Time: 1314.6492 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #844: GFLOPs: 893.4475. Time: 1129.6180 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #845: GFLOPs: 809.1230. Time: 1247.3435 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #846: GFLOPs: 810.0635. Time: 1245.8954 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #847: GFLOPs: 337.1650. Time: 2993.3547 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #848: GFLOPs: 897.1018. Time: 1125.0166 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #849: GFLOPs: 873.4527. Time: 1155.4769 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #850: GFLOPs: 379.4344. Time: 2659.8917 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #851: GFLOPs: 392.1508. Time: 2573.6384 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #852: GFLOPs: 359.9411. Time: 2803.9435 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #853: GFLOPs: 759.0115. Time: 1329.6958 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #854: GFLOPs: 556.8327. Time: 1812.4911 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #855: GFLOPs: 437.7768. Time: 2305.4085 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #856: GFLOPs: 436.8781. Time: 2310.1508 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #857: GFLOPs: 605.0417. Time: 1668.0741 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #858: GFLOPs: 949.7301. Time: 1062.6750 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #859: GFLOPs: 618.6413. Time: 1631.4048 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #860: GFLOPs: 535.3962. Time: 1885.0610 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #861: GFLOPs: 442.0494. Time: 2283.1257 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #862: GFLOPs: 736.7622. Time: 1369.8509 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #863: GFLOPs: 704.0227. Time: 1433.5538 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #864: GFLOPs: 563.7607. Time: 1790.2176 us. Best GFLOPs: 967.0957
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #865: GFLOPs: 969.4070. Time: 1041.1049 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #866: GFLOPs: 256.8813. Time: 3928.8739 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #867: GFLOPs: 587.7816. Time: 1717.0569 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #868: GFLOPs: 236.9724. Time: 4258.9536 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #869: GFLOPs: 581.2519. Time: 1736.3460 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #870: GFLOPs: 292.8331. Time: 3446.5177 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #871: GFLOPs: 570.1637. Time: 1770.1134 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #872: GFLOPs: 683.1024. Time: 1477.4569 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #873: GFLOPs: 182.1465. Time: 5540.8951 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #874: GFLOPs: 890.3671. Time: 1133.5262 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #875: GFLOPs: 890.8200. Time: 1132.9498 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #876: GFLOPs: 877.2530. Time: 1150.4713 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #877: GFLOPs: 809.4707. Time: 1246.8078 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #878: GFLOPs: 875.5671. Time: 1152.6866 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #879: GFLOPs: 639.4073. Time: 1578.4219 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #880: GFLOPs: 244.1285. Time: 4134.1119 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #881: GFLOPs: 893.6030. Time: 1129.4214 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #882: GFLOPs: 419.2264. Time: 2407.4208 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #883: GFLOPs: 168.7919. Time: 5979.2833 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #884: GFLOPs: 348.6987. Time: 2894.3450 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #885: GFLOPs: 621.4046. Time: 1624.1502 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #886: GFLOPs: 941.4624. Time: 1072.0072 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #887: GFLOPs: 622.0919. Time: 1622.3558 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #888: GFLOPs: 857.4524. Time: 1177.0384 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #889: GFLOPs: 644.1793. Time: 1566.7289 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #890: GFLOPs: 893.8094. Time: 1129.1606 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #891: GFLOPs: 908.3247. Time: 1111.1163 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #892: GFLOPs: 406.5693. Time: 2482.3678 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #893: GFLOPs: 661.9576. Time: 1524.6510 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #894: GFLOPs: 183.1427. Time: 5510.7546 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #895: GFLOPs: 12.9679. Time: 77827.0137 us. Best GFLOPs: 969.4070
2023-05-18 19:13:50 [INFO] [task_scheduler.cc:121] [Task #21: fused_conv2d_add] Trial #896: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x00000001134f7613
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[19:13:49] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(inp_0: T.Buffer((T.int64(1), T.int64(3), T.int64(640), T.int64(448)), "float32"), self_rrdb_conv_first_weight: T.Buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), "float32"), lv1: T.Buffer((T.int64(1), T.int64(64), T.int64(1), T.int64(1)), "float32"), var_T_add_intermediate: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(3), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_conv_first_weight_shared = T.alloc_buffer((T.int64(64), T.int64(3), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(112), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(2), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(4), T.int64(1), T.int64(2), T.int64(32), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(10) * T.int64(8) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_2_ff_2_yy_2_xx_2_fused % T.int64(10) * T.int64(64) + yy_3_init * T.int64(32) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused * T.int64(4) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(3), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(24)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(3), rc_0)
                                        v2 = T.axis.spatial(T.int64(642), ry_0 + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(6))
                                        v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(160) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6))
                                        T.reads(inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), inp_0[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(5)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(40), thread="threadIdx.x"):
                                with T.block("self_rrdb_conv_first.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(64), (ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3))
                                    v1, v2 = T.axis.remap("SS", [rc_0, ry_0])
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.where(ax0_ax1_ax2_ax3_fused_0 * T.int64(40) + ax0_ax1_ax2_ax3_fused_1 < T.int64(192))
                                    T.reads(self_rrdb_conv_first_weight[v0, v1, v2, v3])
                                    T.writes(self_rrdb_conv_first_weight_shared[v0, v1, v2, v3])
                                    self_rrdb_conv_first_weight_shared[v0, v1, v2, v3] = self_rrdb_conv_first_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(2), T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(32), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(10) * T.int64(8) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_2_ff_2_yy_2_xx_2_fused % T.int64(10) * T.int64(64) + yy_3 * T.int64(32) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused * T.int64(4) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(3), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_conv_first_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(64), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(64), nn_1_ff_1_yy_1_xx_1_fused * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(10) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_2_ff_2_yy_2_xx_2_fused % T.int64(10) * T.int64(64) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv1[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_T_add_intermediate[v0, v1, v2, v3])
                            var_T_add_intermediate[v0, v1, v2, v3] = var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv1[v0, v1, T.int64(0), T.int64(0)]
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l4, l5, l6, l7, l8, l9, l10 = sch.get_loops(block=b1)
v11, v12, v13, v14, v15 = sch.sample_perfect_tile(loop=l4, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l16, l17, l18, l19, l20 = sch.split(loop=l4, factors=[v11, v12, v13, v14, v15], preserve_unit_iters=True)
v21, v22, v23, v24, v25 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 2, 4, 4, 2])
l26, l27, l28, l29, l30 = sch.split(loop=l5, factors=[v21, v22, v23, v24, v25], preserve_unit_iters=True)
v31, v32, v33, v34, v35 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 10, 2, 32])
l36, l37, l38, l39, l40 = sch.split(loop=l6, factors=[v31, v32, v33, v34, v35], preserve_unit_iters=True)
v41, v42, v43, v44, v45 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[112, 1, 1, 4, 1])
l46, l47, l48, l49, l50 = sch.split(loop=l7, factors=[v41, v42, v43, v44, v45], preserve_unit_iters=True)
v51, v52, v53 = sch.sample_perfect_tile(loop=l8, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l54, l55, l56 = sch.split(loop=l8, factors=[v51, v52, v53], preserve_unit_iters=True)
v57, v58, v59 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l60, l61, l62 = sch.split(loop=l9, factors=[v57, v58, v59], preserve_unit_iters=True)
v63, v64, v65 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l66, l67, l68 = sch.split(loop=l10, factors=[v63, v64, v65], preserve_unit_iters=True)
sch.reorder(l16, l26, l36, l46, l17, l27, l37, l47, l18, l28, l38, l48, l54, l60, l66, l55, l61, l67, l19, l29, l39, l49, l56, l62, l68, l20, l30, l40, l50)
l69 = sch.fuse(l16, l26, l36, l46, preserve_unit_iters=True)
sch.bind(loop=l69, thread_axis="blockIdx.x")
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="vthread.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b72 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b72, loop=l71, preserve_unit_loops=True, index=-1)
b73 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b73, loop=l66, preserve_unit_loops=True, index=-1)
l74, l75, l76, l77, l78, l79, l80, l81, l82, l83 = sch.get_loops(block=b73)
l84 = sch.fuse(l80, l81, l82, l83, preserve_unit_iters=True)
v85 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch", ann_val=v85)
b86 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b86, loop=l66, preserve_unit_loops=True, index=-1)
l87, l88, l89, l90, l91, l92, l93, l94, l95, l96 = sch.get_loops(block=b86)
l97 = sch.fuse(l93, l94, l95, l96, preserve_unit_iters=True)
v98 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch", ann_val=v98)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v99 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b3, ann_key="meta_schedule.unroll_explicit", ann_val=v99)
sch.enter_postproc()
sch.unannotate(block_or_loop=b73, ann_key="meta_schedule.cooperative_fetch")
l100, l101, l102, l103, l104, l105, l106 = sch.get_loops(block=b73)
l107, l108, l109 = sch.split(loop=l106, factors=[None, 40, 4], preserve_unit_iters=True)
sch.vectorize(loop=l109)
sch.bind(loop=l108, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b86, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b86)
l117, l118 = sch.split(loop=l116, factors=[None, 40], preserve_unit_iters=True)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b119 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b119, ann_key="meta_schedule.unroll_explicit")
b120, b121, b122, b123 = sch.get_child_blocks(b119)
l124, l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b120)
l133, l134, l135, l136, l137, l138, l139, l140 = sch.get_loops(block=b121)
l141, l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160 = sch.get_loops(block=b122)
sch.annotate(block_or_loop=l141, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l141, ann_key="pragma_unroll_explicit", ann_val=1)
l161, l162, l163, l164, l165, l166, l167 = sch.get_loops(block=b123)
b168 = sch.get_block(name="conv2d_nchw", func_name="main")
l169, l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188 = sch.get_loops(block=b168)
b189 = sch.decompose_reduction(block=b168, loop=l172)
2023-05-18 19:13:50 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 19:13:50 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 19:13:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 398 failure(s)
2023-05-18 19:13:52 [INFO] [evolutionary_search.cc:723] Sampled 12 candidate(s)
2023-05-18 19:13:56 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 68 failure(s)
2023-05-18 19:14:01 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 51 failure(s)
2023-05-18 19:14:07 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 54 failure(s)
2023-05-18 19:14:12 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 47 failure(s)
2023-05-18 19:14:15 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0206  0.9840  0.9755  0.9664  0.9664  0.9654  0.9639  0.9624  0.9623  0.9616  0.9602  0.9602  0.9588  0.9556  0.9525  0.9516
[17 : 32]:	0.9495  0.9460  0.9400  0.9397  0.9387  0.9384  0.9363  0.9361  0.9324  0.9321  0.9314  0.9309  0.9301  0.9289  0.9249  0.9238
[33 : 48]:	0.9234  0.9232  0.9226  0.9226  0.9226  0.9222  0.9201  0.9200  0.9189  0.9188  0.9186  0.9179  0.9179  0.9174  0.9160  0.9159
[49 : 64]:	0.9157  0.9153  0.9153  0.9152  0.9142  0.9139  0.9136  0.9130  0.9124  0.9118  0.9118  0.9110  0.9108  0.9096  0.9095  0.9073
2023-05-18 19:14:15 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 19:14:15 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #897: GFLOPs: 779.6998. Time: 1294.4140 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #898: GFLOPs: 800.8736. Time: 1260.1918 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #899: GFLOPs: 317.5357. Time: 3178.3968 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #900: GFLOPs: 806.7219. Time: 1251.0561 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #901: GFLOPs: 812.6276. Time: 1241.9642 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #902: GFLOPs: 756.4378. Time: 1334.2200 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #903: GFLOPs: 353.5991. Time: 2854.2333 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #904: GFLOPs: 822.6149. Time: 1226.8856 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #905: GFLOPs: 892.3756. Time: 1130.9749 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #906: GFLOPs: 777.4851. Time: 1298.1013 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #907: GFLOPs: 540.8429. Time: 1866.0770 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #908: GFLOPs: 518.9619. Time: 1944.7561 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #909: GFLOPs: 333.5227. Time: 3026.0441 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #910: GFLOPs: 485.8795. Time: 2077.1701 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #911: GFLOPs: 753.8910. Time: 1338.7271 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #912: GFLOPs: 872.6660. Time: 1156.5185 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #913: GFLOPs: 646.4401. Time: 1561.2496 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #914: GFLOPs: 820.4539. Time: 1230.1171 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #915: GFLOPs: 541.0238. Time: 1865.4527 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #916: GFLOPs: 784.9180. Time: 1285.8088 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #917: GFLOPs: 806.9323. Time: 1250.7300 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #918: GFLOPs: 813.0453. Time: 1241.3262 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #919: GFLOPs: 854.6866. Time: 1180.8473 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #920: GFLOPs: 805.2311. Time: 1253.3724 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #921: GFLOPs: 621.4232. Time: 1624.1015 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #922: GFLOPs: 871.1370. Time: 1158.5484 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #923: GFLOPs: 872.3473. Time: 1156.9410 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #924: GFLOPs: 791.1035. Time: 1275.7552 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #925: GFLOPs: 778.2424. Time: 1296.8382 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #926: GFLOPs: 753.4308. Time: 1339.5448 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #927: GFLOPs: 506.4445. Time: 1992.8234 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #928: GFLOPs: 539.9644. Time: 1869.1128 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #929: GFLOPs: 856.6092. Time: 1178.1971 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #930: GFLOPs: 219.5883. Time: 4596.1211 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #931: GFLOPs: 852.3057. Time: 1184.1460 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #932: GFLOPs: 874.8668. Time: 1153.6092 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #933: GFLOPs: 872.7311. Time: 1156.4323 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #934: GFLOPs: 350.0996. Time: 2882.7639 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #935: GFLOPs: 758.9161. Time: 1329.8629 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #936: GFLOPs: 660.8863. Time: 1527.1226 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #937: GFLOPs: 951.7623. Time: 1060.4060 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #938: GFLOPs: 549.1247. Time: 1837.9328 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #939: GFLOPs: 862.9849. Time: 1169.4925 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #940: GFLOPs: 887.5658. Time: 1137.1037 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #941: GFLOPs: 839.5321. Time: 1202.1630 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #942: GFLOPs: 535.1724. Time: 1885.8490 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #943: GFLOPs: 831.7991. Time: 1213.3391 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #944: GFLOPs: 679.8663. Time: 1484.4896 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #945: GFLOPs: 902.5874. Time: 1118.1792 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #946: GFLOPs: 795.3651. Time: 1268.9197 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #947: GFLOPs: 831.1631. Time: 1214.2676 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #948: GFLOPs: 930.8335. Time: 1084.2480 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #949: GFLOPs: 938.9148. Time: 1074.9158 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #950: GFLOPs: 482.2441. Time: 2092.8290 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #951: GFLOPs: 625.1258. Time: 1614.4821 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #952: GFLOPs: 793.0433. Time: 1272.6348 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #953: GFLOPs: 910.3349. Time: 1108.6628 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #954: GFLOPs: 901.3237. Time: 1119.7468 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #955: GFLOPs: 903.7399. Time: 1116.7531 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #956: GFLOPs: 902.2551. Time: 1118.5909 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #957: GFLOPs: 553.7947. Time: 1822.4343 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #958: GFLOPs: 10.9994. Time: 91755.1527 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #959: GFLOPs: 22.1938. Time: 45474.6390 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #960: GFLOPs: 43.1488. Time: 23390.0834 us. Best GFLOPs: 969.4070
2023-05-18 19:15:27 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 19:15:28 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 19:15:29 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 395 failure(s)
2023-05-18 19:15:29 [INFO] [evolutionary_search.cc:723] Sampled 15 candidate(s)
2023-05-18 19:15:33 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 56 failure(s)
2023-05-18 19:15:39 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 59 failure(s)
2023-05-18 19:15:44 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 75 failure(s)
2023-05-18 19:15:50 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f634c8)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62c08)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f62028)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f61228)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f61aa8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149e9d8)]: 64 failure(s)
2023-05-18 19:15:52 [INFO] [evolutionary_search.cc:649] Scores of the best 40 candidates:
[1 : 16]:	0.9787  0.9703  0.9612  0.9568  0.9568  0.9543  0.9465  0.9442  0.9423  0.9382  0.9378  0.9372  0.9343  0.9337  0.9315  0.9299
[17 : 32]:	0.9279  0.9236  0.9234  0.9230  0.9229  0.9223  0.9212  0.9203  0.9188  0.9174  0.9169  0.9162  0.9161  0.9154  0.9140  0.9138
[33 : 40]:	0.9122  0.9118  0.9113  0.9111  0.9109  0.9104  0.9096  0.9095
2023-05-18 19:15:52 [INFO] [evolutionary_search.cc:727] Got 40 candidate(s) with evolutionary search
2023-05-18 19:15:52 [INFO] [evolutionary_search.cc:730] Sending 40 candidates(s) for measurement
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #961: GFLOPs: 471.5208. Time: 2140.4240 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #962: GFLOPs: 351.7909. Time: 2868.9040 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #963: GFLOPs: 814.0004. Time: 1239.8696 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #964: GFLOPs: 381.2780. Time: 2647.0306 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #965: GFLOPs: 382.3602. Time: 2639.5385 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #966: GFLOPs: 788.4407. Time: 1280.0637 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #967: GFLOPs: 898.9284. Time: 1122.7306 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #968: GFLOPs: 382.1551. Time: 2640.9551 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #969: GFLOPs: 809.1443. Time: 1247.3107 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #970: GFLOPs: 693.2138. Time: 1455.9064 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #971: GFLOPs: 376.6080. Time: 2679.8538 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #972: GFLOPs: 803.7203. Time: 1255.7284 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #973: GFLOPs: 882.3639. Time: 1143.8075 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #974: GFLOPs: 376.7963. Time: 2678.5147 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #975: GFLOPs: 383.9879. Time: 2628.3497 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #976: GFLOPs: 874.8765. Time: 1153.5964 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #977: GFLOPs: 887.7182. Time: 1136.9086 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #978: GFLOPs: 489.2787. Time: 2062.7394 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #979: GFLOPs: 467.6020. Time: 2158.3620 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #980: GFLOPs: 624.2186. Time: 1616.8284 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #981: GFLOPs: 541.4595. Time: 1863.9518 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #982: GFLOPs: 501.8537. Time: 2011.0530 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #983: GFLOPs: 201.1063. Time: 5018.5125 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #984: GFLOPs: 532.8706. Time: 1893.9952 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #985: GFLOPs: 529.8266. Time: 1904.8769 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #986: GFLOPs: 549.0362. Time: 1838.2292 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #987: GFLOPs: 498.4269. Time: 2024.8795 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #988: GFLOPs: 678.7802. Time: 1486.8647 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #989: GFLOPs: 813.5458. Time: 1240.5625 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #990: GFLOPs: 905.9020. Time: 1114.0879 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #991: GFLOPs: 849.3604. Time: 1188.2522 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #992: GFLOPs: 949.8917. Time: 1062.4942 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #993: GFLOPs: 669.2479. Time: 1508.0427 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #994: GFLOPs: 904.0286. Time: 1116.3965 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #995: GFLOPs: 538.1329. Time: 1875.4741 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #996: GFLOPs: 832.9980. Time: 1211.5928 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #997: GFLOPs: 843.6707. Time: 1196.2658 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #998: GFLOPs: 797.6325. Time: 1265.3125 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #999: GFLOPs: 85.4974. Time: 11804.5000 us. Best GFLOPs: 969.4070
2023-05-18 19:16:31 [INFO] [task_scheduler.cc:131] [Task #21: fused_conv2d_add] Trial #1000: GFLOPs: 116.3846. Time: 8671.7152 us. Best GFLOPs: 969.4070
