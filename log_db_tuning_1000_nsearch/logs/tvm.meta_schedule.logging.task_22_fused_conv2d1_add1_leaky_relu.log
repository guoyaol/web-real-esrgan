2023-05-18 11:57:31 [INFO] [task_scheduler.cc:160] Initializing Task #22: "fused_conv2d1_add1_leaky_relu"
2023-05-18 11:57:31 [INFO] [task_scheduler.cc:35] 
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32"), self_rrdb_body_0_rdb1_conv1_weight: T.Buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), "float32"), lv4: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        pad_temp = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(642), T.int64(450)))
        var_conv2d_nchw_intermediate = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)))
        var_T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)))
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(64), T.int64(642), T.int64(450)):
            with T.block("pad_temp"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(lv2[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)])
                T.writes(pad_temp[v_i0, v_i1, v_i2, v_i3])
                pad_temp[v_i0, v_i1, v_i2, v_i3] = T.if_then_else(T.int64(1) <= v_i2 and v_i2 < T.int64(641) and T.int64(1) <= v_i3 and v_i3 < T.int64(449), lv2[v_i0, v_i1, v_i2 - T.int64(1), v_i3 - T.int64(1)], T.float32(0))
        for nn, ff, yy, xx, rc, ry, rx in T.grid(T.int64(1), T.int64(32), T.int64(640), T.int64(448), T.int64(64), T.int64(3), T.int64(3)):
            with T.block("conv2d_nchw"):
                v_nn, v_ff, v_yy, v_xx, v_rc, v_ry, v_rx = T.axis.remap("SSSSRRR", [nn, ff, yy, xx, rc, ry, rx])
                T.reads(pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_body_0_rdb1_conv1_weight[v_ff, v_rc, v_ry, v_rx])
                T.writes(var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx])
                with T.init():
                    var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate[v_nn, v_ff, v_yy, v_xx] + pad_temp[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_body_0_rdb1_conv1_weight[v_ff, v_rc, v_ry, v_rx]
        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(32), T.int64(640), T.int64(448)):
            with T.block("T_add"):
                v_ax0, v_ax1, v_ax2, v_ax3 = T.axis.remap("SSSS", [ax0, ax1, ax2, ax3])
                T.reads(var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3], lv4[v_ax0, v_ax1, T.int64(0), T.int64(0)])
                T.writes(var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3])
                var_T_add_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] = var_conv2d_nchw_intermediate[v_ax0, v_ax1, v_ax2, v_ax3] + lv4[v_ax0, v_ax1, T.int64(0), T.int64(0)]
        for i0, i1, i2, i3 in T.grid(T.int64(1), T.int64(32), T.int64(640), T.int64(448)):
            with T.block("compute"):
                v_i0, v_i1, v_i2, v_i3 = T.axis.remap("SSSS", [i0, i1, i2, i3])
                T.reads(var_T_add_intermediate[v_i0, v_i1, v_i2, v_i3])
                T.writes(var_compute_intermediate[v_i0, v_i1, v_i2, v_i3])
                var_compute_intermediate[v_i0, v_i1, v_i2, v_i3] = T.Select(T.float32(0) < var_T_add_intermediate[v_i0, v_i1, v_i2, v_i3], var_T_add_intermediate[v_i0, v_i1, v_i2, v_i3], var_T_add_intermediate[v_i0, v_i1, v_i2, v_i3] * T.float32(0.20000000000000001))
2023-05-18 11:57:31 [INFO] [task_scheduler.cc:164] Total 1 design space(s) generated
2023-05-18 11:57:31 [INFO] [task_scheduler.cc:170] Design space #0:
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32"), self_rrdb_body_0_rdb1_conv1_weight: T.Buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), "float32"), lv4: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        with T.block("root"):
            T.reads()
            T.writes()
            T.block_attr({"meta_schedule.unroll_explicit": 16})
            var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), scope="local")
            pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(642), T.int64(450)), scope="shared")
            self_rrdb_body_0_rdb1_conv1_weight_shared = T.alloc_buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), scope="shared")
            for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(2), thread="blockIdx.x"):
                for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(140), thread="vthread.x"):
                    for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(64), thread="threadIdx.x"):
                        for rc_0, ry_0, rx_0 in T.grid(T.int64(4), T.int64(1), T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(2321472)):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(16) + ax0_ax1_ax2_ax3_fused // T.int64(145092))
                                    v2 = T.axis.spatial(T.int64(642), ax0_ax1_ax2_ax3_fused % T.int64(145092) // T.int64(226))
                                    v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused * T.int64(224) + ax0_ax1_ax2_ax3_fused % T.int64(226))
                                    T.reads(lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 4})
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                            for ax0_ax1_ax2_ax3_fused in range(T.int64(4608)):
                                with T.block("self_rrdb_body_0_rdb1_conv1.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(32), ax0_ax1_ax2_ax3_fused // T.int64(144))
                                    v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(16) + ax0_ax1_ax2_ax3_fused % T.int64(144) // T.int64(9))
                                    v2 = T.axis.spatial(T.int64(3), ax0_ax1_ax2_ax3_fused % T.int64(9) // T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), ax0_ax1_ax2_ax3_fused % T.int64(3))
                                    T.reads(self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3])
                                    T.writes(self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3])
                                    T.block_attr({"meta_schedule.cooperative_fetch": 1})
                                    self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3] = self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3]
                            for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(3), T.int64(3), T.int64(1), T.int64(4), T.int64(4), T.int64(16)):
                                with T.block("conv2d_nchw"):
                                    v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                    v_ff = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3 * T.int64(4) + ff_4)
                                    v_yy = T.axis.spatial(T.int64(640), nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) * T.int64(4) + yy_3 * T.int64(4) + yy_4)
                                    v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused * T.int64(224) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(16) + xx_3 * T.int64(16) + xx_4)
                                    v_rc = T.axis.reduce(T.int64(64), rc_0 * T.int64(16) + rc_1 * T.int64(2) + rc_2)
                                    v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                    v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                    T.reads(pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                    T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                    T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                    with T.init():
                                        var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                                    var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                        for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(4), T.int64(16)):
                            with T.block("var_conv2d_nchw_intermediate_local"):
                                v0 = T.axis.spatial(T.int64(1), ax0)
                                v1 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ax1)
                                v2 = T.axis.spatial(T.int64(640), nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) * T.int64(4) + ax2)
                                v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused * T.int64(224) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) * T.int64(32) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(16) + ax3)
                                T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv4[v0, v1, T.int64(0), T.int64(0)])
                                T.writes(var_compute_intermediate[v0, v1, v2, v3])
                                var_compute_intermediate[v0, v1, v2, v3] = T.Select(T.float32(0) < var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.20000000000000001))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 4, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[1, 20, 8, 1, 4])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[2, 7, 2, 1, 16])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[4, 8, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
2023-05-18 12:05:18 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 12:05:18 [INFO] [evolutionary_search.cc:715] Picked top 0 candidate(s) from database
2023-05-18 12:05:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 506 failure(s)
2023-05-18 12:05:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1014 failure(s)
2023-05-18 12:05:22 [INFO] [evolutionary_search.cc:723] Sampled 10 candidate(s)
2023-05-18 12:05:26 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 130 failure(s)
2023-05-18 12:05:30 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 90 failure(s)
2023-05-18 12:05:33 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 83 failure(s)
2023-05-18 12:05:37 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 116 failure(s)
2023-05-18 12:05:37 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9972  0.9967  0.9961  0.9954  0.9944  0.9931  0.9915  0.9900  0.9898  0.9887  0.9881  0.9874  0.9874  0.9867  0.9865  0.9865
[17 : 32]:	0.9844  0.9843  0.9829  0.9807  0.9804  0.9770  0.9769  0.9758  0.9754  0.9750  0.9742  0.9722  0.9715  0.9713  0.9699  0.9697
[33 : 48]:	0.9674  0.9658  0.9658  0.9656  0.9648  0.9647  0.9638  0.9636  0.9631  0.9625  0.9615  0.9606  0.9604  0.9600  0.9599  0.9597
[49 : 64]:	0.9592  0.9585  0.9579  0.9563  0.9561  0.9555  0.9543  0.9536  0.9533  0.9532  0.9523  0.9516  0.9513  0.9506  0.9504  0.9500
2023-05-18 12:05:37 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 12:05:37 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:121] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #1: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x000000011476a483
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[13:20:48] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32"), self_rrdb_body_0_rdb1_conv1_weight: T.Buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), "float32"), lv4: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_body_0_rdb1_conv1_weight_shared = T.alloc_buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(28), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(4), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) * T.int64(64) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(113)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(64), rc_0)
                                        v2 = T.axis.spatial(T.int64(642), ry_0 + nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(450))
                                        v3 = T.axis.spatial(T.int64(450), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(450))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(7200))
                                        T.reads(lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(3)):
                                    with T.block("self_rrdb_body_0_rdb1_conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(96) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3))
                                        v1, v2 = T.axis.remap("SS", [rc_0, ry_0])
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(96) + ax0_ax1_ax2_ax3_fused_1 * T.int64(3) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3] = self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) * T.int64(64) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(64), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(4), T.int64(1), T.int64(64)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) * T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + nn_1_ff_1_yy_1_xx_1_fused // T.int64(7) * T.int64(4) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_1_ff_1_yy_1_xx_1_fused % T.int64(7) * T.int64(64) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv4[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_compute_intermediate[v0, v1, v2, v3])
                            var_compute_intermediate[v0, v1, v2, v3] = T.Select(T.float32(0) < var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.20000000000000001))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 8, 1, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[40, 4, 4, 1, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 7, 1, 16, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 3], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #2: GFLOPs: 16.5718. Time: 638915.7640 us. Best GFLOPs: 16.5718
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #3: GFLOPs: 50.2210. Time: 210828.1807 us. Best GFLOPs: 50.2210
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #4: GFLOPs: 57.9573. Time: 182686.2083 us. Best GFLOPs: 57.9573
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #5: GFLOPs: 20.8409. Time: 508039.9027 us. Best GFLOPs: 57.9573
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:121] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #6: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32"), self_rrdb_body_0_rdb1_conv1_weight: T.Buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), "float32"), lv4: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_body_0_rdb1_conv1_weight_shared = T.alloc_buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(10), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + yy_3_init * T.int64(10) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(32), T.int64(3), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(20)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1280))
                                        v2 = T.axis.spatial(T.int64(642), ry_0 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(450), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.reads(lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("self_rrdb_body_0_rdb1_conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2, v3 = T.axis.remap("SS", [ry_0, rx_0])
                                        T.reads(self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3] = self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(10), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + yy_3 * T.int64(10) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(64), rc_0 * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(20), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv4[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_compute_intermediate[v0, v1, v2, v3])
                            var_compute_intermediate[v0, v1, v2, v3] = T.Select(T.float32(0) < var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.20000000000000001))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 8, 1, 2, 10])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[56, 1, 2, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #7: GFLOPs: 325.2693. Time: 32551.4793 us. Best GFLOPs: 325.2693
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #8: GFLOPs: 19.7274. Time: 536714.3333 us. Best GFLOPs: 325.2693
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #9: GFLOPs: 213.8879. Time: 49502.5557 us. Best GFLOPs: 325.2693
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #10: GFLOPs: 36.7584. Time: 288042.8750 us. Best GFLOPs: 325.2693
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #11: GFLOPs: 16.6699. Time: 635158.2500 us. Best GFLOPs: 325.2693
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #12: GFLOPs: 60.0108. Time: 176434.9167 us. Best GFLOPs: 325.2693
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #13: GFLOPs: 207.1613. Time: 51109.9027 us. Best GFLOPs: 325.2693
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #14: GFLOPs: 110.2064. Time: 96074.2640 us. Best GFLOPs: 325.2693
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #15: GFLOPs: 519.6971. Time: 20373.4000 us. Best GFLOPs: 519.6971
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:121] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #16: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32"), self_rrdb_body_0_rdb1_conv1_weight: T.Buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), "float32"), lv4: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_body_0_rdb1_conv1_weight_shared = T.alloc_buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(10), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + yy_3_init * T.int64(10) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(32), T.int64(1), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(26)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1620))
                                        v2 = T.axis.spatial(T.int64(642), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1620) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(3240))
                                        T.reads(lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(9)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("self_rrdb_body_0_rdb1_conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(18))
                                        v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(18) // T.int64(9))
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(9) // T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3] = self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(2), T.int64(3), T.int64(3), T.int64(1), T.int64(1), T.int64(10), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + yy_3 * T.int64(10) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(64), rc_0 * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(20), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv4[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_compute_intermediate[v0, v1, v2, v3])
                            var_compute_intermediate[v0, v1, v2, v3] = T.Select(T.float32(0) < var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.20000000000000001))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 8, 1, 2, 10])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[56, 1, 2, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #17: GFLOPs: 41.9973. Time: 252111.2500 us. Best GFLOPs: 519.6971
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #18: GFLOPs: 19.9972. Time: 529474.9583 us. Best GFLOPs: 519.6971
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #19: GFLOPs: 145.4272. Time: 72806.1530 us. Best GFLOPs: 519.6971
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:121] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #20: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32"), self_rrdb_body_0_rdb1_conv1_weight: T.Buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), "float32"), lv4: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_body_0_rdb1_conv1_weight_shared = T.alloc_buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(10), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + yy_3_init * T.int64(10) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(32), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(21)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1296))
                                        v2 = T.axis.spatial(T.int64(642), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1296) // T.int64(8))
                                        v3 = T.axis.spatial(T.int64(450), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(8))
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(2592))
                                        T.reads(lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(6)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("self_rrdb_body_0_rdb1_conv1.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(6))
                                    v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(6) // T.int64(3))
                                    v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    v3 = T.axis.spatial(T.int64(3), rx_0)
                                    T.reads(self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3])
                                    T.writes(self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3])
                                    self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3] = self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(10), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + yy_3 * T.int64(10) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(64), rc_0 * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(20), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv4[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_compute_intermediate[v0, v1, v2, v3])
                            var_compute_intermediate[v0, v1, v2, v3] = T.Select(T.float32(0) < var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.20000000000000001))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 2, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 8, 1, 2, 10])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[56, 1, 2, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #21: GFLOPs: 30.8404. Time: 343316.0417 us. Best GFLOPs: 519.6971
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #22: GFLOPs: 825.8125. Time: 12821.3072 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #23: GFLOPs: 65.2460. Time: 162278.1943 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #24: GFLOPs: 52.2045. Time: 202817.5000 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #25: GFLOPs: 431.3665. Time: 24545.2430 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #26: GFLOPs: 397.0314. Time: 26667.9062 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #27: GFLOPs: 58.1428. Time: 182103.3057 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:121] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #28: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x000000011325cd73
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:109
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 109
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[13:21:31] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: Fail to compile metal source:program_source:47:269: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:47:1137: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:47:1962: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:47:2765: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:47:3664: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:49:329: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:49:1325: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:49:2278: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:49:3209: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:49:4236: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:51:329: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:51:1325: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:51:2278: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:51:3209: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'
program_source:51:4236: error: value of type 'bool __attribute__((ext_vector_type(4)))' (vector of 4 'bool' values) is not contextually convertible to 'bool'



# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32"), self_rrdb_body_0_rdb1_conv1_weight: T.Buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), "float32"), lv4: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_body_0_rdb1_conv1_weight_shared = T.alloc_buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(280), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 16, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(16), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3_init + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(16) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(4) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(16), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(40)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(1280))
                                        v2 = T.axis.spatial(T.int64(642), ry_0 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(128) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(1280) // T.int64(10))
                                        v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(10))
                                        T.reads(lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("self_rrdb_body_0_rdb1_conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(12))
                                        v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(12) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), ry_0)
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3] = self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(4), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(16), T.int64(1), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(1), T.int64(4)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ff_3 + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(16) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(4) + xx_4)
                                v_rc = T.axis.reduce(T.int64(64), rc_0 * T.int64(4) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(1), T.int64(16), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(4) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(128) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) // T.int64(2) * T.int64(64) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(16) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv4[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_compute_intermediate[v0, v1, v2, v3])
                            var_compute_intermediate[v0, v1, v2, v3] = T.Select(T.float32(0) < var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.20000000000000001))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 8, 1, 1])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[5, 2, 4, 16, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[56, 2, 1, 1, 4])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[16, 4, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=1)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
sch.annotate(block_or_loop=l144, ann_key="pragma_auto_unroll_max_step", ann_val=16)
sch.annotate(block_or_loop=l144, ann_key="pragma_unroll_explicit", ann_val=1)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #29: GFLOPs: 44.9023. Time: 235800.7360 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #30: GFLOPs: 25.5181. Time: 414921.0277 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #31: GFLOPs: 26.5416. Time: 398920.8053 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #32: GFLOPs: 223.5330. Time: 47366.5970 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #33: GFLOPs: 38.5009. Time: 275006.7223 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #34: GFLOPs: 4.3859. Time: 2414120.4443 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #35: GFLOPs: 26.0364. Time: 406661.6387 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:121] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #36: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x0000000105e0e903
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:109
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 109
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[13:21:57] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: Fail to compile metal source:program_source:37:251: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
program_source:37:1069: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
program_source:37:1856: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
program_source:37:2627: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'
program_source:37:3458: error: value of type 'bool __attribute__((ext_vector_type(2)))' (vector of 2 'bool' values) is not contextually convertible to 'bool'



# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32"), self_rrdb_body_0_rdb1_conv1_weight: T.Buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), "float32"), lv4: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_body_0_rdb1_conv1_weight_shared = T.alloc_buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(4480), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(4), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(112) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) * T.int64(2) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(112) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3_init + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(8), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(96))
                                        v2 = T.axis.spatial(T.int64(642), ry_0 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(112) * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(96) // T.int64(6))
                                        v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(112) * T.int64(4) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(6))
                                        T.reads(lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(12)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("self_rrdb_body_0_rdb1_conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(24))
                                        v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(24) // T.int64(3))
                                        v2 = T.axis.spatial(T.int64(3), ry_0)
                                        v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        T.reads(self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3] = self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(8), T.int64(1), T.int64(3), T.int64(1), T.int64(4), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(1), T.int64(1)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(112) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) * T.int64(2) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(112) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + xx_3 + xx_4)
                                v_rc = T.axis.reduce(T.int64(64), rc_0 * T.int64(8) + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(2), T.int64(1)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(112) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(2) * T.int64(2) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(112) * T.int64(4) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(2) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv4[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_compute_intermediate[v0, v1, v2, v3])
                            var_compute_intermediate[v0, v1, v2, v3] = T.Select(T.float32(0) < var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.20000000000000001))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 4, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[40, 1, 8, 2, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[112, 2, 2, 1, 1])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[8, 8, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 3, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119, l120 = sch.split(loop=l117, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l120)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b121 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b121, ann_key="meta_schedule.unroll_explicit")
b122, b123, b124, b125 = sch.get_child_blocks(b121)
l126, l127, l128, l129, l130, l131, l132, l133, l134 = sch.get_loops(block=b122)
l135, l136, l137, l138, l139, l140, l141, l142, l143 = sch.get_loops(block=b123)
l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161, l162, l163 = sch.get_loops(block=b124)
l164, l165, l166, l167, l168, l169, l170 = sch.get_loops(block=b125)
b171 = sch.get_block(name="conv2d_nchw", func_name="main")
l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189, l190, l191 = sch.get_loops(block=b171)
b192 = sch.decompose_reduction(block=b171, loop=l175)
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #37: GFLOPs: 636.2425. Time: 16641.4464 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #38: GFLOPs: 23.2671. Time: 455063.8197 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #39: GFLOPs: 87.9799. Time: 120345.6250 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #40: GFLOPs: 15.3722. Time: 688774.8750 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #41: GFLOPs: 84.9044. Time: 124704.9307 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #42: GFLOPs: 23.5549. Time: 449502.2223 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #43: GFLOPs: 63.0971. Time: 167804.8333 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #44: GFLOPs: 303.8403. Time: 34847.2360 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #45: GFLOPs: 32.7301. Time: 323494.3193 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #46: GFLOPs: 118.6293. Time: 89252.8193 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #47: GFLOPs: 146.1120. Time: 72464.9443 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #48: GFLOPs: 19.9098. Time: 531798.5557 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #49: GFLOPs: 10.8673. Time: 974298.9723 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:121] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #50: Error in building:
LocalBuilder: Timeout, killed after 30.0 seconds
# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32"), self_rrdb_body_0_rdb1_conv1_weight: T.Buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), "float32"), lv4: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_body_0_rdb1_conv1_weight_shared = T.alloc_buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(224), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 1024, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(1), T.int64(2), T.int64(10), T.int64(2)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + yy_3_init * T.int64(10) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3_init * T.int64(2) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(32), T.int64(3), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(80)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(1280))
                                    v2 = T.axis.spatial(T.int64(642), ry_0 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(1280) // T.int64(8))
                                    v3 = T.axis.spatial(T.int64(450), rx_0 + nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(8))
                                    T.reads(lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(1)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(4)):
                                    with T.block("self_rrdb_body_0_rdb1_conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) // T.int64(2))
                                        v1 = T.axis.spatial(T.int64(64), rc_0 * T.int64(2) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(128) + ax0_ax1_ax2_ax3_fused_1 * T.int64(4) + ax0_ax1_ax2_ax3_fused_2) % T.int64(2))
                                        v2, v3 = T.axis.remap("SS", [ry_0, rx_0])
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(4) + ax0_ax1_ax2_ax3_fused_2 < T.int64(64))
                                        T.reads(self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3] = self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(2), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(10), T.int64(2)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + yy_3 * T.int64(10) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + xx_3 * T.int64(2) + xx_4)
                                v_rc = T.axis.reduce(T.int64(64), rc_0 * T.int64(2) + rc_1 * T.int64(2) + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(20), T.int64(4)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), nn_2_ff_2_yy_2_xx_2_fused // T.int64(2) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(56) * T.int64(160) + nn_1_ff_1_yy_1_xx_1_fused * T.int64(20) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(56) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(2) * T.int64(4) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv4[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_compute_intermediate[v0, v1, v2, v3])
                            var_compute_intermediate[v0, v1, v2, v3] = T.Select(T.float32(0) < var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.20000000000000001))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 1, 16, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 8, 1, 2, 10])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[56, 1, 2, 2, 2])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[32, 1, 2])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=2)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=3)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=4)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 4], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=1024)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #51: GFLOPs: 36.4915. Time: 290149.3057 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #52: GFLOPs: 50.2051. Time: 210894.9583 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #53: GFLOPs: 270.0022. Time: 39214.4860 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #54: GFLOPs: 38.9869. Time: 271578.0973 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #55: GFLOPs: 55.9305. Time: 189306.1527 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #56: GFLOPs: 32.5689. Time: 325095.6807 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #57: GFLOPs: 53.1030. Time: 199386.0000 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #58: GFLOPs: 5.9112. Time: 1791188.9723 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #59: GFLOPs: 104.1958. Time: 101616.3890 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #60: GFLOPs: 22.2246. Time: 476408.5833 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #61: GFLOPs: 88.4019. Time: 119771.1943 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #62: GFLOPs: 54.8902. Time: 192893.9720 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #63: GFLOPs: 44.6566. Time: 237098.3750 us. Best GFLOPs: 825.8125
2023-05-18 13:22:57 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #64: GFLOPs: 16.7010. Time: 633972.8890 us. Best GFLOPs: 825.8125
2023-05-18 13:54:10 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 13:54:11 [INFO] [evolutionary_search.cc:715] Picked top 57 candidate(s) from database
2023-05-18 13:54:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 447 failure(s)
2023-05-18 13:54:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 897 failure(s)
2023-05-18 13:54:15 [INFO] [evolutionary_search.cc:723] Sampled 13 candidate(s)
2023-05-18 13:54:19 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 108 failure(s)
2023-05-18 13:54:25 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 79 failure(s)
2023-05-18 13:54:31 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 77 failure(s)
2023-05-18 13:54:37 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 66 failure(s)
2023-05-18 13:54:40 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9977  0.9922  0.9922  0.9807  0.9803  0.9803  0.9527  0.9492  0.9492  0.9420  0.9395  0.9390  0.9382  0.9382  0.9382  0.9331
[17 : 32]:	0.9187  0.9173  0.9129  0.9119  0.9117  0.9108  0.9100  0.9100  0.9066  0.9062  0.9047  0.9012  0.9012  0.8973  0.8960  0.8960
[33 : 48]:	0.8959  0.8959  0.8955  0.8915  0.8907  0.8907  0.8886  0.8870  0.8869  0.8867  0.8853  0.8844  0.8824  0.8801  0.8732  0.8729
[49 : 64]:	0.8719  0.8716  0.8684  0.8664  0.8663  0.8658  0.8655  0.8650  0.8645  0.8635  0.8626  0.8621  0.8608  0.8597  0.8550  0.8540
2023-05-18 13:54:40 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 13:54:40 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #65: GFLOPs: 816.3232. Time: 12970.3489 us. Best GFLOPs: 825.8125
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #66: GFLOPs: 487.2880. Time: 21728.4166 us. Best GFLOPs: 825.8125
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #67: GFLOPs: 2.9805. Time: 3552479.2500 us. Best GFLOPs: 825.8125
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #68: GFLOPs: 383.1128. Time: 27636.7582 us. Best GFLOPs: 825.8125
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #69: GFLOPs: 4.3710. Time: 2422332.9583 us. Best GFLOPs: 825.8125
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #70: GFLOPs: 777.9294. Time: 13610.4844 us. Best GFLOPs: 825.8125
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #71: GFLOPs: 571.2427. Time: 18535.0208 us. Best GFLOPs: 825.8125
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #72: GFLOPs: 709.8050. Time: 14916.7679 us. Best GFLOPs: 825.8125
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #73: GFLOPs: 892.1518. Time: 11867.9306 us. Best GFLOPs: 892.1518
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #74: GFLOPs: 896.3506. Time: 11812.3380 us. Best GFLOPs: 896.3506
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #75: GFLOPs: 704.6841. Time: 15025.1667 us. Best GFLOPs: 896.3506
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #76: GFLOPs: 806.0520. Time: 13135.6250 us. Best GFLOPs: 896.3506
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #77: GFLOPs: 880.8120. Time: 12020.7222 us. Best GFLOPs: 896.3506
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #78: GFLOPs: 809.5823. Time: 13078.3438 us. Best GFLOPs: 896.3506
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #79: GFLOPs: 842.2218. Time: 12571.5051 us. Best GFLOPs: 896.3506
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #80: GFLOPs: 1069.2043. Time: 9902.6875 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #81: GFLOPs: 568.6115. Time: 18620.7917 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #82: GFLOPs: 751.8930. Time: 14081.7865 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #83: GFLOPs: 615.7010. Time: 17196.6528 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #84: GFLOPs: 793.2959. Time: 13346.8438 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #85: GFLOPs: 737.7978. Time: 14350.8094 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #86: GFLOPs: 946.3050. Time: 11188.7778 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #87: GFLOPs: 915.9716. Time: 11559.3056 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #88: GFLOPs: 828.8036. Time: 12775.0365 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #89: GFLOPs: 810.0617. Time: 13070.6041 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #90: GFLOPs: 778.5148. Time: 13600.2500 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #91: GFLOPs: 742.3176. Time: 14263.4322 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #92: GFLOPs: 1037.1830. Time: 10208.4167 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #93: GFLOPs: 854.3060. Time: 12393.6806 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #94: GFLOPs: 738.3647. Time: 14339.7917 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #95: GFLOPs: 680.0228. Time: 15570.0594 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #96: GFLOPs: 690.8597. Time: 15325.8273 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #97: GFLOPs: 781.2626. Time: 13552.4166 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #98: GFLOPs: 775.3759. Time: 13655.3072 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #99: GFLOPs: 873.2661. Time: 12124.5927 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #100: GFLOPs: 852.3892. Time: 12421.5509 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #101: GFLOPs: 953.5005. Time: 11104.3423 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #102: GFLOPs: 900.9236. Time: 11752.3797 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #103: GFLOPs: 950.4647. Time: 11139.8102 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #104: GFLOPs: 370.2767. Time: 28594.8230 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #105: GFLOPs: 779.9814. Time: 13574.6770 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #106: GFLOPs: 895.2180. Time: 11827.2824 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #107: GFLOPs: 828.1705. Time: 12784.8020 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #108: GFLOPs: 463.8978. Time: 22823.9834 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #109: GFLOPs: 804.9833. Time: 13153.0625 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #110: GFLOPs: 349.0949. Time: 30329.8543 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #111: GFLOPs: 385.9695. Time: 27432.2085 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #112: GFLOPs: 804.3963. Time: 13162.6615 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #113: GFLOPs: 716.6341. Time: 14774.6191 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #114: GFLOPs: 738.7112. Time: 14333.0656 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #115: GFLOPs: 978.5047. Time: 10820.5875 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #116: GFLOPs: 798.8771. Time: 13253.5989 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #117: GFLOPs: 653.6879. Time: 16197.3274 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #118: GFLOPs: 835.4575. Time: 12673.2918 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #119: GFLOPs: 884.6008. Time: 11969.2361 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #120: GFLOPs: 627.1852. Time: 16881.7708 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #121: GFLOPs: 851.5821. Time: 12433.3241 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #122: GFLOPs: 809.9555. Time: 13072.3178 us. Best GFLOPs: 1069.2043
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #123: GFLOPs: 1088.9427. Time: 9723.1894 us. Best GFLOPs: 1088.9427
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #124: GFLOPs: 717.3694. Time: 14759.4761 us. Best GFLOPs: 1088.9427
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #125: GFLOPs: 461.2483. Time: 22955.0916 us. Best GFLOPs: 1088.9427
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #126: GFLOPs: 92.6279. Time: 114306.8193 us. Best GFLOPs: 1088.9427
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #127: GFLOPs: 14.4018. Time: 735184.8333 us. Best GFLOPs: 1088.9427
2023-05-18 14:06:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #128: GFLOPs: 251.6199. Time: 42079.3333 us. Best GFLOPs: 1088.9427
2023-05-18 14:06:07 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 14:06:08 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 14:06:09 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 404 failure(s)
2023-05-18 14:06:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 811 failure(s)
2023-05-18 14:06:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1215 failure(s)
2023-05-18 14:06:13 [INFO] [evolutionary_search.cc:723] Sampled 15 candidate(s)
2023-05-18 14:06:17 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 81 failure(s)
2023-05-18 14:06:23 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 57 failure(s)
2023-05-18 14:06:29 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 74 failure(s)
2023-05-18 14:06:35 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 61 failure(s)
2023-05-18 14:06:37 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	1.0208  0.9780  0.9672  0.9642  0.9539  0.9496  0.9473  0.9446  0.9438  0.9410  0.9282  0.9277  0.9242  0.9199  0.9118  0.9117
[17 : 32]:	0.9070  0.9021  0.9004  0.9003  0.9000  0.8994  0.8968  0.8929  0.8908  0.8819  0.8784  0.8776  0.8769  0.8764  0.8753  0.8750
[33 : 48]:	0.8738  0.8698  0.8683  0.8683  0.8680  0.8668  0.8653  0.8650  0.8647  0.8620  0.8602  0.8600  0.8598  0.8591  0.8584  0.8575
[49 : 64]:	0.8566  0.8561  0.8560  0.8558  0.8550  0.8512  0.8507  0.8507  0.8503  0.8492  0.8490  0.8481  0.8475  0.8466  0.8457  0.8448
2023-05-18 14:06:37 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 14:06:37 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #129: GFLOPs: 913.9880. Time: 11584.3936 us. Best GFLOPs: 1088.9427
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #130: GFLOPs: 1207.7907. Time: 8766.4167 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #131: GFLOPs: 1077.2122. Time: 9829.0720 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #132: GFLOPs: 930.2623. Time: 11381.7316 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #133: GFLOPs: 1057.2455. Time: 10014.7000 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #134: GFLOPs: 982.0135. Time: 10781.9250 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #135: GFLOPs: 988.6715. Time: 10709.3167 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #136: GFLOPs: 1001.0128. Time: 10577.2833 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #137: GFLOPs: 1066.8474. Time: 9924.5651 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #138: GFLOPs: 951.9974. Time: 11121.8750 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #139: GFLOPs: 1056.1474. Time: 10025.1125 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #140: GFLOPs: 973.0131. Time: 10881.6584 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #141: GFLOPs: 916.4286. Time: 11553.5417 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #142: GFLOPs: 964.1246. Time: 10981.9791 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #143: GFLOPs: 1124.0114. Time: 9419.8295 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #144: GFLOPs: 1043.1313. Time: 10150.2042 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #145: GFLOPs: 970.1101. Time: 10914.2208 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #146: GFLOPs: 742.6563. Time: 14256.9270 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #147: GFLOPs: 989.5404. Time: 10699.9125 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #148: GFLOPs: 827.0469. Time: 12802.1719 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #149: GFLOPs: 973.4442. Time: 10876.8393 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #150: GFLOPs: 868.1037. Time: 12196.6944 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #151: GFLOPs: 824.3054. Time: 12844.7500 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #152: GFLOPs: 1067.1226. Time: 9922.0052 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #153: GFLOPs: 909.9948. Time: 11635.2269 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #154: GFLOPs: 991.3645. Time: 10680.2250 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #155: GFLOPs: 1010.2524. Time: 10480.5458 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #156: GFLOPs: 671.6534. Time: 15764.0774 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #157: GFLOPs: 1084.7687. Time: 9760.6023 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #158: GFLOPs: 956.5787. Time: 11068.6101 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #159: GFLOPs: 843.1035. Time: 12558.3594 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #160: GFLOPs: 976.1000. Time: 10847.2458 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #161: GFLOPs: 990.2986. Time: 10691.7208 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #162: GFLOPs: 1007.3185. Time: 10511.0709 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #163: GFLOPs: 1006.6960. Time: 10517.5709 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #164: GFLOPs: 1076.5583. Time: 9835.0416 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #165: GFLOPs: 981.3821. Time: 10788.8625 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #166: GFLOPs: 839.9578. Time: 12605.3906 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #167: GFLOPs: 794.1335. Time: 13332.7656 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #168: GFLOPs: 962.0302. Time: 11005.8875 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #169: GFLOPs: 835.1321. Time: 12678.2292 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #170: GFLOPs: 977.8635. Time: 10827.6833 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #171: GFLOPs: 1034.0892. Time: 10238.9583 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #172: GFLOPs: 1053.5490. Time: 10049.8375 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #173: GFLOPs: 952.1980. Time: 11119.5323 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #174: GFLOPs: 848.7728. Time: 12474.4757 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #175: GFLOPs: 957.4089. Time: 11059.0119 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #176: GFLOPs: 921.8554. Time: 11485.5278 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #177: GFLOPs: 887.0286. Time: 11936.4769 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #178: GFLOPs: 770.1901. Time: 13747.2500 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #179: GFLOPs: 938.8726. Time: 11277.3518 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #180: GFLOPs: 1169.3907. Time: 9054.2843 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #181: GFLOPs: 1057.2270. Time: 10014.8750 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #182: GFLOPs: 897.3230. Time: 11799.5371 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #183: GFLOPs: 908.4903. Time: 11654.4953 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #184: GFLOPs: 909.2633. Time: 11644.5879 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #185: GFLOPs: 1139.5636. Time: 9291.2727 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #186: GFLOPs: 823.2785. Time: 12860.7709 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #187: GFLOPs: 959.2622. Time: 11037.6458 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #188: GFLOPs: 945.3396. Time: 11200.2038 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #189: GFLOPs: 713.5630. Time: 14838.2084 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #190: GFLOPs: 452.6569. Time: 23390.7750 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #191: GFLOPs: 14.9320. Time: 709079.1807 us. Best GFLOPs: 1207.7907
2023-05-18 14:09:38 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #192: GFLOPs: 282.5787. Time: 37469.1947 us. Best GFLOPs: 1207.7907
2023-05-18 14:57:09 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 14:57:10 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 14:57:11 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 407 failure(s)
2023-05-18 14:57:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 813 failure(s)
2023-05-18 14:57:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1216 failure(s)
2023-05-18 14:57:15 [INFO] [evolutionary_search.cc:723] Sampled 14 candidate(s)
2023-05-18 14:57:19 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 68 failure(s)
2023-05-18 14:57:25 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 70 failure(s)
2023-05-18 14:57:31 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 66 failure(s)
2023-05-18 14:57:37 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 62 failure(s)
2023-05-18 14:57:39 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9792  0.9700  0.9593  0.9505  0.9499  0.9293  0.9238  0.9193  0.9123  0.9122  0.9122  0.9072  0.9056  0.8834  0.8831  0.8796
[17 : 32]:	0.8758  0.8758  0.8758  0.8758  0.8752  0.8723  0.8690  0.8646  0.8621  0.8589  0.8572  0.8520  0.8509  0.8509  0.8507  0.8501
[33 : 48]:	0.8496  0.8483  0.8467  0.8446  0.8442  0.8437  0.8418  0.8406  0.8373  0.8373  0.8372  0.8343  0.8343  0.8342  0.8342  0.8325
[49 : 64]:	0.8324  0.8324  0.8323  0.8321  0.8302  0.8292  0.8284  0.8284  0.8281  0.8270  0.8265  0.8265  0.8255  0.8252  0.8243  0.8216
2023-05-18 14:57:39 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 14:57:39 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #193: GFLOPs: 1181.8943. Time: 8958.4966 us. Best GFLOPs: 1207.7907
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #194: GFLOPs: 1202.2246. Time: 8807.0035 us. Best GFLOPs: 1207.7907
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #195: GFLOPs: 1135.4157. Time: 9325.2159 us. Best GFLOPs: 1207.7907
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #196: GFLOPs: 953.3247. Time: 11106.3899 us. Best GFLOPs: 1207.7907
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #197: GFLOPs: 1051.5931. Time: 10068.5292 us. Best GFLOPs: 1207.7907
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #198: GFLOPs: 1221.5264. Time: 8667.8403 us. Best GFLOPs: 1221.5264
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #199: GFLOPs: 1169.4639. Time: 9053.7181 us. Best GFLOPs: 1221.5264
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #200: GFLOPs: 1206.1859. Time: 8778.0799 us. Best GFLOPs: 1221.5264
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #201: GFLOPs: 1195.0325. Time: 8860.0069 us. Best GFLOPs: 1221.5264
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #202: GFLOPs: 1264.3678. Time: 8374.1424 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #203: GFLOPs: 1175.5682. Time: 9006.7049 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #204: GFLOPs: 1163.9725. Time: 9096.4318 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #205: GFLOPs: 1057.9629. Time: 10007.9084 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #206: GFLOPs: 630.5522. Time: 16791.6250 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #207: GFLOPs: 1126.0164. Time: 9403.0568 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #208: GFLOPs: 1177.1143. Time: 8994.8750 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #209: GFLOPs: 1004.7555. Time: 10537.8833 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #210: GFLOPs: 1017.1519. Time: 10409.4542 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #211: GFLOPs: 943.2151. Time: 11225.4306 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #212: GFLOPs: 1008.0998. Time: 10502.9250 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #213: GFLOPs: 1016.1924. Time: 10419.2833 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #214: GFLOPs: 1132.1180. Time: 9352.3788 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #215: GFLOPs: 797.3226. Time: 13279.4375 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #216: GFLOPs: 1151.1412. Time: 9197.8257 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #217: GFLOPs: 943.1712. Time: 11225.9537 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #218: GFLOPs: 895.2327. Time: 11827.0879 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #219: GFLOPs: 1038.6392. Time: 10194.1041 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #220: GFLOPs: 1160.8661. Time: 9120.7727 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #221: GFLOPs: 1022.0850. Time: 10359.2125 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #222: GFLOPs: 925.5657. Time: 11439.4861 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #223: GFLOPs: 1170.2629. Time: 9047.5368 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #224: GFLOPs: 1110.4895. Time: 9534.5303 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #225: GFLOPs: 1161.0436. Time: 9119.3788 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #226: GFLOPs: 1220.7152. Time: 8673.6007 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #227: GFLOPs: 939.2732. Time: 11272.5417 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #228: GFLOPs: 941.9021. Time: 11241.0787 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #229: GFLOPs: 1069.0369. Time: 9904.2386 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #230: GFLOPs: 917.0695. Time: 11545.4676 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #231: GFLOPs: 975.2151. Time: 10857.0875 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #232: GFLOPs: 906.4235. Time: 11681.0694 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #233: GFLOPs: 700.5972. Time: 15112.8154 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #234: GFLOPs: 956.6059. Time: 11068.2946 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #235: GFLOPs: 894.8408. Time: 11832.2684 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #236: GFLOPs: 983.1845. Time: 10769.0833 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #237: GFLOPs: 939.6112. Time: 11268.4861 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #238: GFLOPs: 980.6818. Time: 10796.5667 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #239: GFLOPs: 1039.1153. Time: 10189.4333 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #240: GFLOPs: 846.2259. Time: 12512.0209 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #241: GFLOPs: 635.1834. Time: 16669.1945 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #242: GFLOPs: 1184.8490. Time: 8936.1562 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #243: GFLOPs: 632.0529. Time: 16751.7570 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #244: GFLOPs: 964.7904. Time: 10974.4000 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #245: GFLOPs: 926.6901. Time: 11425.6066 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #246: GFLOPs: 991.9020. Time: 10674.4375 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #247: GFLOPs: 919.9117. Time: 11509.7962 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #248: GFLOPs: 1004.2619. Time: 10543.0625 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #249: GFLOPs: 992.3692. Time: 10669.4125 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #250: GFLOPs: 905.6012. Time: 11691.6759 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #251: GFLOPs: 941.2861. Time: 11248.4351 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #252: GFLOPs: 984.2550. Time: 10757.3708 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #253: GFLOPs: 999.5206. Time: 10593.0750 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #254: GFLOPs: 126.9452. Time: 83406.0417 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #255: GFLOPs: 108.2298. Time: 97828.8473 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #256: GFLOPs: 115.5759. Time: 91610.7777 us. Best GFLOPs: 1264.3678
2023-05-18 14:59:11 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 14:59:12 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 14:59:14 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 405 failure(s)
2023-05-18 14:59:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 814 failure(s)
2023-05-18 14:59:17 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1222 failure(s)
2023-05-18 14:59:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1630 failure(s)
2023-05-18 14:59:19 [INFO] [evolutionary_search.cc:723] Sampled 10 candidate(s)
2023-05-18 14:59:23 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 66 failure(s)
2023-05-18 14:59:29 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 72 failure(s)
2023-05-18 14:59:34 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 67 failure(s)
2023-05-18 14:59:40 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 71 failure(s)
2023-05-18 14:59:43 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9792  0.9700  0.9617  0.9617  0.9593  0.9591  0.9505  0.9505  0.9342  0.9342  0.9342  0.9304  0.9293  0.9293  0.9279  0.9238
[17 : 32]:	0.9238  0.9232  0.9232  0.9196  0.9182  0.9182  0.9182  0.9143  0.9143  0.9143  0.9132  0.9132  0.9132  0.9132  0.9122  0.9088
[33 : 48]:	0.9072  0.9056  0.8982  0.8977  0.8946  0.8944  0.8888  0.8885  0.8838  0.8835  0.8834  0.8816  0.8815  0.8798  0.8792  0.8773
[49 : 64]:	0.8773  0.8765  0.8758  0.8753  0.8739  0.8695  0.8685  0.8685  0.8675  0.8646  0.8621  0.8621  0.8609  0.8595  0.8591  0.8587
2023-05-18 14:59:43 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 14:59:43 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #257: GFLOPs: 1272.5897. Time: 8320.0395 us. Best GFLOPs: 1272.5897
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #258: GFLOPs: 1240.9767. Time: 8531.9861 us. Best GFLOPs: 1272.5897
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #259: GFLOPs: 1169.4116. Time: 9054.1225 us. Best GFLOPs: 1272.5897
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #260: GFLOPs: 1200.6568. Time: 8818.5035 us. Best GFLOPs: 1272.5897
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #261: GFLOPs: 1150.9341. Time: 9199.4811 us. Best GFLOPs: 1272.5897
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #262: GFLOPs: 1052.0760. Time: 10063.9083 us. Best GFLOPs: 1272.5897
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #263: GFLOPs: 1030.6023. Time: 10273.6000 us. Best GFLOPs: 1272.5897
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #264: GFLOPs: 1010.9984. Time: 10472.8125 us. Best GFLOPs: 1272.5897
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #265: GFLOPs: 1387.2786. Time: 7632.2063 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #266: GFLOPs: 1138.8245. Time: 9297.3031 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #267: GFLOPs: 1282.1543. Time: 8257.9737 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #268: GFLOPs: 929.0991. Time: 11395.9814 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #269: GFLOPs: 1167.8834. Time: 9065.9706 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #270: GFLOPs: 1129.3826. Time: 9375.0303 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #271: GFLOPs: 835.5615. Time: 12671.7135 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #272: GFLOPs: 1140.7407. Time: 9281.6856 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #273: GFLOPs: 1225.9384. Time: 8636.6458 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #274: GFLOPs: 1185.8890. Time: 8928.3194 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #275: GFLOPs: 1229.5564. Time: 8611.2326 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #276: GFLOPs: 856.3498. Time: 12364.1019 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #277: GFLOPs: 704.1362. Time: 15036.8571 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #278: GFLOPs: 728.3666. Time: 14536.6310 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #279: GFLOPs: 694.2986. Time: 15249.9167 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #280: GFLOPs: 717.5884. Time: 14754.9703 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #281: GFLOPs: 691.5225. Time: 15311.1370 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #282: GFLOPs: 1176.8386. Time: 8996.9827 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #283: GFLOPs: 1166.4532. Time: 9077.0858 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #284: GFLOPs: 1133.7950. Time: 9338.5455 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #285: GFLOPs: 1153.9283. Time: 9175.6099 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #286: GFLOPs: 1190.5143. Time: 8893.6319 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #287: GFLOPs: 1224.0541. Time: 8649.9410 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #288: GFLOPs: 1175.5476. Time: 9006.8628 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #289: GFLOPs: 1212.5053. Time: 8732.3298 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #290: GFLOPs: 1148.2566. Time: 9220.9318 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #291: GFLOPs: 676.5174. Time: 15650.7380 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #292: GFLOPs: 1168.8702. Time: 9058.3162 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #293: GFLOPs: 1221.9474. Time: 8664.8542 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #294: GFLOPs: 702.6035. Time: 15069.6607 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #295: GFLOPs: 859.2735. Time: 12322.0324 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #296: GFLOPs: 1193.9554. Time: 8868.0000 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #297: GFLOPs: 862.7287. Time: 12272.6840 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #298: GFLOPs: 1385.5392. Time: 7641.7877 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #299: GFLOPs: 641.4644. Time: 16505.9761 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #300: GFLOPs: 1106.1605. Time: 9571.8447 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #301: GFLOPs: 1083.2050. Time: 9774.6932 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #302: GFLOPs: 947.1699. Time: 11178.5601 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #303: GFLOPs: 1056.1184. Time: 10025.3875 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #304: GFLOPs: 1190.2915. Time: 8895.2966 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #305: GFLOPs: 1140.6355. Time: 9282.5416 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #306: GFLOPs: 1077.0044. Time: 9830.9688 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #307: GFLOPs: 1092.1717. Time: 9694.4432 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #308: GFLOPs: 1015.1150. Time: 10430.3417 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #309: GFLOPs: 634.0307. Time: 16699.5000 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #310: GFLOPs: 1350.5649. Time: 7839.6795 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #311: GFLOPs: 1243.4264. Time: 8515.1771 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #312: GFLOPs: 974.6612. Time: 10863.2583 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #313: GFLOPs: 1269.5800. Time: 8339.7632 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #314: GFLOPs: 1195.3913. Time: 8857.3472 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #315: GFLOPs: 1017.9599. Time: 10401.1917 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #316: GFLOPs: 1008.1890. Time: 10501.9958 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #317: GFLOPs: 810.0004. Time: 13071.5938 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #318: GFLOPs: 47.3273. Time: 223718.7363 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #319: GFLOPs: 245.6404. Time: 43103.6390 us. Best GFLOPs: 1387.2786
2023-05-18 15:03:46 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #320: GFLOPs: 23.9638. Time: 441833.7360 us. Best GFLOPs: 1387.2786
2023-05-18 15:39:17 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 15:39:17 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 15:39:19 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 405 failure(s)
2023-05-18 15:39:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 812 failure(s)
2023-05-18 15:39:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1220 failure(s)
2023-05-18 15:39:23 [INFO] [evolutionary_search.cc:723] Sampled 10 candidate(s)
2023-05-18 15:39:27 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 64 failure(s)
2023-05-18 15:39:33 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 67 failure(s)
2023-05-18 15:39:38 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 49 failure(s)
2023-05-18 15:39:44 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 52 failure(s)
2023-05-18 15:39:47 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9639  0.9494  0.9418  0.9418  0.9418  0.9418  0.9409  0.9327  0.9327  0.9325  0.9325  0.9325  0.9325  0.9279  0.9256  0.9256
[17 : 32]:	0.9256  0.9199  0.9199  0.9199  0.9189  0.9180  0.9143  0.9074  0.9074  0.8971  0.8930  0.8930  0.8930  0.8930  0.8900  0.8900
[33 : 48]:	0.8900  0.8877  0.8860  0.8834  0.8809  0.8809  0.8743  0.8743  0.8743  0.8734  0.8716  0.8710  0.8710  0.8710  0.8701  0.8692
[49 : 64]:	0.8664  0.8645  0.8641  0.8641  0.8628  0.8617  0.8615  0.8615  0.8615  0.8615  0.8613  0.8601  0.8601  0.8589  0.8539  0.8539
2023-05-18 15:39:47 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 15:39:47 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #321: GFLOPs: 1117.3867. Time: 9475.6780 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #322: GFLOPs: 1150.4793. Time: 9203.1175 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #323: GFLOPs: 1200.3567. Time: 8820.7083 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #324: GFLOPs: 1108.5346. Time: 9551.3447 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #325: GFLOPs: 1124.0942. Time: 9419.1364 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #326: GFLOPs: 1132.9205. Time: 9345.7538 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #327: GFLOPs: 1151.0530. Time: 9198.5303 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #328: GFLOPs: 1111.7602. Time: 9523.6326 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #329: GFLOPs: 1135.8918. Time: 9321.3068 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #330: GFLOPs: 1178.4653. Time: 8984.5637 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #331: GFLOPs: 1167.0346. Time: 9072.5638 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #332: GFLOPs: 1293.4689. Time: 8185.7372 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #333: GFLOPs: 1147.6155. Time: 9226.0834 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #334: GFLOPs: 1160.5509. Time: 9123.2500 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #335: GFLOPs: 1256.5148. Time: 8426.4792 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #336: GFLOPs: 1186.4977. Time: 8923.7396 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #337: GFLOPs: 1179.1084. Time: 8979.6632 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #338: GFLOPs: 1238.0278. Time: 8552.3090 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #339: GFLOPs: 1254.6273. Time: 8439.1562 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #340: GFLOPs: 1253.5690. Time: 8446.2812 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #341: GFLOPs: 1166.3603. Time: 9077.8088 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #342: GFLOPs: 1198.2021. Time: 8836.5694 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #343: GFLOPs: 1112.2119. Time: 9519.7651 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #344: GFLOPs: 1159.8878. Time: 9128.4659 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #345: GFLOPs: 1216.7347. Time: 8701.9758 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #346: GFLOPs: 1206.3887. Time: 8776.6042 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #347: GFLOPs: 1204.3271. Time: 8791.6285 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #348: GFLOPs: 1155.3625. Time: 9164.2197 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #349: GFLOPs: 1033.2907. Time: 10246.8708 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #350: GFLOPs: 1114.0506. Time: 9504.0531 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #351: GFLOPs: 1174.0863. Time: 9018.0735 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #352: GFLOPs: 1150.2790. Time: 9204.7197 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #353: GFLOPs: 1200.2997. Time: 8821.1274 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #354: GFLOPs: 1090.0238. Time: 9713.5455 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #355: GFLOPs: 1149.6177. Time: 9210.0152 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #356: GFLOPs: 1205.1533. Time: 8785.6007 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #357: GFLOPs: 1038.4325. Time: 10196.1334 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #358: GFLOPs: 1114.9256. Time: 9496.5947 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #359: GFLOPs: 1152.9450. Time: 9183.4356 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #360: GFLOPs: 1109.2856. Time: 9544.8787 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #361: GFLOPs: 1110.7539. Time: 9532.2614 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #362: GFLOPs: 1148.5298. Time: 9218.7386 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #363: GFLOPs: 1117.0187. Time: 9478.7992 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #364: GFLOPs: 1108.1395. Time: 9554.7500 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #365: GFLOPs: 1215.8072. Time: 8708.6146 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #366: GFLOPs: 1099.9503. Time: 9625.8864 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #367: GFLOPs: 1165.3808. Time: 9085.4387 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #368: GFLOPs: 1002.2838. Time: 10563.8708 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #369: GFLOPs: 1157.5980. Time: 9146.5227 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #370: GFLOPs: 1135.2552. Time: 9326.5341 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #371: GFLOPs: 1012.7507. Time: 10454.6916 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #372: GFLOPs: 964.1367. Time: 10981.8417 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #373: GFLOPs: 1155.5975. Time: 9162.3561 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #374: GFLOPs: 1118.4288. Time: 9466.8485 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #375: GFLOPs: 1209.4299. Time: 8754.5347 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #376: GFLOPs: 1128.2425. Time: 9384.5038 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #377: GFLOPs: 1149.4654. Time: 9211.2348 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #378: GFLOPs: 1283.6907. Time: 8248.0898 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #379: GFLOPs: 1104.6370. Time: 9585.0455 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #380: GFLOPs: 1188.5116. Time: 8908.6181 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #381: GFLOPs: 1153.7792. Time: 9176.7955 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #382: GFLOPs: 479.6345. Time: 22075.1334 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #383: GFLOPs: 97.7465. Time: 108320.9447 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #384: GFLOPs: 101.7353. Time: 104073.9167 us. Best GFLOPs: 1387.2786
2023-05-18 15:44:19 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 15:44:20 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 15:44:21 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 407 failure(s)
2023-05-18 15:44:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 816 failure(s)
2023-05-18 15:44:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1226 failure(s)
2023-05-18 15:44:27 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1629 failure(s)
2023-05-18 15:44:27 [INFO] [evolutionary_search.cc:723] Sampled 11 candidate(s)
2023-05-18 15:44:31 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 58 failure(s)
2023-05-18 15:44:37 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 61 failure(s)
2023-05-18 15:44:43 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 55 failure(s)
2023-05-18 15:44:49 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 58 failure(s)
2023-05-18 15:44:51 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9639  0.9639  0.9639  0.9418  0.9327  0.9327  0.9325  0.9279  0.9199  0.9199  0.9132  0.9074  0.9056  0.9056  0.9056  0.9056
[17 : 32]:	0.8971  0.8971  0.8928  0.8921  0.8877  0.8860  0.8860  0.8827  0.8827  0.8827  0.8809  0.8783  0.8764  0.8743  0.8743  0.8722
[33 : 48]:	0.8722  0.8717  0.8716  0.8716  0.8716  0.8716  0.8716  0.8710  0.8710  0.8655  0.8628  0.8617  0.8617  0.8617  0.8617  0.8615
[49 : 64]:	0.8615  0.8601  0.8589  0.8579  0.8577  0.8539  0.8523  0.8523  0.8523  0.8514  0.8514  0.8508  0.8500  0.8498  0.8498  0.8496
2023-05-18 15:44:51 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 15:44:51 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #385: GFLOPs: 1107.1169. Time: 9563.5758 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #386: GFLOPs: 1206.0538. Time: 8779.0417 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #387: GFLOPs: 1132.4441. Time: 9349.6855 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #388: GFLOPs: 1113.1912. Time: 9511.3902 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #389: GFLOPs: 1146.5695. Time: 9234.5000 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #390: GFLOPs: 1333.2844. Time: 7941.2885 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #391: GFLOPs: 1141.2329. Time: 9277.6818 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #392: GFLOPs: 1213.5910. Time: 8724.5174 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #393: GFLOPs: 1143.8927. Time: 9256.1099 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #394: GFLOPs: 1164.3303. Time: 9093.6364 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #395: GFLOPs: 1160.4045. Time: 9124.4015 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #396: GFLOPs: 1177.6585. Time: 8990.7188 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #397: GFLOPs: 1161.9968. Time: 9111.8977 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #398: GFLOPs: 1260.0028. Time: 8403.1528 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #399: GFLOPs: 1214.7862. Time: 8715.9341 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #400: GFLOPs: 844.9980. Time: 12530.2031 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #401: GFLOPs: 1251.0679. Time: 8463.1667 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #402: GFLOPs: 1159.2538. Time: 9133.4584 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #403: GFLOPs: 1110.1636. Time: 9537.3295 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #404: GFLOPs: 1145.2551. Time: 9245.0985 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #405: GFLOPs: 889.3507. Time: 11905.3101 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #406: GFLOPs: 1262.0120. Time: 8389.7743 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #407: GFLOPs: 1215.3585. Time: 8711.8298 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #408: GFLOPs: 1274.7854. Time: 8305.7084 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #409: GFLOPs: 1170.0020. Time: 9049.5539 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #410: GFLOPs: 1206.3038. Time: 8777.2222 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #411: GFLOPs: 1121.5661. Time: 9440.3675 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #412: GFLOPs: 1169.5243. Time: 9053.2500 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #413: GFLOPs: 831.2355. Time: 12737.6615 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #414: GFLOPs: 1018.3981. Time: 10396.7166 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #415: GFLOPs: 1192.4454. Time: 8879.2292 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #416: GFLOPs: 1131.4576. Time: 9357.8371 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #417: GFLOPs: 1102.4908. Time: 9603.7045 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #418: GFLOPs: 1208.7003. Time: 8759.8194 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #419: GFLOPs: 1219.5484. Time: 8681.8993 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #420: GFLOPs: 1125.6568. Time: 9406.0606 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #421: GFLOPs: 1031.8310. Time: 10261.3667 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #422: GFLOPs: 1159.0481. Time: 9135.0795 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #423: GFLOPs: 1112.0035. Time: 9521.5493 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #424: GFLOPs: 1104.8034. Time: 9583.6023 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #425: GFLOPs: 1158.0747. Time: 9142.7576 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #426: GFLOPs: 1110.9463. Time: 9530.6098 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #427: GFLOPs: 1088.6501. Time: 9725.8031 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #428: GFLOPs: 1176.2925. Time: 9001.1593 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #429: GFLOPs: 1329.8510. Time: 7961.7916 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #430: GFLOPs: 1259.4944. Time: 8406.5451 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #431: GFLOPs: 1102.9289. Time: 9599.8902 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #432: GFLOPs: 1158.7852. Time: 9137.1515 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #433: GFLOPs: 1114.5223. Time: 9500.0304 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #434: GFLOPs: 1268.1026. Time: 8349.4792 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #435: GFLOPs: 1217.2690. Time: 8698.1562 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #436: GFLOPs: 1080.1694. Time: 9802.1629 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #437: GFLOPs: 1153.3103. Time: 9180.5265 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #438: GFLOPs: 1175.8311. Time: 9004.6910 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #439: GFLOPs: 1121.8258. Time: 9438.1818 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #440: GFLOPs: 1111.5825. Time: 9525.1553 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #441: GFLOPs: 1128.6439. Time: 9381.1667 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #442: GFLOPs: 1005.0893. Time: 10534.3833 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #443: GFLOPs: 915.7520. Time: 11562.0787 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #444: GFLOPs: 814.4994. Time: 12999.3906 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #445: GFLOPs: 1097.1625. Time: 9650.3446 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #446: GFLOPs: 44.3686. Time: 238637.0557 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #447: GFLOPs: 29.7640. Time: 355732.1530 us. Best GFLOPs: 1387.2786
2023-05-18 15:47:40 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #448: GFLOPs: 14.0114. Time: 755671.6667 us. Best GFLOPs: 1387.2786
2023-05-18 16:10:31 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 16:10:31 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 16:10:33 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 408 failure(s)
2023-05-18 16:10:35 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 811 failure(s)
2023-05-18 16:10:37 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1217 failure(s)
2023-05-18 16:10:37 [INFO] [evolutionary_search.cc:723] Sampled 13 candidate(s)
2023-05-18 16:10:41 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 81 failure(s)
2023-05-18 16:10:47 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 57 failure(s)
2023-05-18 16:10:53 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 61 failure(s)
2023-05-18 16:10:59 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 59 failure(s)
2023-05-18 16:11:01 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.8867  0.8792  0.8792  0.8754  0.8754  0.8708  0.8708  0.8663  0.8663  0.8658  0.8658  0.8657  0.8644  0.8640  0.8603  0.8603
[17 : 32]:	0.8603  0.8564  0.8564  0.8516  0.8504  0.8475  0.8475  0.8475  0.8475  0.8475  0.8474  0.8464  0.8453  0.8430  0.8429  0.8406
[33 : 48]:	0.8406  0.8406  0.8397  0.8397  0.8397  0.8397  0.8392  0.8391  0.8389  0.8383  0.8363  0.8363  0.8363  0.8358  0.8354  0.8353
[49 : 64]:	0.8353  0.8342  0.8342  0.8342  0.8327  0.8327  0.8322  0.8322  0.8322  0.8322  0.8305  0.8299  0.8299  0.8299  0.8297  0.8285
2023-05-18 16:11:02 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 16:11:02 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #449: GFLOPs: 1095.6745. Time: 9663.4507 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #450: GFLOPs: 1269.6914. Time: 8339.0312 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #451: GFLOPs: 1173.4341. Time: 9023.0858 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #452: GFLOPs: 1158.6205. Time: 9138.4508 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #453: GFLOPs: 1086.8456. Time: 9741.9508 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #454: GFLOPs: 1161.6264. Time: 9114.8031 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #455: GFLOPs: 1158.0219. Time: 9143.1743 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #456: GFLOPs: 1263.3971. Time: 8380.5764 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #457: GFLOPs: 1169.6773. Time: 9052.0662 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #458: GFLOPs: 1074.7780. Time: 9851.3334 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #459: GFLOPs: 1157.1983. Time: 9149.6818 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #460: GFLOPs: 1148.1189. Time: 9222.0379 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #461: GFLOPs: 1211.9564. Time: 8736.2847 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #462: GFLOPs: 1197.6760. Time: 8840.4513 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #463: GFLOPs: 1064.2575. Time: 9948.7162 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #464: GFLOPs: 1159.3504. Time: 9132.6970 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #465: GFLOPs: 1171.6051. Time: 9037.1715 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #466: GFLOPs: 1196.7509. Time: 8847.2848 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #467: GFLOPs: 1222.6809. Time: 8659.6562 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #468: GFLOPs: 1176.8822. Time: 8996.6493 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #469: GFLOPs: 1179.8241. Time: 8974.2157 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #470: GFLOPs: 1262.8399. Time: 8384.2743 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #471: GFLOPs: 1217.5373. Time: 8696.2396 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #472: GFLOPs: 1176.6594. Time: 8998.3529 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #473: GFLOPs: 1187.9551. Time: 8912.7917 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #474: GFLOPs: 1151.4233. Time: 9195.5719 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #475: GFLOPs: 1175.8633. Time: 9004.4444 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #476: GFLOPs: 926.0886. Time: 11433.0278 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #477: GFLOPs: 1188.7836. Time: 8906.5798 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #478: GFLOPs: 1142.7114. Time: 9265.6780 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #479: GFLOPs: 1233.1464. Time: 8586.1632 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #480: GFLOPs: 1178.2450. Time: 8986.2431 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #481: GFLOPs: 1236.3277. Time: 8564.0694 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #482: GFLOPs: 1097.9468. Time: 9643.4508 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #483: GFLOPs: 1177.2788. Time: 8993.6181 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #484: GFLOPs: 1164.4438. Time: 9092.7500 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #485: GFLOPs: 1236.8623. Time: 8560.3681 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #486: GFLOPs: 1081.6821. Time: 9788.4545 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #487: GFLOPs: 1144.0631. Time: 9254.7311 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #488: GFLOPs: 1151.1403. Time: 9197.8333 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #489: GFLOPs: 1127.8160. Time: 9388.0530 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #490: GFLOPs: 1155.3945. Time: 9163.9659 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #491: GFLOPs: 1129.1020. Time: 9377.3599 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #492: GFLOPs: 1129.4460. Time: 9374.5037 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #493: GFLOPs: 1155.4165. Time: 9163.7916 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #494: GFLOPs: 1179.0883. Time: 8979.8160 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #495: GFLOPs: 889.4645. Time: 11903.7870 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #496: GFLOPs: 1158.1023. Time: 9142.5392 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #497: GFLOPs: 1097.3589. Time: 9648.6174 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #498: GFLOPs: 1222.2898. Time: 8662.4271 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #499: GFLOPs: 1197.6082. Time: 8840.9513 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #500: GFLOPs: 1179.0943. Time: 8979.7708 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #501: GFLOPs: 1146.3833. Time: 9236.0000 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #502: GFLOPs: 1197.6397. Time: 8840.7187 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #503: GFLOPs: 1101.2907. Time: 9614.1705 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #504: GFLOPs: 1249.4818. Time: 8473.9097 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #505: GFLOPs: 1122.9002. Time: 9429.1515 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #506: GFLOPs: 1193.9638. Time: 8867.9375 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #507: GFLOPs: 1197.9785. Time: 8838.2188 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #508: GFLOPs: 928.9432. Time: 11397.8936 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #509: GFLOPs: 1161.7978. Time: 9113.4583 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #510: GFLOPs: 11.1029. Time: 953627.5970 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #511: GFLOPs: 10.9402. Time: 967803.6110 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #512: GFLOPs: 380.9360. Time: 27794.6875 us. Best GFLOPs: 1387.2786
2023-05-18 16:12:48 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 16:12:49 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 16:12:50 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 406 failure(s)
2023-05-18 16:12:52 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 814 failure(s)
2023-05-18 16:12:54 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1223 failure(s)
2023-05-18 16:12:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1629 failure(s)
2023-05-18 16:12:55 [INFO] [evolutionary_search.cc:723] Sampled 11 candidate(s)
2023-05-18 16:13:00 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 60 failure(s)
2023-05-18 16:13:05 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 67 failure(s)
2023-05-18 16:13:12 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 59 failure(s)
2023-05-18 16:13:18 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 65 failure(s)
2023-05-18 16:13:20 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.8757  0.8757  0.8757  0.8643  0.8643  0.8626  0.8626  0.8626  0.8626  0.8608  0.8573  0.8548  0.8527  0.8503  0.8461  0.8447
[17 : 32]:	0.8447  0.8447  0.8442  0.8442  0.8442  0.8424  0.8418  0.8402  0.8387  0.8387  0.8378  0.8362  0.8357  0.8347  0.8312  0.8295
[33 : 48]:	0.8293  0.8290  0.8269  0.8260  0.8259  0.8249  0.8249  0.8247  0.8247  0.8247  0.8244  0.8243  0.8207  0.8165  0.8158  0.8134
[49 : 64]:	0.8125  0.8076  0.8071  0.8069  0.8069  0.8066  0.8034  0.8034  0.8016  0.8006  0.7989  0.7961  0.7961  0.7960  0.7954  0.7948
2023-05-18 16:13:20 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 16:13:20 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #513: GFLOPs: 1233.0656. Time: 8586.7258 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #514: GFLOPs: 1192.2142. Time: 8880.9514 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #515: GFLOPs: 1191.6961. Time: 8884.8125 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #516: GFLOPs: 1084.9291. Time: 9759.1591 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #517: GFLOPs: 1153.5636. Time: 9178.5114 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #518: GFLOPs: 1199.4251. Time: 8827.5590 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #519: GFLOPs: 1175.6104. Time: 9006.3820 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #520: GFLOPs: 1266.1397. Time: 8362.4236 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #521: GFLOPs: 1192.3974. Time: 8879.5868 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #522: GFLOPs: 1099.5332. Time: 9629.5379 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #523: GFLOPs: 1178.3233. Time: 8985.6458 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #524: GFLOPs: 1237.1925. Time: 8558.0833 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #525: GFLOPs: 1130.3142. Time: 9367.3031 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #526: GFLOPs: 1122.2537. Time: 9434.5833 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #527: GFLOPs: 1153.8383. Time: 9176.3257 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #528: GFLOPs: 1161.7457. Time: 9113.8675 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #529: GFLOPs: 1102.2452. Time: 9605.8446 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #530: GFLOPs: 1262.7285. Time: 8385.0139 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #531: GFLOPs: 1170.9607. Time: 9042.1446 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #532: GFLOPs: 1210.2531. Time: 8748.5799 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #533: GFLOPs: 1191.4223. Time: 8886.8542 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #534: GFLOPs: 1152.4155. Time: 9187.6554 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #535: GFLOPs: 1218.2635. Time: 8691.0556 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #536: GFLOPs: 1386.0531. Time: 7638.9544 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #537: GFLOPs: 1219.9704. Time: 8678.8958 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #538: GFLOPs: 1110.2721. Time: 9536.3977 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #539: GFLOPs: 1020.7179. Time: 10373.0875 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #540: GFLOPs: 1147.3117. Time: 9228.5265 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #541: GFLOPs: 1179.7539. Time: 8974.7500 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #542: GFLOPs: 1128.3609. Time: 9383.5189 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #543: GFLOPs: 1142.1983. Time: 9269.8409 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #544: GFLOPs: 1183.2426. Time: 8948.2883 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #545: GFLOPs: 1102.0679. Time: 9607.3902 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #546: GFLOPs: 1125.0035. Time: 9411.5227 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #547: GFLOPs: 1120.6291. Time: 9448.2614 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #548: GFLOPs: 1207.8304. Time: 8766.1285 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #549: GFLOPs: 1206.0685. Time: 8778.9341 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #550: GFLOPs: 1233.0905. Time: 8586.5521 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #551: GFLOPs: 1139.6723. Time: 9290.3864 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #552: GFLOPs: 1094.9654. Time: 9669.7084 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #553: GFLOPs: 1258.5597. Time: 8412.7882 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #554: GFLOPs: 1176.6615. Time: 8998.3368 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #555: GFLOPs: 1363.8427. Time: 7763.3558 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #556: GFLOPs: 1179.1668. Time: 8979.2181 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #557: GFLOPs: 1162.2398. Time: 9109.9925 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #558: GFLOPs: 1186.0994. Time: 8926.7361 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #559: GFLOPs: 1019.5561. Time: 10384.9083 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #560: GFLOPs: 1105.3119. Time: 9579.1932 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #561: GFLOPs: 839.3065. Time: 12615.1719 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #562: GFLOPs: 1034.2790. Time: 10237.0791 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #563: GFLOPs: 926.9575. Time: 11422.3101 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #564: GFLOPs: 1203.9267. Time: 8794.5521 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #565: GFLOPs: 1087.2941. Time: 9737.9318 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #566: GFLOPs: 1191.3259. Time: 8887.5729 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #567: GFLOPs: 1127.0320. Time: 9394.5834 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #568: GFLOPs: 1070.1091. Time: 9894.3145 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #569: GFLOPs: 1131.9575. Time: 9353.7045 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #570: GFLOPs: 1227.7005. Time: 8624.2500 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #571: GFLOPs: 984.8463. Time: 10750.9125 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #572: GFLOPs: 1141.1360. Time: 9278.4696 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #573: GFLOPs: 1028.8619. Time: 10290.9792 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #574: GFLOPs: 107.3781. Time: 98604.8197 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #575: GFLOPs: 7.7070. Time: 1373817.3473 us. Best GFLOPs: 1387.2786
2023-05-18 16:16:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #576: GFLOPs: 11.3142. Time: 935817.2500 us. Best GFLOPs: 1387.2786
2023-05-18 16:39:18 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 16:39:18 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 16:39:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 406 failure(s)
2023-05-18 16:39:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 813 failure(s)
2023-05-18 16:39:24 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1222 failure(s)
2023-05-18 16:39:25 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1629 failure(s)
2023-05-18 16:39:25 [INFO] [evolutionary_search.cc:723] Sampled 11 candidate(s)
2023-05-18 16:39:29 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 51 failure(s)
2023-05-18 16:39:36 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 58 failure(s)
2023-05-18 16:39:42 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 72 failure(s)
2023-05-18 16:39:48 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 56 failure(s)
2023-05-18 16:39:50 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.8714  0.8689  0.8679  0.8665  0.8652  0.8583  0.8549  0.8527  0.8518  0.8470  0.8464  0.8411  0.8402  0.8399  0.8387  0.8362
[17 : 32]:	0.8336  0.8335  0.8332  0.8322  0.8317  0.8315  0.8290  0.8290  0.8269  0.8269  0.8263  0.8244  0.8244  0.8244  0.8241  0.8219
[33 : 48]:	0.8206  0.8199  0.8199  0.8180  0.8180  0.8180  0.8176  0.8165  0.8165  0.8164  0.8134  0.8125  0.8121  0.8111  0.8086  0.8079
[49 : 64]:	0.8074  0.8066  0.8042  0.8042  0.8022  0.8022  0.8022  0.8006  0.7983  0.7983  0.7977  0.7961  0.7960  0.7960  0.7942  0.7937
2023-05-18 16:39:50 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 16:39:50 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #577: GFLOPs: 1239.7003. Time: 8540.7708 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #578: GFLOPs: 1163.3437. Time: 9101.3481 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #579: GFLOPs: 1208.8431. Time: 8758.7848 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #580: GFLOPs: 1209.5628. Time: 8753.5729 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #581: GFLOPs: 1121.9123. Time: 9437.4545 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #582: GFLOPs: 1235.3020. Time: 8571.1806 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #583: GFLOPs: 1214.1965. Time: 8720.1667 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #584: GFLOPs: 1130.2823. Time: 9367.5682 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #585: GFLOPs: 1224.7149. Time: 8645.2743 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #586: GFLOPs: 1213.2757. Time: 8726.7847 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #587: GFLOPs: 1217.2583. Time: 8698.2327 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #588: GFLOPs: 1211.6670. Time: 8738.3716 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #589: GFLOPs: 1067.7978. Time: 9915.7318 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #590: GFLOPs: 1228.8379. Time: 8616.2673 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #591: GFLOPs: 1143.7452. Time: 9257.3030 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #592: GFLOPs: 1207.5979. Time: 8767.8160 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #593: GFLOPs: 1176.4757. Time: 8999.7574 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #594: GFLOPs: 1259.3784. Time: 8407.3194 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #595: GFLOPs: 1149.6484. Time: 9209.7689 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #596: GFLOPs: 1370.5996. Time: 7725.0833 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #597: GFLOPs: 1207.9237. Time: 8765.4514 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #598: GFLOPs: 1268.7864. Time: 8344.9792 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #599: GFLOPs: 1151.9890. Time: 9191.0568 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #600: GFLOPs: 1136.5426. Time: 9315.9697 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #601: GFLOPs: 1239.0173. Time: 8545.4792 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #602: GFLOPs: 1171.2776. Time: 9039.6985 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #603: GFLOPs: 1173.5316. Time: 9022.3358 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #604: GFLOPs: 1273.2813. Time: 8315.5197 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #605: GFLOPs: 1163.8944. Time: 9097.0416 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #606: GFLOPs: 1200.5178. Time: 8819.5243 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #607: GFLOPs: 1254.4782. Time: 8440.1597 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #608: GFLOPs: 1214.2052. Time: 8720.1042 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #609: GFLOPs: 1242.2102. Time: 8523.5139 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #610: GFLOPs: 1090.9990. Time: 9704.8636 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #611: GFLOPs: 1075.4537. Time: 9845.1439 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #612: GFLOPs: 1180.1447. Time: 8971.7777 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #613: GFLOPs: 1184.1957. Time: 8941.0868 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #614: GFLOPs: 1252.1812. Time: 8455.6423 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #615: GFLOPs: 827.0253. Time: 12802.5053 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #616: GFLOPs: 1054.7800. Time: 10038.1083 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #617: GFLOPs: 1138.5717. Time: 9299.3674 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #618: GFLOPs: 1378.8954. Time: 7678.6071 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #619: GFLOPs: 936.5166. Time: 11305.7222 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #620: GFLOPs: 1220.1925. Time: 8677.3160 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #621: GFLOPs: 1232.0269. Time: 8593.9652 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #622: GFLOPs: 1258.9463. Time: 8410.2048 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #623: GFLOPs: 1107.0169. Time: 9564.4394 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #624: GFLOPs: 1132.1368. Time: 9352.2235 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #625: GFLOPs: 1094.2307. Time: 9676.2007 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #626: GFLOPs: 1101.0603. Time: 9616.1818 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #627: GFLOPs: 1137.5944. Time: 9307.3561 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #628: GFLOPs: 1204.2205. Time: 8792.4062 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #629: GFLOPs: 1243.6856. Time: 8513.4027 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #630: GFLOPs: 1175.7217. Time: 9005.5294 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #631: GFLOPs: 1205.7581. Time: 8781.1945 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #632: GFLOPs: 1203.2346. Time: 8799.6111 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #633: GFLOPs: 1164.1927. Time: 9094.7108 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #634: GFLOPs: 1166.7242. Time: 9074.9779 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #635: GFLOPs: 1281.5443. Time: 8261.9038 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #636: GFLOPs: 1057.8321. Time: 10009.1459 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #637: GFLOPs: 1144.7110. Time: 9249.4925 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #638: GFLOPs: 15.7278. Time: 673202.3193 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #639: GFLOPs: 57.6268. Time: 183733.9167 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #640: GFLOPs: 601.9239. Time: 17590.2570 us. Best GFLOPs: 1387.2786
2023-05-18 16:40:51 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 16:40:51 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 16:40:53 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 404 failure(s)
2023-05-18 16:40:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 813 failure(s)
2023-05-18 16:40:56 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1220 failure(s)
2023-05-18 16:40:56 [INFO] [evolutionary_search.cc:723] Sampled 10 candidate(s)
2023-05-18 16:41:01 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 73 failure(s)
2023-05-18 16:41:07 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 69 failure(s)
2023-05-18 16:41:13 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 71 failure(s)
2023-05-18 16:41:19 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 64 failure(s)
2023-05-18 16:41:21 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.8679  0.8565  0.8517  0.8362  0.8342  0.8333  0.8333  0.8332  0.8308  0.8300  0.8288  0.8269  0.8269  0.8266  0.8257  0.8253
[17 : 32]:	0.8249  0.8248  0.8244  0.8244  0.8244  0.8241  0.8236  0.8228  0.8227  0.8223  0.8207  0.8205  0.8199  0.8190  0.8186  0.8184
[33 : 48]:	0.8180  0.8180  0.8159  0.8137  0.8136  0.8122  0.8122  0.8111  0.8104  0.8095  0.8069  0.8061  0.8061  0.8058  0.8058  0.8050
[49 : 64]:	0.8050  0.8049  0.8037  0.8034  0.8034  0.8006  0.7995  0.7983  0.7960  0.7953  0.7921  0.7912  0.7910  0.7905  0.7904  0.7904
2023-05-18 16:41:21 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 16:41:21 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #641: GFLOPs: 1197.7479. Time: 8839.9202 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #642: GFLOPs: 1227.1057. Time: 8628.4305 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #643: GFLOPs: 1206.6451. Time: 8774.7396 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #644: GFLOPs: 1146.6118. Time: 9234.1591 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #645: GFLOPs: 1136.0885. Time: 9319.6932 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #646: GFLOPs: 1261.6481. Time: 8392.1944 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #647: GFLOPs: 1057.4729. Time: 10012.5458 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #648: GFLOPs: 1110.3969. Time: 9535.3257 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #649: GFLOPs: 1077.6399. Time: 9825.1705 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #650: GFLOPs: 1173.1631. Time: 9025.1701 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #651: GFLOPs: 1259.3336. Time: 8407.6181 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #652: GFLOPs: 1203.9776. Time: 8794.1806 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #653: GFLOPs: 1180.0252. Time: 8972.6862 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #654: GFLOPs: 1240.1112. Time: 8537.9410 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #655: GFLOPs: 1129.6752. Time: 9372.6023 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #656: GFLOPs: 1123.8605. Time: 9421.0946 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #657: GFLOPs: 1203.9210. Time: 8794.5937 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #658: GFLOPs: 854.5614. Time: 12389.9769 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #659: GFLOPs: 1202.9417. Time: 8801.7534 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #660: GFLOPs: 1198.0049. Time: 8838.0243 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #661: GFLOPs: 1260.1205. Time: 8402.3681 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #662: GFLOPs: 1257.7042. Time: 8418.5104 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #663: GFLOPs: 1202.9811. Time: 8801.4653 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #664: GFLOPs: 1040.8271. Time: 10172.6750 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #665: GFLOPs: 1045.6673. Time: 10125.5875 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #666: GFLOPs: 1118.9813. Time: 9462.1743 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #667: GFLOPs: 1202.4247. Time: 8805.5382 us. Best GFLOPs: 1387.2786
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #668: GFLOPs: 1387.2955. Time: 7632.1131 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #669: GFLOPs: 1146.4745. Time: 9235.2652 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #670: GFLOPs: 915.1895. Time: 11569.1851 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #671: GFLOPs: 1199.5643. Time: 8826.5348 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #672: GFLOPs: 773.5902. Time: 13686.8281 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #673: GFLOPs: 1261.3548. Time: 8394.1458 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #674: GFLOPs: 1199.6884. Time: 8825.6215 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #675: GFLOPs: 861.5933. Time: 12288.8564 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #676: GFLOPs: 1211.8129. Time: 8737.3194 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #677: GFLOPs: 1218.2971. Time: 8690.8159 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #678: GFLOPs: 1195.8320. Time: 8854.0833 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #679: GFLOPs: 1191.4940. Time: 8886.3194 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #680: GFLOPs: 902.6713. Time: 11729.6250 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #681: GFLOPs: 1372.0364. Time: 7716.9935 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #682: GFLOPs: 857.8492. Time: 12342.4907 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #683: GFLOPs: 1227.2953. Time: 8627.0973 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #684: GFLOPs: 1045.5413. Time: 10126.8084 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #685: GFLOPs: 1318.2824. Time: 8031.6603 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #686: GFLOPs: 1219.0223. Time: 8685.6458 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #687: GFLOPs: 1259.6260. Time: 8405.6667 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #688: GFLOPs: 1143.8145. Time: 9256.7424 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #689: GFLOPs: 1059.1518. Time: 9996.6744 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #690: GFLOPs: 1205.5660. Time: 8782.5938 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #691: GFLOPs: 862.3971. Time: 12277.4028 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #692: GFLOPs: 1054.0861. Time: 10044.7167 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #693: GFLOPs: 929.5141. Time: 11390.8936 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #694: GFLOPs: 1183.8325. Time: 8943.8298 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #695: GFLOPs: 902.7497. Time: 11728.6064 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #696: GFLOPs: 1256.4139. Time: 8427.1562 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #697: GFLOPs: 1059.9363. Time: 9989.2760 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #698: GFLOPs: 908.6578. Time: 11652.3472 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #699: GFLOPs: 822.8620. Time: 12867.2812 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #700: GFLOPs: 1191.6891. Time: 8884.8646 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #701: GFLOPs: 970.9523. Time: 10904.7542 us. Best GFLOPs: 1387.2955
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:121] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #702: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x0000000101d9a033
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[16:42:15] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32"), self_rrdb_body_0_rdb1_conv1_weight: T.Buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), "float32"), lv4: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_body_0_rdb1_conv1_weight_shared = T.alloc_buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(40), thread="blockIdx.x"):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(1), T.int64(4), T.int64(2), T.int64(7)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3_init * T.int64(4) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(4) + yy_3_init * T.int64(2) + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(28) + xx_3_init * T.int64(7) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(1), T.int64(3)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(252)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("pad_temp_shared"):
                                    v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                    v1 = T.axis.spatial(T.int64(64), rc_0)
                                    v2 = T.axis.spatial(T.int64(642), nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(448))
                                    v3 = T.axis.spatial(T.int64(450), rx_0 + (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(448))
                                    T.reads(lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                    T.writes(pad_temp_shared[v0, v1, v2, v3])
                                    pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(2)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("self_rrdb_body_0_rdb1_conv1.weight_shared"):
                                        v0 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(3))
                                        v1 = T.axis.spatial(T.int64(64), rc_0)
                                        v2 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(3))
                                        v3 = T.axis.spatial(T.int64(3), rx_0)
                                        T.where((ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) * T.int64(2) + ax0_ax1_ax2_ax3_fused_2 < T.int64(96))
                                        T.reads(self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3])
                                        T.writes(self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3])
                                        self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3] = self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(2), T.int64(2), T.int64(4), T.int64(1), T.int64(3), T.int64(1), T.int64(1), T.int64(4), T.int64(2), T.int64(7)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ff_3 * T.int64(4) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(4) + yy_3 * T.int64(2) + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(28) + xx_3 * T.int64(7) + xx_4)
                                v_rc = T.axis.reduce(T.int64(64), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 * T.int64(3) + ry_1 * T.int64(3) + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 + rx_1 + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(8), T.int64(4), T.int64(28)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(4) * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(16) * T.int64(8) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused * T.int64(16) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(16) // T.int64(4) * T.int64(4) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_1_ff_1_yy_1_xx_1_fused % T.int64(4) * T.int64(112) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(4) * T.int64(28) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv4[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_compute_intermediate[v0, v1, v2, v3])
                            var_compute_intermediate[v0, v1, v2, v3] = T.Select(T.float32(0) < var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.20000000000000001))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 2, 2, 2, 4])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[40, 1, 4, 2, 2])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[1, 4, 4, 4, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=0)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109 = sch.split(loop=l107, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l110, l111, l112, l113, l114, l115, l116 = sch.get_loops(block=b87)
l117, l118, l119 = sch.split(loop=l116, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l119)
sch.bind(loop=l118, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132 = sch.get_loops(block=b121)
l133, l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:121] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #703: Error in running:
RPCRunner: An exception occurred
Traceback (most recent call last):
  File "/Users/guoyaol/tvm/python/tvm/exec/popen_worker.py", line 87, in main
    result = fn(*args, **kwargs)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 403, in _worker_func
    costs: List[float] = f_run_evaluator(
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/rpc_runner.py", line 515, in default_run_evaluator
    return run_evaluator_common(rt_mod, device, evaluator_config, repeated_args)
  File "/Users/guoyaol/tvm/python/tvm/meta_schedule/runner/utils.py", line 117, in run_evaluator_common
    profile_result = evaluator(*args)
  File "/Users/guoyaol/tvm/python/tvm/runtime/module.py", line 403, in evaluator
    blob = feval(*args)
  File "/Users/guoyaol/tvm/python/tvm/_ffi/_ctypes/packed_func.py", line 238, in __call__
    raise get_last_ffi_error()
tvm.error.RPCError: Traceback (most recent call last):
  [bt] (8) 9   libtvm.dylib                        0x00000001223bf3e4 tvm::runtime::RPCClientSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&) + 160
  [bt] (7) 8   libtvm.dylib                        0x00000001223b80a8 tvm::runtime::RPCEndpoint::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)>) + 332
  [bt] (6) 7   libtvm.dylib                        0x00000001223b6b10 tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 556
  [bt] (5) 6   libtvm.dylib                        0x00000001223b6dfc tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>) + 388
  [bt] (4) 5   libtvm.dylib                        0x00000001223ba95c tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>) + 372
  [bt] (3) 4   libtvm.dylib                        0x00000001223bc580 tvm::runtime::RPCEndpoint::EventHandler::HandleReturn(tvm::runtime::RPCCode, std::__1::function<void (tvm::runtime::TVMArgs)>) + 312
  [bt] (2) 3   libtvm.dylib                        0x0000000120003a44 __clang_call_terminate + 0
  [bt] (1) 2   libtvm.dylib                        0x0000000120005e20 tvm::runtime::detail::LogFatal::Entry::Finalize() + 0
  [bt] (0) 1   libtvm.dylib                        0x0000000120005e74 tvm::runtime::detail::LogFatal::Entry::Finalize() + 84
  18: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  14: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  13: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  12: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  11: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  10: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  9: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  8: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  7: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  6: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  5: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  4: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  3: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  2: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  1: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  0: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87
  29: TVMFuncCall
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:477
  28: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  27: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::$_1> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  26: tvm::runtime::$_1::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:138
  25: tvm::runtime::RPCServerLoop(int)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_socket_impl.cc:119
  24: tvm::runtime::RPCEndpoint::ServerLoop()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:738
  23: tvm::runtime::RPCEndpoint::HandleUntilReturnEvent(bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:647
  22: tvm::runtime::RPCEndpoint::EventHandler::HandleNextEvent(bool, bool, std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:135
  21: tvm::runtime::RPCEndpoint::EventHandler::HandleProcessPacket(std::__1::function<void (tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:302
  20: tvm::runtime::RPCEndpoint::EventHandler::HandleNormalCallFunc()
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_endpoint.cc:479
  19: tvm::runtime::RPCSession::AsyncCallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::RPCCode, tvm::runtime::TVMArgs)>)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_session.cc:47
  18: tvm::runtime::LocalSession::CallFunc(void*, TVMValue const*, int const*, int, std::__1::function<void (tvm::runtime::TVMArgs)> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/rpc/rpc_local_session.cc:91
  17: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  16: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  15: tvm::runtime::profiling::WrapTimeEvaluator(tvm::runtime::PackedFunc, DLDevice, int, int, int, int, int, int, tvm::runtime::PackedFunc)::$_7::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/profiling.cc:879
  14: tvm::runtime::PackedFunc::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1221
  13: tvm::runtime::PackedFuncObj::CallPacked(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1217
  12: tvm::runtime::PackedFuncObj::Extractor<tvm::runtime::PackedFuncSubObj<tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0> >::Call(tvm::runtime::PackedFuncObj const*, tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1213
  11: tvm::runtime::WrapPackedFunc(int (*)(TVMValue*, int*, int, TVMValue*, int*, void*), tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()(tvm::runtime::TVMArgs, tvm::runtime::TVMRetValue*) const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:83
  10: 0x0000000112df8c63
  9: 
  8: TVMBackendGetFuncFromEnv
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/c_runtime_api.cc:426
  7: tvm::runtime::ModuleNode::GetFuncFromEnv(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:114
  6: tvm::runtime::Module::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/include/tvm/runtime/packed_func.h:1946
  5: tvm::runtime::ModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/module.cc:66
  4: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:247
  3: void tvm::runtime::metal::AutoReleasePoolWrapper::operator<<<tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0>(tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0 const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_common.h:89
  2: tvm::runtime::MetalModuleNode::GetFunction(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, tvm::runtime::ObjectPtr<tvm::runtime::Object> const&)::$_0::operator()() const
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:258
  1: tvm::runtime::MetalWrappedFunc::Init(tvm::runtime::MetalModuleNode*, tvm::runtime::ObjectPtr<tvm::runtime::Object>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:187
  0: tvm::runtime::MetalModuleNode::GetPipelineState(unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)
        at /Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm:130
  File "/Users/catalyst/Workspace/tvm-ruihang/src/runtime/metal/metal_module.mm", line 130
  File "/Users/guoyaol/tvm/src/runtime/rpc/rpc_endpoint.cc", line 376
RPCError: Error caught from RPC call:
[16:42:16] /Users/catalyst/Workspace/tvm-ruihang/src/runtime/library_module.cc:87: TVMError: 
---------------------------------------------------------------
An error occurred during the execution of TVM.
For more information, please see: https://tvm.apache.org/docs/errors.html
---------------------------------------------------------------
  Check failed: (state != nil) is false: cannot get state: for function main_kernel0Compute function exceeds available temporary registers


# from tvm.script import ir as I
# from tvm.script import tir as T

@I.ir_module
class Module:
    @T.prim_func
    def main(lv2: T.Buffer((T.int64(1), T.int64(64), T.int64(640), T.int64(448)), "float32"), self_rrdb_body_0_rdb1_conv1_weight: T.Buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), "float32"), lv4: T.Buffer((T.int64(1), T.int64(32), T.int64(1), T.int64(1)), "float32"), var_compute_intermediate: T.Buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), "float32")):
        T.func_attr({"global_symbol": "main", "tir.noalias": T.bool(True)})
        # with T.block("root"):
        var_conv2d_nchw_intermediate_local = T.alloc_buffer((T.int64(1), T.int64(32), T.int64(640), T.int64(448)), scope="local")
        pad_temp_shared = T.alloc_buffer((T.int64(1), T.int64(64), T.int64(642), T.int64(450)), scope="shared")
        self_rrdb_body_0_rdb1_conv1_weight_shared = T.alloc_buffer((T.int64(32), T.int64(64), T.int64(3), T.int64(3)), scope="shared")
        for nn_0_ff_0_yy_0_xx_0_fused in T.thread_binding(T.int64(64), thread="blockIdx.x", annotations={"pragma_auto_unroll_max_step": 64, "pragma_unroll_explicit": 1}):
            for nn_1_ff_1_yy_1_xx_1_fused in T.thread_binding(T.int64(8), thread="vthread.x"):
                for nn_2_ff_2_yy_2_xx_2_fused in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                    for nn_3_init, ff_3_init, yy_3_init, xx_3_init, nn_4_init, ff_4_init, yy_4_init, xx_4_init in T.grid(T.int64(1), T.int64(1), T.int64(20), T.int64(2), T.int64(1), T.int64(2), T.int64(1), T.int64(7)):
                        with T.block("conv2d_nchw_init"):
                            v_nn = T.axis.spatial(T.int64(1), nn_3_init + nn_4_init)
                            v_ff = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3_init * T.int64(2) + ff_4_init)
                            v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(20) + yy_3_init + yy_4_init)
                            v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(28) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(14) + xx_3_init * T.int64(7) + xx_4_init)
                            T.reads()
                            T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                            T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                            var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = T.float32(0)
                    for rc_0, ry_0, rx_0 in T.grid(T.int64(64), T.int64(3), T.int64(1)):
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(75)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                for ax0_ax1_ax2_ax3_fused_2 in T.vectorized(T.int64(2)):
                                    with T.block("pad_temp_shared"):
                                        v0 = T.axis.spatial(T.int64(1), T.int64(0))
                                        v1 = T.axis.spatial(T.int64(64), rc_0)
                                        v2 = T.axis.spatial(T.int64(642), ry_0 + nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(160) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) // T.int64(30))
                                        v3 = T.axis.spatial(T.int64(450), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(28) + (ax0_ax1_ax2_ax3_fused_0 * T.int64(64) + ax0_ax1_ax2_ax3_fused_1 * T.int64(2) + ax0_ax1_ax2_ax3_fused_2) % T.int64(30))
                                        T.reads(lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)])
                                        T.writes(pad_temp_shared[v0, v1, v2, v3])
                                        pad_temp_shared[v0, v1, v2, v3] = T.if_then_else(T.int64(1) <= v2 and v2 < T.int64(641) and T.int64(1) <= v3 and v3 < T.int64(449), lv2[v0, v1, v2 - T.int64(1), v3 - T.int64(1)], T.float32(0))
                        for ax0_ax1_ax2_ax3_fused_0 in range(T.int64(3)):
                            for ax0_ax1_ax2_ax3_fused_1 in T.thread_binding(T.int64(32), thread="threadIdx.x"):
                                with T.block("self_rrdb_body_0_rdb1_conv1.weight_shared"):
                                    v0 = T.axis.spatial(T.int64(32), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) // T.int64(3))
                                    v1, v2 = T.axis.remap("SS", [rc_0, ry_0])
                                    v3 = T.axis.spatial(T.int64(3), (ax0_ax1_ax2_ax3_fused_0 * T.int64(32) + ax0_ax1_ax2_ax3_fused_1) % T.int64(3))
                                    T.reads(self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3])
                                    T.writes(self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3])
                                    self_rrdb_body_0_rdb1_conv1_weight_shared[v0, v1, v2, v3] = self_rrdb_body_0_rdb1_conv1_weight[v0, v1, v2, v3]
                        for rc_1, ry_1, rx_1, nn_3, ff_3, yy_3, xx_3, rc_2, ry_2, rx_2, nn_4, ff_4, yy_4, xx_4 in T.grid(T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(1), T.int64(20), T.int64(2), T.int64(1), T.int64(1), T.int64(3), T.int64(1), T.int64(2), T.int64(1), T.int64(7)):
                            with T.block("conv2d_nchw_update"):
                                v_nn = T.axis.spatial(T.int64(1), nn_3 + nn_4)
                                v_ff = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ff_3 * T.int64(2) + ff_4)
                                v_yy = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(20) + yy_3 + yy_4)
                                v_xx = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(28) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(14) + xx_3 * T.int64(7) + xx_4)
                                v_rc = T.axis.reduce(T.int64(64), rc_0 + rc_1 + rc_2)
                                v_ry = T.axis.reduce(T.int64(3), ry_0 + ry_1 + ry_2)
                                v_rx = T.axis.reduce(T.int64(3), rx_0 * T.int64(3) + rx_1 * T.int64(3) + rx_2)
                                T.reads(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx], pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx], self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx])
                                T.writes(var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx])
                                T.block_attr({"meta_schedule.thread_extent_high_inclusive": 256, "meta_schedule.thread_extent_low_inclusive": 32, "meta_schedule.tiling_structure": "SSSRRSRS"})
                                var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] = var_conv2d_nchw_intermediate_local[v_nn, v_ff, v_yy, v_xx] + pad_temp_shared[v_nn, v_rc, v_yy + v_ry, v_xx + v_rx] * self_rrdb_body_0_rdb1_conv1_weight_shared[v_ff, v_rc, v_ry, v_rx]
                    for ax0, ax1, ax2, ax3 in T.grid(T.int64(1), T.int64(2), T.int64(20), T.int64(14)):
                        with T.block("var_conv2d_nchw_intermediate_local"):
                            v0 = T.axis.spatial(T.int64(1), ax0)
                            v1 = T.axis.spatial(T.int64(32), nn_1_ff_1_yy_1_xx_1_fused // T.int64(2) * T.int64(8) + nn_2_ff_2_yy_2_xx_2_fused // T.int64(8) * T.int64(2) + ax1)
                            v2 = T.axis.spatial(T.int64(640), nn_0_ff_0_yy_0_xx_0_fused // T.int64(16) * T.int64(160) + nn_2_ff_2_yy_2_xx_2_fused % T.int64(8) * T.int64(20) + ax2)
                            v3 = T.axis.spatial(T.int64(448), nn_0_ff_0_yy_0_xx_0_fused % T.int64(16) * T.int64(28) + nn_1_ff_1_yy_1_xx_1_fused % T.int64(2) * T.int64(14) + ax3)
                            T.reads(var_conv2d_nchw_intermediate_local[v0, v1, v2, v3], lv4[v0, v1, T.int64(0), T.int64(0)])
                            T.writes(var_compute_intermediate[v0, v1, v2, v3])
                            var_compute_intermediate[v0, v1, v2, v3] = T.Select(T.float32(0) < var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)], (var_conv2d_nchw_intermediate_local[v0, v1, v2, v3] + lv4[v0, v1, T.int64(0), T.int64(0)]) * T.float32(0.20000000000000001))
b0 = sch.get_block(name="pad_temp", func_name="main")
b1 = sch.get_block(name="conv2d_nchw", func_name="main")
b2 = sch.get_block(name="T_add", func_name="main")
b3 = sch.get_block(name="compute", func_name="main")
b4 = sch.get_block(name="root", func_name="main")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.tiling_structure", ann_val="SSSRRSRS")
l5, l6, l7, l8, l9, l10, l11 = sch.get_loops(block=b1)
v12, v13, v14, v15, v16 = sch.sample_perfect_tile(loop=l5, n=5, max_innermost_factor=64, decision=[1, 1, 1, 1, 1])
l17, l18, l19, l20, l21 = sch.split(loop=l5, factors=[v12, v13, v14, v15, v16], preserve_unit_iters=True)
v22, v23, v24, v25, v26 = sch.sample_perfect_tile(loop=l6, n=5, max_innermost_factor=64, decision=[1, 4, 4, 1, 2])
l27, l28, l29, l30, l31 = sch.split(loop=l6, factors=[v22, v23, v24, v25, v26], preserve_unit_iters=True)
v32, v33, v34, v35, v36 = sch.sample_perfect_tile(loop=l7, n=5, max_innermost_factor=64, decision=[4, 1, 8, 20, 1])
l37, l38, l39, l40, l41 = sch.split(loop=l7, factors=[v32, v33, v34, v35, v36], preserve_unit_iters=True)
v42, v43, v44, v45, v46 = sch.sample_perfect_tile(loop=l8, n=5, max_innermost_factor=64, decision=[16, 2, 1, 2, 7])
l47, l48, l49, l50, l51 = sch.split(loop=l8, factors=[v42, v43, v44, v45, v46], preserve_unit_iters=True)
v52, v53, v54 = sch.sample_perfect_tile(loop=l9, n=3, max_innermost_factor=64, decision=[64, 1, 1])
l55, l56, l57 = sch.split(loop=l9, factors=[v52, v53, v54], preserve_unit_iters=True)
v58, v59, v60 = sch.sample_perfect_tile(loop=l10, n=3, max_innermost_factor=64, decision=[3, 1, 1])
l61, l62, l63 = sch.split(loop=l10, factors=[v58, v59, v60], preserve_unit_iters=True)
v64, v65, v66 = sch.sample_perfect_tile(loop=l11, n=3, max_innermost_factor=64, decision=[1, 1, 3])
l67, l68, l69 = sch.split(loop=l11, factors=[v64, v65, v66], preserve_unit_iters=True)
sch.reorder(l17, l27, l37, l47, l18, l28, l38, l48, l19, l29, l39, l49, l55, l61, l67, l56, l62, l68, l20, l30, l40, l50, l57, l63, l69, l21, l31, l41, l51)
l70 = sch.fuse(l17, l27, l37, l47, preserve_unit_iters=True)
sch.bind(loop=l70, thread_axis="blockIdx.x")
l71 = sch.fuse(l18, l28, l38, l48, preserve_unit_iters=True)
sch.bind(loop=l71, thread_axis="vthread.x")
l72 = sch.fuse(l19, l29, l39, l49, preserve_unit_iters=True)
sch.bind(loop=l72, thread_axis="threadIdx.x")
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_low_inclusive", ann_val=32)
sch.annotate(block_or_loop=b1, ann_key="meta_schedule.thread_extent_high_inclusive", ann_val=256)
b73 = sch.cache_write(block=b1, write_buffer_index=0, storage_scope="local")
sch.reverse_compute_at(block=b73, loop=l72, preserve_unit_loops=True, index=-1)
b74 = sch.cache_read(block=b1, read_buffer_index=0, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b74, loop=l67, preserve_unit_loops=True, index=-1)
l75, l76, l77, l78, l79, l80, l81, l82, l83, l84 = sch.get_loops(block=b74)
l85 = sch.fuse(l81, l82, l83, l84, preserve_unit_iters=True)
v86 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=1)
sch.annotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch", ann_val=v86)
b87 = sch.cache_read(block=b1, read_buffer_index=1, storage_scope="shared", consumer_blocks=[b1])
sch.compute_at(block=b87, loop=l67, preserve_unit_loops=True, index=-1)
l88, l89, l90, l91, l92, l93, l94, l95, l96, l97 = sch.get_loops(block=b87)
l98 = sch.fuse(l94, l95, l96, l97, preserve_unit_iters=True)
v99 = sch.sample_categorical(candidates=[1, 2, 3, 4], probs=[0.25, 0.25, 0.25, 0.25], decision=0)
sch.annotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch", ann_val=v99)
sch.reverse_compute_inline(block=b3)
sch.reverse_compute_inline(block=b2)
sch.compute_inline(block=b0)
v100 = sch.sample_categorical(candidates=[0, 16, 64, 512, 1024], probs=[0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001, 0.20000000000000001], decision=2)
sch.annotate(block_or_loop=b4, ann_key="meta_schedule.unroll_explicit", ann_val=v100)
sch.enter_postproc()
sch.unannotate(block_or_loop=b74, ann_key="meta_schedule.cooperative_fetch")
l101, l102, l103, l104, l105, l106, l107 = sch.get_loops(block=b74)
l108, l109, l110 = sch.split(loop=l107, factors=[None, 32, 2], preserve_unit_iters=True)
sch.vectorize(loop=l110)
sch.bind(loop=l109, thread_axis="threadIdx.x")
sch.unannotate(block_or_loop=b87, ann_key="meta_schedule.cooperative_fetch")
l111, l112, l113, l114, l115, l116, l117 = sch.get_loops(block=b87)
l118, l119 = sch.split(loop=l117, factors=[None, 32], preserve_unit_iters=True)
sch.bind(loop=l119, thread_axis="threadIdx.x")
b120 = sch.get_block(name="root", func_name="main")
sch.unannotate(block_or_loop=b120, ann_key="meta_schedule.unroll_explicit")
b121, b122, b123, b124 = sch.get_child_blocks(b120)
l125, l126, l127, l128, l129, l130, l131, l132, l133 = sch.get_loops(block=b121)
l134, l135, l136, l137, l138, l139, l140, l141 = sch.get_loops(block=b122)
l142, l143, l144, l145, l146, l147, l148, l149, l150, l151, l152, l153, l154, l155, l156, l157, l158, l159, l160, l161 = sch.get_loops(block=b123)
sch.annotate(block_or_loop=l142, ann_key="pragma_auto_unroll_max_step", ann_val=64)
sch.annotate(block_or_loop=l142, ann_key="pragma_unroll_explicit", ann_val=1)
l162, l163, l164, l165, l166, l167, l168 = sch.get_loops(block=b124)
b169 = sch.get_block(name="conv2d_nchw", func_name="main")
l170, l171, l172, l173, l174, l175, l176, l177, l178, l179, l180, l181, l182, l183, l184, l185, l186, l187, l188, l189 = sch.get_loops(block=b169)
b190 = sch.decompose_reduction(block=b169, loop=l173)
2023-05-18 16:43:59 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #704: GFLOPs: 25.2851. Time: 418744.0557 us. Best GFLOPs: 1387.2955
2023-05-18 16:51:53 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 16:51:54 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 16:51:55 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 408 failure(s)
2023-05-18 16:51:57 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 813 failure(s)
2023-05-18 16:51:59 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1218 failure(s)
2023-05-18 16:51:59 [INFO] [evolutionary_search.cc:723] Sampled 12 candidate(s)
2023-05-18 16:52:03 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 85 failure(s)
2023-05-18 16:52:09 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 56 failure(s)
2023-05-18 16:52:15 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 49 failure(s)
2023-05-18 16:52:21 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 65 failure(s)
2023-05-18 16:52:23 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.8664  0.8582  0.8483  0.8482  0.8469  0.8432  0.8394  0.8347  0.8342  0.8333  0.8333  0.8315  0.8315  0.8315  0.8308  0.8308
[17 : 32]:	0.8301  0.8290  0.8287  0.8283  0.8281  0.8281  0.8249  0.8248  0.8241  0.8237  0.8215  0.8207  0.8206  0.8206  0.8205  0.8205
[33 : 48]:	0.8198  0.8198  0.8164  0.8164  0.8161  0.8157  0.8155  0.8134  0.8133  0.8133  0.8133  0.8122  0.8117  0.8113  0.8111  0.8101
[49 : 64]:	0.8094  0.8087  0.8085  0.8081  0.8069  0.8061  0.8061  0.8058  0.8050  0.8048  0.8048  0.8042  0.8040  0.8038  0.8037  0.8034
2023-05-18 16:52:23 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 16:52:23 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #705: GFLOPs: 1227.5918. Time: 8625.0138 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #706: GFLOPs: 1207.8729. Time: 8765.8195 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #707: GFLOPs: 1255.8928. Time: 8430.6528 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #708: GFLOPs: 1274.0736. Time: 8310.3487 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #709: GFLOPs: 1232.1951. Time: 8592.7917 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #710: GFLOPs: 1231.9373. Time: 8594.5902 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #711: GFLOPs: 1385.2575. Time: 7643.3413 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #712: GFLOPs: 1127.4243. Time: 9391.3144 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #713: GFLOPs: 840.5041. Time: 12597.1980 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #714: GFLOPs: 1166.1280. Time: 9079.6176 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #715: GFLOPs: 1199.0893. Time: 8830.0312 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #716: GFLOPs: 1171.5504. Time: 9037.5932 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #717: GFLOPs: 1180.2299. Time: 8971.1299 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #718: GFLOPs: 1270.7067. Time: 8332.3684 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #719: GFLOPs: 747.4501. Time: 14165.4895 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #720: GFLOPs: 1107.2414. Time: 9562.5000 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #721: GFLOPs: 1223.4908. Time: 8653.9236 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #722: GFLOPs: 1118.2436. Time: 9468.4167 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #723: GFLOPs: 1270.1404. Time: 8336.0833 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #724: GFLOPs: 1220.7777. Time: 8673.1562 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #725: GFLOPs: 1021.5513. Time: 10364.6250 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #726: GFLOPs: 1198.2887. Time: 8835.9306 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #727: GFLOPs: 1168.9509. Time: 9057.6912 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #728: GFLOPs: 1209.0770. Time: 8757.0903 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #729: GFLOPs: 1192.8778. Time: 8876.0104 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #730: GFLOPs: 1020.0243. Time: 10380.1416 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #731: GFLOPs: 1049.7538. Time: 10086.1708 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #732: GFLOPs: 1101.3501. Time: 9613.6515 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #733: GFLOPs: 1177.2369. Time: 8993.9388 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #734: GFLOPs: 1269.3980. Time: 8340.9583 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #735: GFLOPs: 737.6241. Time: 14354.1904 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #736: GFLOPs: 1121.1028. Time: 9444.2690 us. Best GFLOPs: 1387.2955
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #737: GFLOPs: 1420.0843. Time: 7455.8929 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #738: GFLOPs: 1110.1460. Time: 9537.4811 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #739: GFLOPs: 1111.4552. Time: 9526.2463 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #740: GFLOPs: 1104.4467. Time: 9586.6969 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #741: GFLOPs: 1225.5482. Time: 8639.3958 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #742: GFLOPs: 1250.5759. Time: 8466.4965 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #743: GFLOPs: 1040.7930. Time: 10173.0083 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #744: GFLOPs: 1105.2918. Time: 9579.3675 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #745: GFLOPs: 1197.5523. Time: 8841.3646 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #746: GFLOPs: 1271.8766. Time: 8324.7039 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #747: GFLOPs: 1178.6279. Time: 8983.3235 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #748: GFLOPs: 1228.7345. Time: 8616.9930 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #749: GFLOPs: 1234.4703. Time: 8576.9548 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #750: GFLOPs: 1124.2673. Time: 9417.6856 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #751: GFLOPs: 1055.2123. Time: 10033.9959 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #752: GFLOPs: 922.4344. Time: 11478.3194 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #753: GFLOPs: 1088.4432. Time: 9727.6515 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #754: GFLOPs: 1251.3585. Time: 8461.2014 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #755: GFLOPs: 936.9321. Time: 11300.7083 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #756: GFLOPs: 850.9158. Time: 12443.0601 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #757: GFLOPs: 1259.6796. Time: 8405.3091 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #758: GFLOPs: 1043.7638. Time: 10144.0541 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #759: GFLOPs: 940.2989. Time: 11260.2453 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #760: GFLOPs: 1172.2581. Time: 9032.1373 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #761: GFLOPs: 1066.9778. Time: 9923.3523 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #762: GFLOPs: 856.9662. Time: 12355.2083 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #763: GFLOPs: 284.3180. Time: 37239.9723 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #764: GFLOPs: 1123.1736. Time: 9426.8560 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #765: GFLOPs: 393.4551. Time: 26910.3023 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #766: GFLOPs: 36.5820. Time: 289431.6527 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #767: GFLOPs: 31.0737. Time: 340738.2500 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #768: GFLOPs: 232.0287. Time: 45632.2640 us. Best GFLOPs: 1420.0843
2023-05-18 16:58:01 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 16:58:02 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 16:58:03 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 407 failure(s)
2023-05-18 16:58:05 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 815 failure(s)
2023-05-18 16:58:07 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1217 failure(s)
2023-05-18 16:58:07 [INFO] [evolutionary_search.cc:723] Sampled 13 candidate(s)
2023-05-18 16:58:11 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 73 failure(s)
2023-05-18 16:58:17 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 50 failure(s)
2023-05-18 16:58:23 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 64 failure(s)
2023-05-18 16:58:29 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 64 failure(s)
2023-05-18 16:58:31 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9055  0.8981  0.8977  0.8971  0.8858  0.8795  0.8795  0.8785  0.8767  0.8764  0.8753  0.8694  0.8661  0.8657  0.8632  0.8632
[17 : 32]:	0.8612  0.8607  0.8602  0.8602  0.8600  0.8592  0.8590  0.8589  0.8579  0.8578  0.8574  0.8574  0.8544  0.8544  0.8539  0.8507
[33 : 48]:	0.8496  0.8454  0.8454  0.8429  0.8391  0.8389  0.8370  0.8337  0.8324  0.8295  0.8257  0.8241  0.8227  0.8227  0.8227  0.8227
[49 : 64]:	0.8220  0.8196  0.8196  0.8191  0.8177  0.8169  0.8169  0.8151  0.8143  0.8119  0.8113  0.8067  0.8036  0.8020  0.8020  0.8013
2023-05-18 16:58:31 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 16:58:31 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #769: GFLOPs: 1220.8799. Time: 8672.4305 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #770: GFLOPs: 1383.5021. Time: 7653.0397 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #771: GFLOPs: 1350.8223. Time: 7838.1859 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #772: GFLOPs: 1181.5257. Time: 8961.2917 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #773: GFLOPs: 1216.1553. Time: 8706.1215 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #774: GFLOPs: 1282.2213. Time: 8257.5416 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #775: GFLOPs: 1222.9555. Time: 8657.7118 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #776: GFLOPs: 1266.6361. Time: 8359.1458 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #777: GFLOPs: 1286.2014. Time: 8231.9890 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #778: GFLOPs: 1271.5272. Time: 8326.9912 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #779: GFLOPs: 1265.0104. Time: 8369.8889 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #780: GFLOPs: 1261.9483. Time: 8390.1979 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #781: GFLOPs: 1197.8670. Time: 8839.0417 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #782: GFLOPs: 1215.2127. Time: 8712.8750 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #783: GFLOPs: 1171.9496. Time: 9034.5147 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #784: GFLOPs: 1164.6121. Time: 9091.4356 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #785: GFLOPs: 1224.2286. Time: 8648.7083 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #786: GFLOPs: 1274.0639. Time: 8310.4123 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #787: GFLOPs: 1067.0633. Time: 9922.5568 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #788: GFLOPs: 1157.1260. Time: 9150.2538 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #789: GFLOPs: 1262.4081. Time: 8387.1423 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #790: GFLOPs: 1366.5169. Time: 7748.1635 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #791: GFLOPs: 1249.5474. Time: 8473.4653 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #792: GFLOPs: 1211.6886. Time: 8738.2152 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #793: GFLOPs: 1185.6543. Time: 8930.0868 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #794: GFLOPs: 1226.7201. Time: 8631.1424 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #795: GFLOPs: 1172.1344. Time: 9033.0902 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #796: GFLOPs: 1293.4495. Time: 8185.8596 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #797: GFLOPs: 1238.3948. Time: 8549.7743 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #798: GFLOPs: 1280.2130. Time: 8270.4956 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #799: GFLOPs: 1212.5525. Time: 8731.9896 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #800: GFLOPs: 1229.2892. Time: 8613.1042 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #801: GFLOPs: 1249.2561. Time: 8475.4410 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #802: GFLOPs: 1174.1140. Time: 9017.8603 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #803: GFLOPs: 1195.7865. Time: 8854.4202 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #804: GFLOPs: 1128.0545. Time: 9386.0682 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #805: GFLOPs: 1220.9200. Time: 8672.1458 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #806: GFLOPs: 1248.2737. Time: 8482.1111 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #807: GFLOPs: 1271.2912. Time: 8328.5373 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #808: GFLOPs: 1222.3603. Time: 8661.9271 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #809: GFLOPs: 1179.0626. Time: 8980.0122 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #810: GFLOPs: 1058.7559. Time: 10000.4125 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #811: GFLOPs: 1227.2558. Time: 8627.3750 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #812: GFLOPs: 1125.7674. Time: 9405.1364 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #813: GFLOPs: 1104.5222. Time: 9586.0416 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #814: GFLOPs: 1418.0013. Time: 7466.8453 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #815: GFLOPs: 1112.1469. Time: 9520.3219 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #816: GFLOPs: 1272.3596. Time: 8321.5439 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #817: GFLOPs: 1249.1281. Time: 8476.3090 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #818: GFLOPs: 1402.0277. Time: 7551.9167 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #819: GFLOPs: 1117.5618. Time: 9474.1932 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #820: GFLOPs: 1265.4655. Time: 8366.8785 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #821: GFLOPs: 1285.3905. Time: 8237.1827 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #822: GFLOPs: 1079.5478. Time: 9807.8068 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #823: GFLOPs: 1147.4840. Time: 9227.1402 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #824: GFLOPs: 1119.8393. Time: 9454.9243 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #825: GFLOPs: 1291.5251. Time: 8198.0570 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #826: GFLOPs: 1296.1761. Time: 8168.6404 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #827: GFLOPs: 1164.3938. Time: 9093.1401 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #828: GFLOPs: 782.1659. Time: 13536.7656 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #829: GFLOPs: 1229.5723. Time: 8611.1215 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #830: GFLOPs: 113.8084. Time: 93033.5140 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #831: GFLOPs: 31.1377. Time: 340037.9723 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #832: GFLOPs: 97.9459. Time: 108100.4583 us. Best GFLOPs: 1420.0843
2023-05-18 16:59:31 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 16:59:32 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 16:59:34 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 406 failure(s)
2023-05-18 16:59:36 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 812 failure(s)
2023-05-18 16:59:38 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1217 failure(s)
2023-05-18 16:59:38 [INFO] [evolutionary_search.cc:723] Sampled 13 candidate(s)
2023-05-18 16:59:42 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 73 failure(s)
2023-05-18 16:59:48 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 61 failure(s)
2023-05-18 16:59:54 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 62 failure(s)
2023-05-18 17:00:00 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 50 failure(s)
2023-05-18 17:00:02 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9182  0.9059  0.8878  0.8797  0.8793  0.8783  0.8783  0.8778  0.8778  0.8774  0.8753  0.8743  0.8725  0.8709  0.8660  0.8632
[17 : 32]:	0.8624  0.8622  0.8602  0.8600  0.8600  0.8592  0.8592  0.8579  0.8553  0.8544  0.8522  0.8512  0.8511  0.8509  0.8497  0.8460
[33 : 48]:	0.8439  0.8434  0.8385  0.8382  0.8372  0.8353  0.8347  0.8332  0.8332  0.8313  0.8313  0.8304  0.8304  0.8304  0.8303  0.8303
[49 : 64]:	0.8302  0.8302  0.8302  0.8295  0.8280  0.8254  0.8252  0.8249  0.8237  0.8237  0.8220  0.8218  0.8215  0.8198  0.8192  0.8187
2023-05-18 17:00:02 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 17:00:02 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #833: GFLOPs: 1191.3269. Time: 8887.5660 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #834: GFLOPs: 1277.3341. Time: 8289.1359 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #835: GFLOPs: 1263.3406. Time: 8380.9514 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #836: GFLOPs: 1276.2526. Time: 8296.1601 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #837: GFLOPs: 1258.2580. Time: 8414.8056 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #838: GFLOPs: 1187.1189. Time: 8919.0695 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #839: GFLOPs: 1202.3474. Time: 8806.1042 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #840: GFLOPs: 1143.1993. Time: 9261.7235 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #841: GFLOPs: 1228.9469. Time: 8615.5034 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #842: GFLOPs: 1243.1755. Time: 8516.8958 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #843: GFLOPs: 1183.6353. Time: 8945.3194 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #844: GFLOPs: 1279.7081. Time: 8273.7588 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #845: GFLOPs: 1265.8133. Time: 8364.5799 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #846: GFLOPs: 1341.9110. Time: 7890.2372 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #847: GFLOPs: 1221.8573. Time: 8665.4931 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #848: GFLOPs: 1209.5057. Time: 8753.9861 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #849: GFLOPs: 1220.6028. Time: 8674.3993 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #850: GFLOPs: 1201.0848. Time: 8815.3611 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #851: GFLOPs: 1260.4575. Time: 8400.1215 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #852: GFLOPs: 1128.1792. Time: 9385.0303 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #853: GFLOPs: 1096.0650. Time: 9660.0075 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #854: GFLOPs: 1107.1208. Time: 9563.5417 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #855: GFLOPs: 1111.7205. Time: 9523.9735 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #856: GFLOPs: 1270.4341. Time: 8334.1563 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #857: GFLOPs: 1232.7247. Time: 8589.1007 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #858: GFLOPs: 1190.7379. Time: 8891.9618 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #859: GFLOPs: 1075.3730. Time: 9845.8825 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #860: GFLOPs: 1216.7716. Time: 8701.7118 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #861: GFLOPs: 1349.9495. Time: 7843.2532 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #862: GFLOPs: 1322.2324. Time: 8007.6667 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #863: GFLOPs: 1285.7164. Time: 8235.0943 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #864: GFLOPs: 1356.1777. Time: 7807.2339 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #865: GFLOPs: 1181.4168. Time: 8962.1176 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #866: GFLOPs: 1328.5182. Time: 7969.7788 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #867: GFLOPs: 1235.1043. Time: 8572.5521 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #868: GFLOPs: 1351.2400. Time: 7835.7628 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #869: GFLOPs: 1266.0277. Time: 8363.1632 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #870: GFLOPs: 1297.5060. Time: 8160.2676 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #871: GFLOPs: 1243.6105. Time: 8513.9167 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #872: GFLOPs: 1208.7626. Time: 8759.3681 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #873: GFLOPs: 1276.5540. Time: 8294.2017 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #874: GFLOPs: 1119.3833. Time: 9458.7765 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #875: GFLOPs: 1105.0178. Time: 9581.7425 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #876: GFLOPs: 1319.5609. Time: 8023.8782 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #877: GFLOPs: 1110.5456. Time: 9534.0493 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #878: GFLOPs: 913.1417. Time: 11595.1297 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #879: GFLOPs: 1268.5309. Time: 8346.6597 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #880: GFLOPs: 1245.7570. Time: 8499.2466 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #881: GFLOPs: 1130.9522. Time: 9362.0189 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #882: GFLOPs: 1210.2157. Time: 8748.8508 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #883: GFLOPs: 1209.5647. Time: 8753.5591 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #884: GFLOPs: 1278.0837. Time: 8284.2742 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #885: GFLOPs: 1153.6797. Time: 9177.5871 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #886: GFLOPs: 1055.1891. Time: 10034.2167 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #887: GFLOPs: 1269.9484. Time: 8337.3438 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #888: GFLOPs: 866.6419. Time: 12217.2684 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #889: GFLOPs: 1172.6005. Time: 9029.5000 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #890: GFLOPs: 1251.9169. Time: 8457.4271 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #891: GFLOPs: 1218.7295. Time: 8687.7327 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #892: GFLOPs: 1375.0307. Time: 7700.1892 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #893: GFLOPs: 1094.4299. Time: 9674.4395 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #894: GFLOPs: 264.5067. Time: 40029.2220 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #895: GFLOPs: 13.3300. Time: 794300.2360 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #896: GFLOPs: 82.2897. Time: 128667.3610 us. Best GFLOPs: 1420.0843
2023-05-18 17:01:10 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 17:01:11 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 17:01:13 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 405 failure(s)
2023-05-18 17:01:15 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 811 failure(s)
2023-05-18 17:01:16 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1219 failure(s)
2023-05-18 17:01:16 [INFO] [evolutionary_search.cc:723] Sampled 11 candidate(s)
2023-05-18 17:01:20 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 73 failure(s)
2023-05-18 17:01:26 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 54 failure(s)
2023-05-18 17:01:32 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 62 failure(s)
2023-05-18 17:01:39 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 69 failure(s)
2023-05-18 17:01:41 [INFO] [evolutionary_search.cc:649] Scores of the best 64 candidates:
[1 : 16]:	0.9239  0.9209  0.9099  0.9089  0.8861  0.8857  0.8783  0.8778  0.8778  0.8778  0.8759  0.8724  0.8647  0.8622  0.8617  0.8589
[17 : 32]:	0.8583  0.8563  0.8559  0.8544  0.8544  0.8538  0.8538  0.8536  0.8433  0.8423  0.8415  0.8411  0.8403  0.8397  0.8393  0.8373
[33 : 48]:	0.8370  0.8320  0.8312  0.8305  0.8304  0.8303  0.8295  0.8280  0.8234  0.8227  0.8227  0.8216  0.8192  0.8177  0.8177  0.8169
[49 : 64]:	0.8142  0.8121  0.8119  0.8117  0.8108  0.8077  0.8063  0.8051  0.8020  0.8020  0.8019  0.8000  0.7976  0.7973  0.7964  0.7961
2023-05-18 17:01:41 [INFO] [evolutionary_search.cc:727] Got 64 candidate(s) with evolutionary search
2023-05-18 17:01:41 [INFO] [evolutionary_search.cc:730] Sending 64 candidates(s) for measurement
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #897: GFLOPs: 1279.2683. Time: 8276.6031 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #898: GFLOPs: 1283.8588. Time: 8247.0096 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #899: GFLOPs: 1205.4359. Time: 8783.5417 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #900: GFLOPs: 1286.7082. Time: 8228.7468 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #901: GFLOPs: 1148.2208. Time: 9221.2197 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #902: GFLOPs: 1288.7933. Time: 8215.4342 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #903: GFLOPs: 1353.3957. Time: 7823.2821 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #904: GFLOPs: 1297.3585. Time: 8161.1955 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #905: GFLOPs: 1286.5106. Time: 8230.0110 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #906: GFLOPs: 1259.6192. Time: 8405.7118 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #907: GFLOPs: 1241.3379. Time: 8529.5034 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #908: GFLOPs: 1263.5008. Time: 8379.8888 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #909: GFLOPs: 1282.7207. Time: 8254.3269 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #910: GFLOPs: 1225.2971. Time: 8641.1667 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #911: GFLOPs: 1278.7289. Time: 8280.0943 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #912: GFLOPs: 1251.1223. Time: 8462.7986 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #913: GFLOPs: 1324.2258. Time: 7995.6122 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #914: GFLOPs: 1261.5265. Time: 8393.0034 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #915: GFLOPs: 1253.1033. Time: 8449.4201 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #916: GFLOPs: 1214.9541. Time: 8714.7292 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #917: GFLOPs: 1174.7152. Time: 9013.2451 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #918: GFLOPs: 1238.5915. Time: 8548.4167 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #919: GFLOPs: 1189.9378. Time: 8897.9409 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #920: GFLOPs: 1365.2456. Time: 7755.3782 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #921: GFLOPs: 1253.5942. Time: 8446.1112 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #922: GFLOPs: 1220.2648. Time: 8676.8021 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #923: GFLOPs: 1230.0732. Time: 8607.6146 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #924: GFLOPs: 1310.2352. Time: 8080.9890 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #925: GFLOPs: 1226.7882. Time: 8630.6632 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #926: GFLOPs: 1299.9248. Time: 8145.0833 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #927: GFLOPs: 1286.1221. Time: 8232.4968 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #928: GFLOPs: 1172.7074. Time: 9028.6765 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #929: GFLOPs: 1168.6767. Time: 9059.8162 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #930: GFLOPs: 1208.3712. Time: 8762.2048 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #931: GFLOPs: 1224.9658. Time: 8643.5035 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #932: GFLOPs: 1300.9697. Time: 8138.5417 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #933: GFLOPs: 1104.4406. Time: 9586.7500 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #934: GFLOPs: 1185.0710. Time: 8934.4827 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #935: GFLOPs: 835.5107. Time: 12672.4844 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #936: GFLOPs: 1322.1800. Time: 8007.9840 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #937: GFLOPs: 991.6063. Time: 10677.6208 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #938: GFLOPs: 1117.3232. Time: 9476.2159 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #939: GFLOPs: 718.5822. Time: 14734.5654 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #940: GFLOPs: 1187.7654. Time: 8914.2153 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #941: GFLOPs: 1311.5313. Time: 8073.0032 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #942: GFLOPs: 1411.6279. Time: 7500.5575 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #943: GFLOPs: 1180.2141. Time: 8971.2500 us. Best GFLOPs: 1420.0843
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #944: GFLOPs: 1428.3447. Time: 7412.7739 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #945: GFLOPs: 1172.0250. Time: 9033.9338 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #946: GFLOPs: 1201.5477. Time: 8811.9652 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #947: GFLOPs: 1253.7726. Time: 8444.9098 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #948: GFLOPs: 1064.3402. Time: 9947.9432 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #949: GFLOPs: 1110.7773. Time: 9532.0606 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #950: GFLOPs: 1236.9034. Time: 8560.0833 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #951: GFLOPs: 1035.5418. Time: 10224.5959 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #952: GFLOPs: 1395.0885. Time: 7589.4801 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #953: GFLOPs: 1292.4693. Time: 8192.0680 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #954: GFLOPs: 1011.2373. Time: 10470.3375 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #955: GFLOPs: 1135.4461. Time: 9324.9659 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #956: GFLOPs: 1197.3463. Time: 8842.8854 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #957: GFLOPs: 1153.6088. Time: 9178.1515 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #958: GFLOPs: 120.6695. Time: 87743.7500 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #959: GFLOPs: 12.4791. Time: 848461.2500 us. Best GFLOPs: 1428.3447
2023-05-18 17:02:56 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #960: GFLOPs: 38.0390. Time: 278345.5973 us. Best GFLOPs: 1428.3447
2023-05-18 17:19:18 [INFO] [evolutionary_search.cc:713] Generating candidates......
2023-05-18 17:19:19 [INFO] [evolutionary_search.cc:715] Picked top 102 candidate(s) from database
2023-05-18 17:19:20 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 405 failure(s)
2023-05-18 17:19:22 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 812 failure(s)
2023-05-18 17:19:23 [INFO] [evolutionary_search.cc:533] Sample-Init-Population summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 1219 failure(s)
2023-05-18 17:19:23 [INFO] [evolutionary_search.cc:723] Sampled 11 candidate(s)
2023-05-18 17:19:28 [INFO] [evolutionary_search.cc:621] Evolve iter #0 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 71 failure(s)
2023-05-18 17:19:34 [INFO] [evolutionary_search.cc:621] Evolve iter #1 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 70 failure(s)
2023-05-18 17:19:40 [INFO] [evolutionary_search.cc:621] Evolve iter #2 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 69 failure(s)
2023-05-18 17:19:46 [INFO] [evolutionary_search.cc:621] Evolve iter #3 done. Summary:
Postproc #0 [meta_schedule.DisallowDynamicLoop(0x600001f60b08)]: 0 failure(s)
Postproc #1 [meta_schedule.RewriteCooperativeFetch(0x600001f62488)]: 0 failure(s)
Postproc #2 [meta_schedule.RewriteUnboundBlock(0x600001f61288)]: 0 failure(s)
Postproc #3 [meta_schedule.RewriteParallelVectorizeUnroll(0x600001f63248)]: 0 failure(s)
Postproc #4 [meta_schedule.RewriteReductionBlock(0x600001f633e8)]: 0 failure(s)
Postproc #5 [meta_schedule.VerifyGPUCode(0x60000149efd8)]: 71 failure(s)
2023-05-18 17:19:48 [INFO] [evolutionary_search.cc:649] Scores of the best 40 candidates:
[1 : 16]:	0.9031  0.9029  0.9003  0.9001  0.8845  0.8753  0.8753  0.8640  0.8614  0.8579  0.8556  0.8542  0.8522  0.8482  0.8473  0.8470
[17 : 32]:	0.8469  0.8467  0.8436  0.8429  0.8408  0.8404  0.8387  0.8385  0.8375  0.8369  0.8343  0.8313  0.8303  0.8302  0.8295  0.8289
[33 : 40]:	0.8281  0.8279  0.8257  0.8246  0.8233  0.8196  0.8191  0.8187
2023-05-18 17:19:48 [INFO] [evolutionary_search.cc:727] Got 40 candidate(s) with evolutionary search
2023-05-18 17:19:48 [INFO] [evolutionary_search.cc:730] Sending 40 candidates(s) for measurement
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #961: GFLOPs: 1252.2562. Time: 8455.1354 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #962: GFLOPs: 1269.2749. Time: 8341.7673 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #963: GFLOPs: 1289.1446. Time: 8213.1952 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #964: GFLOPs: 1254.9717. Time: 8436.8402 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #965: GFLOPs: 1225.9725. Time: 8636.4062 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #966: GFLOPs: 1245.7891. Time: 8499.0277 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #967: GFLOPs: 1164.3560. Time: 9093.4356 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #968: GFLOPs: 1179.7365. Time: 8974.8824 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #969: GFLOPs: 1206.7745. Time: 8773.7987 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #970: GFLOPs: 1354.9339. Time: 7814.4006 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #971: GFLOPs: 1265.9331. Time: 8363.7882 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #972: GFLOPs: 1404.7440. Time: 7537.3135 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #973: GFLOPs: 1012.8924. Time: 10453.2292 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #974: GFLOPs: 1206.2074. Time: 8777.9236 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #975: GFLOPs: 1209.0338. Time: 8757.4028 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #976: GFLOPs: 1232.7207. Time: 8589.1284 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #977: GFLOPs: 1194.2598. Time: 8865.7396 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #978: GFLOPs: 1289.0595. Time: 8213.7372 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #979: GFLOPs: 1259.9783. Time: 8403.3160 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #980: GFLOPs: 1251.9293. Time: 8457.3438 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #981: GFLOPs: 1208.0285. Time: 8764.6910 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #982: GFLOPs: 1178.7401. Time: 8982.4688 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #983: GFLOPs: 1345.6159. Time: 7868.5128 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #984: GFLOPs: 1249.5463. Time: 8473.4723 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #985: GFLOPs: 1352.8088. Time: 7826.6762 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #986: GFLOPs: 1316.6499. Time: 8041.6186 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #987: GFLOPs: 1268.5125. Time: 8346.7813 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #988: GFLOPs: 975.9258. Time: 10849.1815 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #989: GFLOPs: 1179.5166. Time: 8976.5556 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #990: GFLOPs: 1266.1675. Time: 8362.2396 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #991: GFLOPs: 1311.1960. Time: 8075.0673 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #992: GFLOPs: 1192.8316. Time: 8876.3542 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #993: GFLOPs: 1263.5113. Time: 8379.8194 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #994: GFLOPs: 1143.8192. Time: 9256.7045 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #995: GFLOPs: 1026.2433. Time: 10317.2375 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #996: GFLOPs: 1220.5329. Time: 8674.8958 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #997: GFLOPs: 1194.9028. Time: 8860.9688 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #998: GFLOPs: 720.1969. Time: 14701.5297 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #999: GFLOPs: 151.3744. Time: 69945.7360 us. Best GFLOPs: 1428.3447
2023-05-18 17:21:07 [INFO] [task_scheduler.cc:131] [Task #22: fused_conv2d1_add1_leaky_relu] Trial #1000: GFLOPs: 17.1556. Time: 617172.8057 us. Best GFLOPs: 1428.3447
